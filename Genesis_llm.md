# ğŸ“¦ Repository: Genesis

> Generated by **repo2llm** v1.1.0 on 2026-02-24 19:16  
> 80 files included

---

## ğŸ—ºï¸ Project Overview

*From `README.md`*

# Genesis AI Evolution Laboratory

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.9+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
</p>

Genesis is a cutting-edge framework for creating efficient AI models through evolutionary algorithms, knowledge distillation, and model pruning. Designed for dual-GPU acceleration, it evolves neural networks to achieve optimal performance with minimal computational resources.

## Features

- **Evolutionary Optimization**: SLERP-based genetic operations for smooth model weight interpolation
- **Knowledge Distillation**: Transfer knowledge from large teacher models to smaller students
- **Model Pruning**: Intelligent weight pruning with multiple saliency methods
- **Dual-GPU Support**: Parallel execution with teacher on GPU 0 and student on GPU 1
- **LoRA Integration**: Efficient fine-tuning with Low-Rank Adaptation
- **TTS Evolution**: Specialized support for Text-to-Speech style token evolution
- **Extensible Design**: Modular architecture for custom experiments

## Installation

```bash
# Clone the repository
git clone https://github.com/genesis-ai/genesis.git
cd genesis

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Genesis in development mode
pip install -e .
```

## Quick Start

```python
from genesis import EvolutionaryOptimizer, GenesisConfig

# Create configuration
config = GenesisConfig(
    teacher_model="meta-llama/Llama-2-7b-hf",
    use_lora=True,
    genetic=dict(
        population_size=20,
        generations=50,
        mutation_rate=0.1,
    ),
)

# Create optimizer
optimizer = EvolutionaryOptimizer(
    config=config,
    train_dataloader=train_loader,
    eval_dataloader=eval_loader,
)

# Run evolution
results = optimizer.run()
print(f"Best fitness: {results['best_fitness']}")

# Optional: Prune and distill
optimizer.prune_model(target_sparsity=0.3)
optimizer.distill(num_steps=1000)
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Genesis Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Teacher    â”‚     â”‚  Population  â”‚     â”‚   Student    â”‚ â”‚
â”‚  â”‚   (GPU 0)    â”‚â”€â”€â”€â”€â–¶â”‚   Manager    â”‚â”€â”€â”€â”€â–¶â”‚   (GPU 1)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â”‚                    â–¼                    â”‚          â”‚
â”‚         â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚          â”‚
â”‚         â”‚           â”‚   Genetics   â”‚              â”‚          â”‚
â”‚         â”‚           â”‚  (SLERP +    â”‚              â”‚          â”‚
â”‚         â”‚           â”‚  Mutation)   â”‚              â”‚          â”‚
â”‚         â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚          â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â–¼                    â–¼                    â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Knowledge Distillation                   â”‚   â”‚
â”‚  â”‚         (KL Divergence + Feature Matching)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                   â”‚
â”‚                          â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Pruning                            â”‚   â”‚
â”‚  â”‚     (Magnitude / Gradient / Taylor / Structured)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Experiments

### Medical LLM Evolution

Evolve a medical question-answering model on PubMedQA:

```bash
cd experiments/llm_medical
python run_evolution.py --config config.yaml
```

### TTS Voice Evolution

Evolve TTS style tokens for voice customization:

```bash
cd experiments/tts_voice
python run_evolution.py --config config.yaml
```

## Configuration

Genesis uses YAML configuration files. Key parameters:

```yaml
# Genetic Algorithm
genetic:
  population_size: 20
  generations: 50
  mutation_rate: 0.1
  crossover_rate: 0.7
  elite_size: 2

# Knowledge Distillation
distillation:
  temperature: 4.0
  alpha: 0.5  # Weight for soft vs hard targets

# Pruning
pruning:
  target_sparsity: 0.3
  pruning_method: "magnitude"
```

## Core Components

| Component | Description |
|-----------|-------------|
| `EvolutionaryOptimizer` | Main orchestrator combining all techniques |
| `Population` | Manages population of model variants |
| `Genetics` | SLERP crossover, mutation operations |
| `TeacherModel` | Large model providing soft targets |
| `StudentModel` | Smaller model being optimized |
| `LoRAManager` | Efficient LoRA adapter management |
| `KDLoss` | Knowledge distillation loss functions |
| `Pruner` | Model pruning with various strategies |

## Hardware Requirements

- **Minimum**: 1x GPU with 16GB VRAM
- **Recommended**: 2x GPUs (24GB+ each) for full dual-GPU mode
- **Optimal**: 2x A100/H100 GPUs

## Documentation

- [Installation Guide](docs/installation.md)
- [Quick Start](docs/quickstart.md)
- [Architecture Overview](docs/architecture.md)
- [API Reference](docs/api/)
- [Tutorials](docs/tutorials/)

## Citation

```bibtex
@software{genesis2024,
  title={Genesis: AI Evolution Laboratory},
  author={Genesis Team},
  year={2024},
  url={https://github.com/genesis-ai/genesis}
}
```

## License

MIT License - see [LICENSE](LICENSE) for details.

## Contributing

Contributions are welcome! Please read our contributing guidelines before submitting PRs.

## Acknowledgments

- HuggingFace Transformers team
- PyTorch team
- PEFT library developers

---

## ğŸ“ Directory Structure

```
Genesis/
â”œâ”€â”€ docs
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”œâ”€â”€ core.md
â”‚   â”‚   â”œâ”€â”€ distillation.md
â”‚   â”‚   â”œâ”€â”€ models.md
â”‚   â”‚   â””â”€â”€ tts.md
â”‚   â”œâ”€â”€ tutorials
â”‚   â”‚   â”œâ”€â”€ medical_llm.md
â”‚   â”‚   â””â”€â”€ tts_evolution.md
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ index.md
â”‚   â”œâ”€â”€ installation.md
â”‚   â””â”€â”€ quickstart.md
â”œâ”€â”€ experiments
â”‚   â”œâ”€â”€ llm_medical
â”‚   â”‚   â”œâ”€â”€ config.yaml
â”‚   â”‚   â””â”€â”€ run_evolution.py
â”‚   â””â”€â”€ tts_voice
â”‚       â”œâ”€â”€ config.yaml
â”‚       â””â”€â”€ run_evolution.py
â”œâ”€â”€ genesis
â”‚   â”œâ”€â”€ config
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ hardware.py
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ fitness.py
â”‚   â”‚   â”œâ”€â”€ genetics.py
â”‚   â”‚   â”œâ”€â”€ population.py
â”‚   â”‚   â””â”€â”€ selection.py
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â””â”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ distillation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ kd_loss.py
â”‚   â”‚   â””â”€â”€ trainer.py
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lora_manager.py
â”‚   â”‚   â”œâ”€â”€ ollama_teacher.py
â”‚   â”‚   â”œâ”€â”€ student.py
â”‚   â”‚   â””â”€â”€ teacher.py
â”‚   â”œâ”€â”€ pruning
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pruner.py
â”‚   â”‚   â””â”€â”€ saliency.py
â”‚   â”œâ”€â”€ tts
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mcd_fitness.py
â”‚   â”‚   â”œâ”€â”€ style_evolution.py
â”‚   â”‚   â””â”€â”€ tts_child.py
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ checkpointing.py
â”‚   â”‚   â”œâ”€â”€ dashboard.py
â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py
â”‚   â””â”€â”€ optimizer.py
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ download_models.py
â”‚   â”œâ”€â”€ setup_environment.sh
â”‚   â”œâ”€â”€ test_evolved_model.py
â”‚   â”œâ”€â”€ train_qwen3.py
â”‚   â””â”€â”€ watch_training.py
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_checkpointing.py
â”‚   â”œâ”€â”€ test_cli.py
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_datasets.py
â”‚   â”œâ”€â”€ test_distillation.py
â”‚   â”œâ”€â”€ test_distillation_trainer.py
â”‚   â”œâ”€â”€ test_fitness.py
â”‚   â”œâ”€â”€ test_genetics.py
â”‚   â”œâ”€â”€ test_integration.py
â”‚   â”œâ”€â”€ test_logging_utils.py
â”‚   â”œâ”€â”€ test_lora_manager.py
â”‚   â”œâ”€â”€ test_metrics.py
â”‚   â”œâ”€â”€ test_optimizer.py
â”‚   â”œâ”€â”€ test_population.py
â”‚   â”œâ”€â”€ test_pruning.py
â”‚   â”œâ”€â”€ test_qwen3_integration.py
â”‚   â”œâ”€â”€ test_selection.py
â”‚   â””â”€â”€ test_tts.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Lab-grade-update-plan.md
â”œâ”€â”€ README.md
â”œâ”€â”€ genesis_plan.md
â”œâ”€â”€ index.html
â”œâ”€â”€ pyproject.toml
â””â”€â”€ requirements.txt
```

---

## ğŸ“‹ Table of Contents

| # | Tier | File | Size | ~Tokens |
|---|------|------|------|---------|
| 1 | ğŸ“– Documentation | [README.md](#readme-md) | 7.3 KB | 1,534 |
| 2 | ğŸ“– Documentation | [docs/architecture.md](#docs-architecture-md) | 8.9 KB | 1,851 |
| 3 | âš™ï¸ Configuration & Dependencies | [pyproject.toml](#pyproject-toml) | 2.3 KB | 580 |
| 4 | âš™ï¸ Configuration & Dependencies | [requirements.txt](#requirements-txt) | 421 B | 105 |
| 5 | ğŸš€ Entry Points | [index.html](#index-html) | 54.1 KB | 13,771 |
| 6 | ğŸš€ Entry Points | [docs/index.md](#docs-index-md) | 2.4 KB | 617 |
| 7 | ğŸš€ Entry Points | [genesis/cli.py](#genesis-cli-py) | 7.8 KB | 2,008 |
| 8 | ğŸ—‚ï¸ Data Models & Types | [docs/api/models.md](#docs-api-models-md) | 3.9 KB | 991 |
| 9 | ğŸ—‚ï¸ Data Models & Types | [genesis/models/__init__.py](#genesis-models-__init__-py) | 691 B | 172 |
| 10 | ğŸ—‚ï¸ Data Models & Types | [genesis/models/lora_manager.py](#genesis-models-lora_manager-py) | 10.3 KB | 2,640 |
| 11 | ğŸ—‚ï¸ Data Models & Types | [genesis/models/ollama_teacher.py](#genesis-models-ollama_teacher-py) | 19.5 KB | 4,967 |
| 12 | ğŸ—‚ï¸ Data Models & Types | [genesis/models/student.py](#genesis-models-student-py) | 10.9 KB | 2,794 |
| 13 | ğŸ—‚ï¸ Data Models & Types | [genesis/models/teacher.py](#genesis-models-teacher-py) | 7.7 KB | 1,958 |
| 14 | ğŸ”Œ Routes & Handlers | [docs/api/core.md](#docs-api-core-md) | 3.8 KB | 968 |
| 15 | ğŸ”Œ Routes & Handlers | [docs/api/distillation.md](#docs-api-distillation-md) | 3.9 KB | 1,002 |
| 16 | ğŸ”Œ Routes & Handlers | [docs/api/tts.md](#docs-api-tts-md) | 4.6 KB | 1,175 |
| 17 | ğŸ§  Core Logic | [genesis/core/__init__.py](#genesis-core-__init__-py) | 566 B | 141 |
| 18 | ğŸ§  Core Logic | [genesis/core/fitness.py](#genesis-core-fitness-py) | 10.9 KB | 2,784 |
| 19 | ğŸ§  Core Logic | [genesis/core/genetics.py](#genesis-core-genetics-py) | 14.6 KB | 3,650 |
| 20 | ğŸ§  Core Logic | [genesis/core/population.py](#genesis-core-population-py) | 10.3 KB | 2,633 |
| 21 | ğŸ§  Core Logic | [genesis/core/selection.py](#genesis-core-selection-py) | 9.9 KB | 2,525 |
| 22 | ğŸ“¦ Source | [.gitignore](#gitignore) | 1.8 KB | 455 |
| 23 | ğŸ“¦ Source | [LICENSE](#license) | 1.0 KB | 267 |
| 24 | ğŸ“¦ Source | [Lab-grade-update-plan.md](#lab-grade-update-plan-md) | 10.8 KB | 2,757 |
| 25 | ğŸ“¦ Source | [genesis_plan.md](#genesis_plan-md) | 12.7 KB | 3,210 |
| 26 | ğŸ“¦ Source | [docs/installation.md](#docs-installation-md) | 2.5 KB | 629 |
| 27 | ğŸ“¦ Source | [docs/quickstart.md](#docs-quickstart-md) | 4.1 KB | 1,042 |
| 28 | ğŸ“¦ Source | [genesis/__init__.py](#genesis-__init__-py) | 830 B | 207 |
| 29 | ğŸ“¦ Source | [genesis/optimizer.py](#genesis-optimizer-py) | 19.7 KB | 5,005 |
| 30 | ğŸ“¦ Source | [docs/tutorials/medical_llm.md](#docs-tutorials-medical_llm-md) | 5.2 KB | 1,337 |
| 31 | ğŸ“¦ Source | [docs/tutorials/tts_evolution.md](#docs-tutorials-tts_evolution-md) | 6.7 KB | 1,720 |
| 32 | ğŸ“¦ Source | [experiments/llm_medical/run_evolution.py](#experiments-llm_medical-run_evolution-py) | 5.9 KB | 1,513 |
| 33 | ğŸ“¦ Source | [experiments/tts_voice/run_evolution.py](#experiments-tts_voice-run_evolution-py) | 7.9 KB | 2,032 |
| 34 | ğŸ“¦ Source | [genesis/data/__init__.py](#genesis-data-__init__-py) | 802 B | 200 |
| 35 | ğŸ“¦ Source | [genesis/data/datasets.py](#genesis-data-datasets-py) | 11.3 KB | 2,893 |
| 36 | ğŸ“¦ Source | [genesis/data/preprocessing.py](#genesis-data-preprocessing-py) | 10.0 KB | 2,572 |
| 37 | ğŸ“¦ Source | [genesis/distillation/__init__.py](#genesis-distillation-__init__-py) | 709 B | 177 |
| 38 | ğŸ“¦ Source | [genesis/distillation/kd_loss.py](#genesis-distillation-kd_loss-py) | 14.1 KB | 3,611 |
| 39 | ğŸ“¦ Source | [genesis/distillation/trainer.py](#genesis-distillation-trainer-py) | 15.7 KB | 4,016 |
| 40 | ğŸ“¦ Source | [genesis/pruning/__init__.py](#genesis-pruning-__init__-py) | 333 B | 83 |
| 41 | ğŸ“¦ Source | [genesis/pruning/pruner.py](#genesis-pruning-pruner-py) | 17.7 KB | 4,504 |
| 42 | ğŸ“¦ Source | [genesis/pruning/saliency.py](#genesis-pruning-saliency-py) | 9.5 KB | 2,427 |
| 43 | ğŸ“¦ Source | [genesis/tts/__init__.py](#genesis-tts-__init__-py) | 347 B | 86 |
| 44 | ğŸ“¦ Source | [genesis/tts/mcd_fitness.py](#genesis-tts-mcd_fitness-py) | 8.9 KB | 2,287 |
| 45 | ğŸ“¦ Source | [genesis/tts/style_evolution.py](#genesis-tts-style_evolution-py) | 10.6 KB | 2,722 |
| 46 | ğŸ“¦ Source | [genesis/tts/tts_child.py](#genesis-tts-tts_child-py) | 10.7 KB | 2,742 |
| 47 | ğŸ”§ Utilities | [genesis/utils/__init__.py](#genesis-utils-__init__-py) | 521 B | 130 |
| 48 | ğŸ”§ Utilities | [genesis/utils/checkpointing.py](#genesis-utils-checkpointing-py) | 10.5 KB | 2,696 |
| 49 | ğŸ”§ Utilities | [genesis/utils/dashboard.py](#genesis-utils-dashboard-py) | 9.4 KB | 2,398 |
| 50 | ğŸ”§ Utilities | [genesis/utils/logging.py](#genesis-utils-logging-py) | 7.5 KB | 1,915 |
| 51 | ğŸ”§ Utilities | [genesis/utils/metrics.py](#genesis-utils-metrics-py) | 10.7 KB | 2,750 |
| 52 | ğŸ”© Config Internals | [experiments/llm_medical/config.yaml](#experiments-llm_medical-config-yaml) | 1.6 KB | 421 |
| 53 | ğŸ”© Config Internals | [experiments/tts_voice/config.yaml](#experiments-tts_voice-config-yaml) | 1.3 KB | 326 |
| 54 | ğŸ”© Config Internals | [genesis/config/__init__.py](#genesis-config-__init__-py) | 310 B | 77 |
| 55 | ğŸ”© Config Internals | [genesis/config/hardware.py](#genesis-config-hardware-py) | 8.2 KB | 2,093 |
| 56 | ğŸ”© Config Internals | [genesis/config/settings.py](#genesis-config-settings-py) | 8.5 KB | 2,163 |
| 57 | ğŸ“œ Scripts & Migrations | [scripts/download_models.py](#scripts-download_models-py) | 6.4 KB | 1,626 |
| 58 | ğŸ“œ Scripts & Migrations | [scripts/setup_environment.sh](#scripts-setup_environment-sh) | 2.8 KB | 717 |
| 59 | ğŸ“œ Scripts & Migrations | [scripts/train_qwen3.py](#scripts-train_qwen3-py) | 16.6 KB | 4,195 |
| 60 | ğŸ“œ Scripts & Migrations | [scripts/watch_training.py](#scripts-watch_training-py) | 10.8 KB | 2,534 |
| 61 | ğŸ§ª Tests | [scripts/test_evolved_model.py](#scripts-test_evolved_model-py) | 12.0 KB | 2,917 |
| 62 | ğŸ§ª Tests | [tests/__init__.py](#tests-__init__-py) | 25 B | 6 |
| 63 | ğŸ§ª Tests | [tests/test_checkpointing.py](#tests-test_checkpointing-py) | 6.9 KB | 1,674 |
| 64 | ğŸ§ª Tests | [tests/test_cli.py](#tests-test_cli-py) | 7.4 KB | 1,697 |
| 65 | ğŸ§ª Tests | [tests/test_config.py](#tests-test_config-py) | 8.9 KB | 2,080 |
| 66 | ğŸ§ª Tests | [tests/test_datasets.py](#tests-test_datasets-py) | 14.8 KB | 3,615 |
| 67 | ğŸ§ª Tests | [tests/test_distillation.py](#tests-test_distillation-py) | 7.7 KB | 1,965 |
| 68 | ğŸ§ª Tests | [tests/test_distillation_trainer.py](#tests-test_distillation_trainer-py) | 11.9 KB | 2,867 |
| 69 | ğŸ§ª Tests | [tests/test_fitness.py](#tests-test_fitness-py) | 14.1 KB | 3,603 |
| 70 | ğŸ§ª Tests | [tests/test_genetics.py](#tests-test_genetics-py) | 7.8 KB | 2,002 |
| 71 | ğŸ§ª Tests | [tests/test_integration.py](#tests-test_integration-py) | 11.9 KB | 2,872 |
| 72 | ğŸ§ª Tests | [tests/test_logging_utils.py](#tests-test_logging_utils-py) | 6.5 KB | 1,482 |
| 73 | ğŸ§ª Tests | [tests/test_lora_manager.py](#tests-test_lora_manager-py) | 9.7 KB | 2,374 |
| 74 | ğŸ§ª Tests | [tests/test_metrics.py](#tests-test_metrics-py) | 9.5 KB | 2,234 |
| 75 | ğŸ§ª Tests | [tests/test_optimizer.py](#tests-test_optimizer-py) | 15.6 KB | 3,729 |
| 76 | ğŸ§ª Tests | [tests/test_population.py](#tests-test_population-py) | 7.8 KB | 1,997 |
| 77 | ğŸ§ª Tests | [tests/test_pruning.py](#tests-test_pruning-py) | 12.5 KB | 3,071 |
| 78 | ğŸ§ª Tests | [tests/test_qwen3_integration.py](#tests-test_qwen3_integration-py) | 13.1 KB | 3,139 |
| 79 | ğŸ§ª Tests | [tests/test_selection.py](#tests-test_selection-py) | 9.9 KB | 2,245 |
| 80 | ğŸ§ª Tests | [tests/test_tts.py](#tests-test_tts-py) | 19.6 KB | 4,759 |

---

## ğŸ“„ Files


### ğŸ“– Documentation

<a id="readme-md"></a>

#### `README.md`
*7445 bytes Â· ~1,534 tokens*

```markdown
# Genesis AI Evolution Laboratory

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.9+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
</p>

Genesis is a cutting-edge framework for creating efficient AI models through evolutionary algorithms, knowledge distillation, and model pruning. Designed for dual-GPU acceleration, it evolves neural networks to achieve optimal performance with minimal computational resources.

## Features

- **Evolutionary Optimization**: SLERP-based genetic operations for smooth model weight interpolation
- **Knowledge Distillation**: Transfer knowledge from large teacher models to smaller students
- **Model Pruning**: Intelligent weight pruning with multiple saliency methods
- **Dual-GPU Support**: Parallel execution with teacher on GPU 0 and student on GPU 1
- **LoRA Integration**: Efficient fine-tuning with Low-Rank Adaptation
- **TTS Evolution**: Specialized support for Text-to-Speech style token evolution
- **Extensible Design**: Modular architecture for custom experiments

## Installation

```bash
# Clone the repository
git clone https://github.com/genesis-ai/genesis.git
cd genesis

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Genesis in development mode
pip install -e .
```

## Quick Start

```python
from genesis import EvolutionaryOptimizer, GenesisConfig

# Create configuration
config = GenesisConfig(
    teacher_model="meta-llama/Llama-2-7b-hf",
    use_lora=True,
    genetic=dict(
        population_size=20,
        generations=50,
        mutation_rate=0.1,
    ),
)

# Create optimizer
optimizer = EvolutionaryOptimizer(
    config=config,
    train_dataloader=train_loader,
    eval_dataloader=eval_loader,
)

# Run evolution
results = optimizer.run()
print(f"Best fitness: {results['best_fitness']}")

# Optional: Prune and distill
optimizer.prune_model(target_sparsity=0.3)
optimizer.distill(num_steps=1000)
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Genesis Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Teacher    â”‚     â”‚  Population  â”‚     â”‚   Student    â”‚ â”‚
â”‚  â”‚   (GPU 0)    â”‚â”€â”€â”€â”€â–¶â”‚   Manager    â”‚â”€â”€â”€â”€â–¶â”‚   (GPU 1)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â”‚                    â–¼                    â”‚          â”‚
â”‚         â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚          â”‚
â”‚         â”‚           â”‚   Genetics   â”‚              â”‚          â”‚
â”‚         â”‚           â”‚  (SLERP +    â”‚              â”‚          â”‚
â”‚         â”‚           â”‚  Mutation)   â”‚              â”‚          â”‚
â”‚         â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚          â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â–¼                    â–¼                    â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Knowledge Distillation                   â”‚   â”‚
â”‚  â”‚         (KL Divergence + Feature Matching)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                   â”‚
â”‚                          â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Pruning                            â”‚   â”‚
â”‚  â”‚     (Magnitude / Gradient / Taylor / Structured)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Experiments

### Medical LLM Evolution

Evolve a medical question-answering model on PubMedQA:

```bash
cd experiments/llm_medical
python run_evolution.py --config config.yaml
```

### TTS Voice Evolution

Evolve TTS style tokens for voice customization:

```bash
cd experiments/tts_voice
python run_evolution.py --config config.yaml
```

## Configuration

Genesis uses YAML configuration files. Key parameters:

```yaml
# Genetic Algorithm
genetic:
  population_size: 20
  generations: 50
  mutation_rate: 0.1
  crossover_rate: 0.7
  elite_size: 2

# Knowledge Distillation
distillation:
  temperature: 4.0
  alpha: 0.5  # Weight for soft vs hard targets

# Pruning
pruning:
  target_sparsity: 0.3
  pruning_method: "magnitude"
```

## Core Components

| Component | Description |
|-----------|-------------|
| `EvolutionaryOptimizer` | Main orchestrator combining all techniques |
| `Population` | Manages population of model variants |
| `Genetics` | SLERP crossover, mutation operations |
| `TeacherModel` | Large model providing soft targets |
| `StudentModel` | Smaller model being optimized |
| `LoRAManager` | Efficient LoRA adapter management |
| `KDLoss` | Knowledge distillation loss functions |
| `Pruner` | Model pruning with various strategies |

## Hardware Requirements

- **Minimum**: 1x GPU with 16GB VRAM
- **Recommended**: 2x GPUs (24GB+ each) for full dual-GPU mode
- **Optimal**: 2x A100/H100 GPUs

## Documentation

- [Installation Guide](docs/installation.md)
- [Quick Start](docs/quickstart.md)
- [Architecture Overview](docs/architecture.md)
- [API Reference](docs/api/)
- [Tutorials](docs/tutorials/)

## Citation

```bibtex
@software{genesis2024,
  title={Genesis: AI Evolution Laboratory},
  author={Genesis Team},
  year={2024},
  url={https://github.com/genesis-ai/genesis}
}
```

## License

MIT License - see [LICENSE](LICENSE) for details.

## Contributing

Contributions are welcome! Please read our contributing guidelines before submitting PRs.

## Acknowledgments

- HuggingFace Transformers team
- PyTorch team
- PEFT library developers
```

<a id="docs-architecture-md"></a>

#### `docs/architecture.md`
*9140 bytes Â· ~1,851 tokens*

```markdown
# Architecture Overview

Genesis is built with a modular architecture that separates concerns and enables flexible experimentation.

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        EvolutionaryOptimizer                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      Configuration                           â”‚    â”‚
â”‚  â”‚  GenesisConfig â”‚ GeneticConfig â”‚ DistillationConfig â”‚ etc.  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â”‚                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â–¼                      â–¼                      â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   Teacher   â”‚      â”‚ Population  â”‚      â”‚   Student   â”‚         â”‚
â”‚  â”‚   Model     â”‚      â”‚  Manager    â”‚      â”‚   Model     â”‚         â”‚
â”‚  â”‚  (GPU 0)    â”‚      â”‚             â”‚      â”‚  (GPU 1)    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                    â”‚                    â”‚                  â”‚
â”‚         â”‚                    â–¼                    â”‚                  â”‚
â”‚         â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚                  â”‚
â”‚         â”‚           â”‚  Genetics   â”‚               â”‚                  â”‚
â”‚         â”‚           â”‚  - SLERP    â”‚               â”‚                  â”‚
â”‚         â”‚           â”‚  - Mutation â”‚               â”‚                  â”‚
â”‚         â”‚           â”‚  - Select   â”‚               â”‚                  â”‚
â”‚         â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚                  â”‚
â”‚         â”‚                    â”‚                    â”‚                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                              â–¼                                       â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                    â”‚   Distillation  â”‚                              â”‚
â”‚                    â”‚    Trainer      â”‚                              â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                              â”‚                                       â”‚
â”‚                              â–¼                                       â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                    â”‚     Pruner      â”‚                              â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. EvolutionaryOptimizer

The main orchestrator that coordinates all components:

```python
class EvolutionaryOptimizer:
    def __init__(self, config, teacher_model, student_model, ...)
    def initialize(self)
    def run(self, num_generations) -> dict
    def prune_model(self, target_sparsity) -> dict
    def distill(self, num_steps) -> dict
```

### 2. Population Manager

Manages the population of model variants:

```python
class Population:
    def initialize_from_model(self, model)
    def evaluate(self, fitness_fn)
    def evolve(self)  # Selection, crossover, mutation

class Individual:
    state_dict: dict[str, Tensor]
    fitness: float
    generation: int
    parent_ids: list[str]
```

### 3. Genetics Module

Implements genetic operations:

```python
# SLERP interpolation for weight blending
def slerp(t, v0, v1) -> Tensor

# Crossover between parents
def crossover(parent1_state, parent2_state, method="slerp") -> dict

# Mutation with adaptive rates
def mutate(state_dict, mutation_rate, mutation_scale) -> dict
```

### 4. Model Wrappers

#### TeacherModel
- Runs on GPU 0 (by default)
- Frozen during training
- Provides soft targets for distillation

#### StudentModel
- Runs on GPU 1 (by default)
- Supports LoRA adapters
- Trainable weights evolved by genetic algorithm

### 5. Knowledge Distillation

```python
class KDLoss:
    def forward(self, student_logits, teacher_logits, hard_labels=None):
        # KL divergence + optional hard label loss
        # Optional feature distillation
        return {"total_loss": ..., "kd_loss": ..., "hard_loss": ...}

class DistillationTrainer:
    def train(self, num_steps)
    def evaluate(self) -> dict
```

### 6. Pruning Module

```python
class Pruner:
    def prune(self, sparsity) -> dict
    def apply_masks(self)  # Reapply after optimizer step

class SaliencyCalculator:
    def compute(self, method="magnitude") -> dict[str, Tensor]
```

## Data Flow

### Evolution Loop

```
1. Initialize population from base model
   â”‚
2. For each generation:
   â”‚
   â”œâ”€â–º Evaluate fitness of all individuals
   â”‚   â”‚
   â”‚   â””â”€â–º Load state â†’ Forward pass â†’ Compute metrics
   â”‚
   â”œâ”€â–º Select best individuals (elitism)
   â”‚
   â”œâ”€â–º Create offspring via crossover + mutation
   â”‚   â”‚
   â”‚   â”œâ”€â–º SLERP interpolation of weights
   â”‚   â””â”€â–º Gaussian noise mutation
   â”‚
   â””â”€â–º Optional: Refine best with distillation

3. Save best evolved model
```

### Distillation Flow

```
Input Batch
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                      â–¼
Teacher Model          Student Model
(GPU 0, frozen)        (GPU 1, training)
    â”‚                      â”‚
    â–¼                      â–¼
Teacher Logits         Student Logits
    â”‚                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
         KD Loss
    (KL Div + Hard Loss)
               â”‚
               â–¼
      Backprop & Update
```

## Memory Management

### Dual-GPU Strategy

```
GPU 0 (Teacher):
â”œâ”€â”€ Full model weights (frozen)
â”œâ”€â”€ Inference only
â””â”€â”€ No gradient computation

GPU 1 (Student):
â”œâ”€â”€ Base model + LoRA adapters
â”œâ”€â”€ Gradient computation
â””â”€â”€ Optimizer states
```

### Memory Optimization

1. **LoRA**: Only train low-rank adapters (< 1% of parameters)
2. **Gradient Checkpointing**: Trade compute for memory
3. **Mixed Precision**: FP16/BF16 for reduced memory
4. **Quantization**: 8-bit teacher model option

## Extensibility

### Custom Fitness Evaluator

```python
from genesis.core.fitness import FitnessEvaluator, FitnessResult

class CustomFitness(FitnessEvaluator):
    def evaluate(self, model, state_dict=None):
        # Load state
        if state_dict:
            model.load_state_dict(state_dict)

        # Your evaluation logic
        score = compute_custom_score(model)

        return FitnessResult(
            score=score,
            metrics={"custom_metric": score}
        )
```

### Custom Selection Strategy

```python
from genesis.core.selection import SelectionStrategy

class CustomSelection(SelectionStrategy):
    def select(self, population, num_to_select):
        # Your selection logic
        return selected_individuals
```

## Configuration Hierarchy

```yaml
GenesisConfig
â”œâ”€â”€ project_name, output_dir, seed
â”œâ”€â”€ teacher_model, student_model
â”œâ”€â”€ teacher_device, student_device
â”‚
â”œâ”€â”€ GeneticConfig
â”‚   â”œâ”€â”€ population_size, generations
â”‚   â”œâ”€â”€ mutation_rate, crossover_rate
â”‚   â””â”€â”€ elite_size, tournament_size
â”‚
â”œâ”€â”€ DistillationConfig
â”‚   â”œâ”€â”€ temperature, alpha
â”‚   â”œâ”€â”€ learning_rate, max_steps
â”‚   â””â”€â”€ use_feature_distillation
â”‚
â”œâ”€â”€ PruningConfig
â”‚   â”œâ”€â”€ target_sparsity
â”‚   â”œâ”€â”€ pruning_method
â”‚   â””â”€â”€ structured
â”‚
â””â”€â”€ LoRAConfig
    â”œâ”€â”€ r, lora_alpha
    â””â”€â”€ target_modules
```
```


### âš™ï¸ Configuration & Dependencies

<a id="pyproject-toml"></a>

#### `pyproject.toml`
*2322 bytes Â· ~580 tokens*

```toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "genesis-ai"
version = "0.1.0"
description = "Genesis AI Evolution Laboratory - Evolutionary algorithms for efficient AI model creation"
readme = "README.md"
license = {text = "MIT"}
requires-python = ">=3.9"
authors = [
    {name = "Genesis Team"}
]
keywords = [
    "machine-learning",
    "evolutionary-algorithms",
    "knowledge-distillation",
    "model-pruning",
    "deep-learning",
    "pytorch",
    "transformers",
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dependencies = [
    "torch>=2.0.0",
    "transformers>=4.36.0",
    "peft>=0.7.0",
    "accelerate>=0.25.0",
    "datasets>=2.16.0",
    "scipy>=1.11.0",
    "numpy>=1.24.0",
    "pyyaml>=6.0",
    "tqdm>=4.66.0",
    "tensorboard>=2.15.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.7.0",
]
tts = [
    "librosa>=0.10.0",
    "soundfile>=0.12.0",
    "pyworld>=0.3.0",
]
docs = [
    "mkdocs>=1.5.0",
    "mkdocs-material>=9.5.0",
    "mkdocstrings[python]>=0.24.0",
]

[project.urls]
Homepage = "https://github.com/genesis-ai/genesis"
Documentation = "https://genesis-ai.github.io/genesis"
Repository = "https://github.com/genesis-ai/genesis"
Issues = "https://github.com/genesis-ai/genesis/issues"

[project.scripts]
genesis = "genesis.cli:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["genesis*"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --tb=short"

[tool.black]
line-length = 100
target-version = ["py39", "py310", "py311"]

[tool.ruff]
line-length = 100
select = ["E", "F", "I", "N", "W", "UP"]
ignore = ["E501"]

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
ignore_missing_imports = true
```

<a id="requirements-txt"></a>

#### `requirements.txt`
*421 bytes Â· ~105 tokens*

```
# Genesis AI Evolution Laboratory - Dependencies

# Core ML Framework
torch>=2.0.0
transformers>=4.36.0
peft>=0.7.0
accelerate>=0.25.0
datasets>=2.16.0

# Scientific Computing
scipy>=1.11.0
numpy>=1.24.0

# Configuration & Utilities
pyyaml>=6.0
tqdm>=4.66.0

# Logging & Visualization
tensorboard>=2.15.0

# Testing
pytest>=7.4.0
pytest-cov>=4.1.0

# TTS (Optional)
# librosa>=0.10.0
# soundfile>=0.12.0
# pyworld>=0.3.0
```


### ğŸš€ Entry Points

<a id="index-html"></a>

#### `index.html`
*55408 bytes Â· ~13,771 tokens*

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Genesis - AI Evolution Laboratory</title>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Inter:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary: #00f5ff;
            --secondary: #bf00ff;
            --accent: #ff00aa;
            --green: #00ff88;
            --orange: #ff8800;
            --dark: #0a0a15;
            --darker: #050508;
            --glass: rgba(255, 255, 255, 0.04);
            --glass-border: rgba(255, 255, 255, 0.1);
        }

        * { margin: 0; padding: 0; box-sizing: border-box; }

        body {
            font-family: 'Inter', sans-serif;
            background: var(--darker);
            color: #fff;
            overflow-x: hidden;
            line-height: 1.6;
        }

        /* â”€â”€ Background â”€â”€ */
        .bg-animation {
            position: fixed; top: 0; left: 0;
            width: 100%; height: 100%; z-index: -1; overflow: hidden;
        }
        .bg-animation::before {
            content: '';
            position: absolute; top: -50%; left: -50%;
            width: 200%; height: 200%;
            background:
                radial-gradient(circle at 20% 80%, rgba(0, 245, 255, 0.07) 0%, transparent 50%),
                radial-gradient(circle at 80% 20%, rgba(191, 0, 255, 0.07) 0%, transparent 50%),
                radial-gradient(circle at 40% 40%, rgba(255, 0, 170, 0.04) 0%, transparent 40%);
            animation: bgRotate 30s linear infinite;
        }
        @keyframes bgRotate {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .neural-bg { position: fixed; top: 0; left: 0; width: 100%; height: 100%; z-index: -1; opacity: 0.08; }
        .neural-node {
            position: absolute; width: 4px; height: 4px;
            background: var(--primary); border-radius: 50%;
            animation: pulse 2s ease-in-out infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 0.3; transform: scale(1); }
            50% { opacity: 1; transform: scale(1.5); }
        }

        /* â”€â”€ Header â”€â”€ */
        header {
            position: fixed; top: 0; width: 100%;
            padding: 1.2rem 5%;
            display: flex; justify-content: space-between; align-items: center;
            z-index: 1000;
            background: rgba(5, 5, 8, 0.85);
            backdrop-filter: blur(20px);
            border-bottom: 1px solid var(--glass-border);
        }
        .logo {
            font-family: 'Orbitron', sans-serif; font-size: 1.6rem; font-weight: 900;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text; -webkit-text-fill-color: transparent;
        }
        nav a {
            color: rgba(255,255,255,0.75); text-decoration: none;
            margin-left: 1.8rem; font-size: 0.88rem; font-weight: 500;
            transition: all 0.3s ease; position: relative;
        }
        nav a::after {
            content: ''; position: absolute; bottom: -4px; left: 0;
            width: 0; height: 2px; background: var(--primary); transition: width 0.3s ease;
        }
        nav a:hover::after { width: 100%; }
        nav a:hover { color: var(--primary); }

        /* â”€â”€ Shared â”€â”€ */
        .glass-card {
            background: var(--glass);
            backdrop-filter: blur(20px);
            border: 1px solid var(--glass-border);
            border-radius: 16px; padding: 2rem;
            transition: all 0.3s ease;
        }
        .glass-card:hover {
            border-color: rgba(0, 245, 255, 0.3);
            box-shadow: 0 0 40px rgba(0, 245, 255, 0.06);
            transform: translateY(-3px);
        }
        section { padding: 6rem 5%; }
        .section-header { text-align: center; margin-bottom: 3.5rem; }
        .section-header h2 {
            font-family: 'Orbitron', sans-serif; font-size: 2.2rem; margin-bottom: 0.8rem;
            background: linear-gradient(135deg, #fff, var(--primary));
            -webkit-background-clip: text; -webkit-text-fill-color: transparent;
        }
        .section-header p { color: rgba(255,255,255,0.55); max-width: 620px; margin: 0 auto; }

        .tag {
            display: inline-block; padding: 0.25rem 0.75rem; border-radius: 50px;
            font-size: 0.72rem; font-weight: 600; letter-spacing: 0.05em;
            text-transform: uppercase;
        }
        .tag-done { background: rgba(0,255,136,0.12); color: var(--green); border: 1px solid rgba(0,255,136,0.3); }
        .tag-wip  { background: rgba(255,136,0,0.12); color: var(--orange); border: 1px solid rgba(255,136,0,0.3); }
        .tag-next { background: rgba(191,0,255,0.12); color: #d070ff; border: 1px solid rgba(191,0,255,0.3); }

        /* â”€â”€ Hero â”€â”€ */
        .hero {
            min-height: 100vh; display: flex; align-items: center;
            justify-content: center; padding: 9rem 5% 5rem; position: relative;
        }
        .hero-content { text-align: center; max-width: 900px; }
        .hero h1 {
            font-family: 'Orbitron', sans-serif;
            font-size: clamp(2.2rem, 7vw, 4.5rem);
            font-weight: 900; margin-bottom: 1.5rem; line-height: 1.1;
        }
        .hero h1 span {
            background: linear-gradient(135deg, var(--primary), var(--secondary), var(--accent));
            -webkit-background-clip: text; -webkit-text-fill-color: transparent;
            animation: gradientShift 5s ease infinite; background-size: 200% 200%;
        }
        @keyframes gradientShift {
            0%, 100% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
        }
        .hero p { font-size: 1.15rem; color: rgba(255,255,255,0.65); margin-bottom: 2.5rem; max-width: 700px; margin-left: auto; margin-right: auto; }

        /* live stat badges in hero */
        .hero-stats {
            display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap;
            margin-bottom: 2.5rem;
        }
        .hero-stat {
            background: var(--glass); border: 1px solid var(--glass-border);
            border-radius: 12px; padding: 1rem 1.8rem; text-align: center;
        }
        .hero-stat .val {
            font-family: 'Orbitron', sans-serif; font-size: 1.8rem;
            font-weight: 700; color: var(--primary);
        }
        .hero-stat .lbl { font-size: 0.78rem; color: rgba(255,255,255,0.5); margin-top: 0.2rem; }

        .cta-buttons { display: flex; gap: 1.2rem; justify-content: center; flex-wrap: wrap; }
        .btn {
            padding: 0.9rem 2.2rem; border-radius: 50px; font-weight: 600;
            text-decoration: none; transition: all 0.3s ease; font-size: 0.95rem;
            display: inline-flex; align-items: center; gap: 0.5rem;
        }
        .btn-primary {
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: var(--darker); box-shadow: 0 0 30px rgba(0,245,255,0.25);
        }
        .btn-primary:hover { transform: translateY(-3px); box-shadow: 0 0 50px rgba(0,245,255,0.45); }
        .btn-secondary {
            background: transparent; color: #fff;
            border: 1px solid var(--glass-border);
        }
        .btn-secondary:hover { border-color: var(--primary); color: var(--primary); }

        /* â”€â”€ Results Banner â”€â”€ */
        .results-banner {
            background: linear-gradient(135deg, rgba(0,245,255,0.05), rgba(191,0,255,0.05));
            border-top: 1px solid rgba(0,245,255,0.15);
            border-bottom: 1px solid rgba(191,0,255,0.15);
            padding: 3rem 5%;
        }
        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
            gap: 2rem; max-width: 1100px; margin: 0 auto;
            text-align: center;
        }
        .result-item .number {
            font-family: 'Orbitron', sans-serif; font-size: 2.5rem; font-weight: 900;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text; -webkit-text-fill-color: transparent;
        }
        .result-item .label {
            font-size: 0.82rem; color: rgba(255,255,255,0.5);
            margin-top: 0.3rem; text-transform: uppercase; letter-spacing: 0.05em;
        }
        .result-item .sub {
            font-size: 0.75rem; color: rgba(255,255,255,0.3); margin-top: 0.15rem;
        }

        /* â”€â”€ Pipeline â”€â”€ */
        .pipeline {
            display: flex; flex-direction: column; gap: 0;
            max-width: 900px; margin: 0 auto;
        }
        .pipeline-step {
            display: grid; grid-template-columns: 60px 1fr;
            gap: 1.5rem; position: relative;
        }
        .pipeline-step:not(:last-child) .step-line {
            position: absolute; left: 30px; top: 60px; bottom: -1px;
            width: 2px;
            background: linear-gradient(180deg, var(--primary), var(--secondary));
        }
        .step-num {
            width: 60px; height: 60px; border-radius: 50%;
            background: var(--glass); border: 2px solid var(--primary);
            display: flex; align-items: center; justify-content: center;
            font-family: 'Orbitron', sans-serif; font-size: 1.1rem; font-weight: 700;
            color: var(--primary); flex-shrink: 0;
            box-shadow: 0 0 20px rgba(0,245,255,0.2);
        }
        .step-body { padding-bottom: 2.5rem; }
        .step-body h3 {
            font-family: 'Orbitron', sans-serif; font-size: 1.1rem;
            color: var(--primary); margin-bottom: 0.6rem;
            display: flex; align-items: center; gap: 0.8rem;
        }
        .step-body p { color: rgba(255,255,255,0.65); font-size: 0.92rem; margin-bottom: 0.6rem; }
        .step-body .insight {
            background: rgba(0,245,255,0.05); border-left: 3px solid var(--primary);
            padding: 0.6rem 1rem; border-radius: 0 8px 8px 0;
            font-size: 0.82rem; color: rgba(255,255,255,0.55);
            font-family: 'JetBrains Mono', monospace; margin-top: 0.6rem;
        }
        .step-body .warning {
            background: rgba(255,136,0,0.07); border-left: 3px solid var(--orange);
            padding: 0.6rem 1rem; border-radius: 0 8px 8px 0;
            font-size: 0.82rem; color: rgba(255,200,100,0.8);
            margin-top: 0.6rem;
        }

        /* â”€â”€ Architecture â”€â”€ */
        .architecture {
            background: linear-gradient(180deg, transparent, rgba(0,245,255,0.02), transparent);
        }
        .arch-diagram { max-width: 1000px; margin: 0 auto; }
        .gpu-container {
            display: grid; grid-template-columns: 1fr auto 1fr;
            gap: 2rem; align-items: center; margin-bottom: 2rem;
        }
        .gpu-box {
            border-radius: 15px; padding: 2rem; text-align: center;
            background: var(--glass); border: 1px solid var(--glass-border);
        }
        .gpu-box h3 { font-family: 'Orbitron', sans-serif; margin-bottom: 1rem; font-size: 0.95rem; }
        .gpu-box.teacher { border-color: var(--primary); box-shadow: 0 0 30px rgba(0,245,255,0.08); }
        .gpu-box.teacher h3 { color: var(--primary); }
        .gpu-box.student { border-color: var(--secondary); box-shadow: 0 0 30px rgba(191,0,255,0.08); }
        .gpu-box.student h3 { color: #d070ff; }
        .gpu-box p { font-size: 0.82rem; color: rgba(255,255,255,0.55); margin: 0.25rem 0; }
        .gpu-mem {
            margin-top: 1rem; padding: 0.5rem; border-radius: 8px;
            background: rgba(0,0,0,0.3); font-family: 'JetBrains Mono', monospace;
            font-size: 0.75rem; color: rgba(255,255,255,0.4);
        }
        .arrow-container { display: flex; flex-direction: column; align-items: center; gap: 0.8rem; }
        .arrow {
            width: 60px; height: 2px;
            background: linear-gradient(90deg, var(--primary), var(--secondary));
            position: relative;
        }
        .arrow::after {
            content: ''; position: absolute; right: -8px; top: -4px;
            border: 5px solid transparent; border-left-color: var(--secondary);
        }
        .arrow-label { color: rgba(255,255,255,0.4); font-size: 0.72rem; text-align: center; }

        .arch-note {
            max-width: 1000px; margin: 1.5rem auto 0;
            background: rgba(255,136,0,0.06); border: 1px solid rgba(255,136,0,0.2);
            border-radius: 10px; padding: 1rem 1.5rem;
            font-size: 0.83rem; color: rgba(255,200,100,0.75);
            display: flex; align-items: flex-start; gap: 0.8rem;
        }
        .arch-note span { flex-shrink: 0; font-size: 1rem; }

        /* â”€â”€ Results Table â”€â”€ */
        .runs-table { width: 100%; border-collapse: collapse; max-width: 900px; margin: 0 auto; }
        .runs-table th {
            font-family: 'Orbitron', sans-serif; font-size: 0.75rem; font-weight: 600;
            color: var(--primary); text-align: left; padding: 0.8rem 1.2rem;
            border-bottom: 1px solid rgba(0,245,255,0.2);
            text-transform: uppercase; letter-spacing: 0.05em;
        }
        .runs-table td {
            padding: 0.9rem 1.2rem; font-size: 0.88rem;
            border-bottom: 1px solid rgba(255,255,255,0.05);
            color: rgba(255,255,255,0.75);
            font-family: 'JetBrains Mono', monospace;
        }
        .runs-table tr:hover td { background: rgba(0,245,255,0.03); }
        .val-good { color: var(--green); }
        .val-base { color: rgba(255,255,255,0.45); }

        /* â”€â”€ Tech Grid â”€â”€ */
        .tech-grid {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
            gap: 1.5rem; max-width: 1200px; margin: 0 auto;
        }
        .tech-card { text-align: center; padding: 2rem; }
        .tech-icon { font-size: 2.5rem; margin-bottom: 0.8rem; display: block; }
        .tech-card h3 { font-family: 'Orbitron', sans-serif; font-size: 0.95rem; margin-bottom: 0.5rem; color: var(--primary); }
        .tech-card p { font-size: 0.85rem; color: rgba(255,255,255,0.55); }

        /* â”€â”€ Fixes / Insights â”€â”€ */
        .fixes-grid {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 1.5rem; max-width: 1200px; margin: 0 auto;
        }
        .fix-card { padding: 1.5rem; }
        .fix-card .fix-title {
            font-family: 'Orbitron', sans-serif; font-size: 0.9rem;
            color: var(--green); margin-bottom: 0.5rem;
            display: flex; align-items: center; gap: 0.6rem;
        }
        .fix-card .fix-title .badge {
            font-size: 0.65rem; padding: 0.15rem 0.5rem;
            background: rgba(0,255,136,0.12); border-radius: 4px;
            color: var(--green); text-transform: uppercase;
        }
        .fix-card p { font-size: 0.85rem; color: rgba(255,255,255,0.55); }
        .fix-card code {
            display: block; margin-top: 0.6rem;
            background: rgba(0,0,0,0.4); border-radius: 6px;
            padding: 0.6rem 0.9rem; font-family: 'JetBrains Mono', monospace;
            font-size: 0.75rem; color: rgba(0,245,255,0.7);
            border-left: 2px solid rgba(0,245,255,0.3);
            white-space: pre-wrap; word-break: break-all;
        }

        /* â”€â”€ Hardware Specs â”€â”€ */
        .specs-grid {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 1.5rem; max-width: 1000px; margin: 0 auto;
        }
        .spec-item { display: flex; align-items: center; gap: 1rem; padding: 1.5rem; }
        .spec-icon { font-size: 2rem; color: var(--primary); }
        .spec-info h4 { font-family: 'Orbitron', sans-serif; font-size: 0.85rem; margin-bottom: 0.25rem; }
        .spec-info p { color: rgba(255,255,255,0.55); font-size: 0.82rem; }

        /* â”€â”€ Roadmap â”€â”€ */
        .roadmap-grid {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 1.5rem; max-width: 1200px; margin: 0 auto 3rem;
        }
        .roadmap-card { padding: 1.8rem; }
        .roadmap-card .phase {
            font-family: 'Orbitron', sans-serif; font-size: 0.7rem;
            color: rgba(255,255,255,0.3); text-transform: uppercase;
            letter-spacing: 0.08em; margin-bottom: 0.4rem;
        }
        .roadmap-card h3 { font-family: 'Orbitron', sans-serif; font-size: 1rem; margin-bottom: 0.8rem; }
        .roadmap-card h3.done { color: var(--green); }
        .roadmap-card h3.wip  { color: var(--orange); }
        .roadmap-card h3.next { color: #d070ff; }
        .roadmap-card ul { list-style: none; }
        .roadmap-card li {
            padding: 0.35rem 0; font-size: 0.85rem;
            color: rgba(255,255,255,0.55);
            display: flex; align-items: flex-start; gap: 0.5rem;
        }
        .roadmap-card li::before { content: 'â€º'; color: inherit; flex-shrink: 0; }
        .roadmap-card li.done { color: rgba(0,255,136,0.75); }
        .roadmap-card li.done::before { content: 'âœ“'; }
        .roadmap-card li.wip  { color: rgba(255,180,80,0.8); }
        .roadmap-card li.wip::before  { content: 'âŸ³'; }

        /* â”€â”€ Next Steps â”€â”€ */
        .next-steps-grid {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 1.5rem; max-width: 1200px; margin: 0 auto;
        }
        .next-card { padding: 1.8rem; }
        .next-card .priority {
            font-size: 0.7rem; text-transform: uppercase; letter-spacing: 0.08em;
            margin-bottom: 0.6rem;
        }
        .next-card h3 { font-family: 'Orbitron', sans-serif; font-size: 1rem; color: #d070ff; margin-bottom: 0.7rem; }
        .next-card p { font-size: 0.85rem; color: rgba(255,255,255,0.55); }
        .next-card .impact {
            margin-top: 1rem; font-size: 0.78rem;
            color: rgba(0,245,255,0.6);
            font-family: 'JetBrains Mono', monospace;
        }

        /* â”€â”€ Status legend â”€â”€ */
        .status-grid { display: flex; justify-content: center; gap: 2.5rem; flex-wrap: wrap; margin-top: 2.5rem; }
        .status-item { text-align: center; font-size: 0.85rem; display: flex; align-items: center; gap: 0.5rem; }
        .status-dot {
            width: 10px; height: 10px; border-radius: 50%;
            display: inline-block; animation: statusPulse 2s ease-in-out infinite;
        }
        .status-dot.active { background: var(--green); box-shadow: 0 0 10px var(--green); }
        .status-dot.wip    { background: var(--orange); box-shadow: 0 0 10px var(--orange); }
        .status-dot.next   { background: #d070ff; box-shadow: 0 0 10px #d070ff; }
        @keyframes statusPulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.4; } }

        /* â”€â”€ Footer â”€â”€ */
        footer { padding: 4rem 5% 2rem; border-top: 1px solid var(--glass-border); text-align: center; }
        .footer-links { display: flex; justify-content: center; gap: 2rem; margin-bottom: 2rem; flex-wrap: wrap; }
        .footer-links a { color: rgba(255,255,255,0.5); text-decoration: none; transition: color 0.3s ease; font-size: 0.88rem; }
        .footer-links a:hover { color: var(--primary); }
        .footer-copyright { color: rgba(255,255,255,0.3); font-size: 0.82rem; }

        /* â”€â”€ Responsive â”€â”€ */
        @media (max-width: 768px) {
            nav { display: none; }
            .gpu-container { grid-template-columns: 1fr; }
            .arrow-container { transform: rotate(90deg); }
        }

        /* â”€â”€ Scrollbar â”€â”€ */
        ::-webkit-scrollbar { width: 6px; }
        ::-webkit-scrollbar-track { background: var(--darker); }
        ::-webkit-scrollbar-thumb {
            background: linear-gradient(180deg, var(--primary), var(--secondary));
            border-radius: 3px;
        }
    </style>
</head>
<body>
    <div class="bg-animation"></div>
    <div class="neural-bg" id="neuralBg"></div>

    <!-- Header -->
    <header>
        <div class="logo">GENESIS</div>
        <nav>
            <a href="#pipeline">Pipeline</a>
            <a href="#architecture">Architecture</a>
            <a href="#results">Results</a>
            <a href="#fixes">Internals</a>
            <a href="#roadmap">Roadmap</a>
            <a href="#next">Next Steps</a>
            <a href="https://github.com/genesis-ai/genesis">GitHub</a>
        </nav>
    </header>

    <!-- Hero -->
    <section class="hero">
        <div class="hero-content">
            <h1><span>AI Evolution Laboratory</span></h1>
            <p>Evolutionary optimization of language model LoRA adapters through knowledge distillation,
            genetic crossover, and tournament selection â€” achieving measurable perplexity reduction
            on a 1.7B-parameter student model.</p>

            <div class="hero-stats">
                <div class="hero-stat">
                    <div class="val">âˆ’48%</div>
                    <div class="lbl">Perplexity Reduction</div>
                </div>
                <div class="hero-stat">
                    <div class="val">154â†’80</div>
                    <div class="lbl">PPL (Base â†’ Evolved)</div>
                </div>
                <div class="hero-stat">
                    <div class="val">0.0123</div>
                    <div class="lbl">Best Fitness Score</div>
                </div>
                <div class="hero-stat">
                    <div class="val">20</div>
                    <div class="lbl">Evolution Generations</div>
                </div>
            </div>

            <div class="cta-buttons">
                <a href="#pipeline" class="btn btn-primary">How It Works</a>
                <a href="#results" class="btn btn-secondary">View Results</a>
            </div>
        </div>
    </section>

    <!-- Results Banner -->
    <div class="results-banner">
        <div class="results-grid">
            <div class="result-item">
                <div class="number">Qwen3-1.7B</div>
                <div class="label">Student Model</div>
                <div class="sub">LoRA r=16, Î±=32</div>
            </div>
            <div class="result-item">
                <div class="number">qwen3.5</div>
                <div class="label">Teacher via Ollama</div>
                <div class="sub">Remote API, hard-label CE</div>
            </div>
            <div class="result-item">
                <div class="number">200</div>
                <div class="label">Distillation Steps</div>
                <div class="sub">Best eval loss 4.6465</div>
            </div>
            <div class="result-item">
                <div class="number">6</div>
                <div class="label">Population Size</div>
                <div class="sub">Elite 2, mutation 30%</div>
            </div>
            <div class="result-item">
                <div class="number">25 MB</div>
                <div class="label">Adapter Size</div>
                <div class="sub">q/k/v/o_proj, float32</div>
            </div>
            <div class="result-item">
                <div class="number">~80 PPL</div>
                <div class="label">Best Evolved PPL</div>
                <div class="sub">vs 154 base model</div>
            </div>
        </div>
    </div>

    <!-- Pipeline -->
    <section id="pipeline">
        <div class="section-header">
            <h2>How It Works</h2>
            <p>A two-phase training pipeline: knowledge distillation seeds the population,
            then evolutionary algorithms refine LoRA adapter weights across generations.</p>
        </div>

        <div class="pipeline">

            <div class="pipeline-step">
                <div class="step-line"></div>
                <div class="step-num">1</div>
                <div class="step-body glass-card">
                    <h3>Teacher Setup <span class="tag tag-done">Done</span></h3>
                    <p>An <strong>OllamaTeacher</strong> connects to a remote Ollama server and probes
                    whether the endpoint supports per-token logprobs. If supported, full soft-target
                    Knowledge Distillation (KL divergence + cross-entropy) is used. If not â€” as is
                    currently the case with Ollama's <code>/v1/completions</code> API â€” the trainer
                    automatically falls back to hard-label cross-entropy only.</p>
                    <div class="warning">
                        âš  Ollama's API returns <code>top_logprobs entries: 0</code> regardless of the
                        <code>logprobs=True</code> parameter. Without logprobs, soft targets are all uniform
                        â€” training toward them actively corrupts the student. The fallback is mandatory, not optional.
                    </div>
                    <div class="insight">logprobs probe â†’ NOT SUPPORTED â†’ hard-label CE only</div>
                </div>
            </div>

            <div class="pipeline-step">
                <div class="step-line"></div>
                <div class="step-num">2</div>
                <div class="step-body glass-card">
                    <h3>Knowledge Distillation <span class="tag tag-done">Done</span></h3>
                    <p>The student (<strong>Qwen3-1.7B + LoRA</strong>) trains against teacher completions
                    using hard-label cross-entropy. Labels are masked at padding positions
                    (<code>-100</code>) so the model is never penalized for pad tokens. Gradient
                    accumulation (Ã—4), cosine LR decay with linear warmup, and BF16 mixed precision
                    keep memory usage manageable on a single 32 GB GPU.</p>
                    <div class="insight">200 steps Â· lr=2e-5 Â· warmup=20 Â· eval every 40 steps â†’ best eval loss: 4.6465</div>
                </div>
            </div>

            <div class="pipeline-step">
                <div class="step-line"></div>
                <div class="step-num">3</div>
                <div class="step-body glass-card">
                    <h3>Population Initialization <span class="tag tag-done">Done</span></h3>
                    <p>After distillation, a <strong>Population</strong> of N individuals is created.
                    Individual 0 copies the distilled LoRA weights exactly. Individuals 1â€¦N-1 are
                    Gaussian-mutated variants. All LoRA A/B tensors are kept on <strong>CPU</strong>
                    throughout evolution â€” only moved to GPU for fitness evaluation â€” preventing OOM
                    when handling large populations.</p>
                    <div class="insight">population size=6 Â· elite=2 Â· mutation_scale=0.05 Â· mutation_rate=0.3</div>
                </div>
            </div>

            <div class="pipeline-step">
                <div class="step-line"></div>
                <div class="step-num">4</div>
                <div class="step-body glass-card">
                    <h3>Fitness Evaluation <span class="tag tag-done">Done</span></h3>
                    <p>Each individual's LoRA weights are injected into the base model and evaluated
                    on a held-out WikiText-2 slice. <strong>Fitness = 1 / (1 + PPL)</strong> â€” higher
                    is better. Adapter loading uses CPU-first allocation
                    (<code>safetensors â†’ CPU â†’ GPU</code>) to avoid OOM on GPUs partially occupied by
                    Ollama. Keys are remapped from PEFT's saved format
                    (<code>lora_A.weight</code>) to the runtime format
                    (<code>lora_A.default.weight</code>).</p>
                    <div class="insight">fitness = 1/(1+ppl) Â· best: 0.0123 â†’ ppl â‰ˆ 80.3</div>
                </div>
            </div>

            <div class="pipeline-step">
                <div class="step-line"></div>
                <div class="step-num">5</div>
                <div class="step-body glass-card">
                    <h3>Genetic Evolution <span class="tag tag-done">Done</span></h3>
                    <p>Tournament selection picks parents; <strong>arithmetic crossover</strong> blends
                    their LoRA A/B weight tensors; Gaussian noise mutation injects variation.
                    Elite individuals (top-2) pass unchanged to the next generation. All crossover and
                    mutation operations run on CPU tensors to avoid GPU fragmentation during the long
                    evolutionary loop.</p>
                    <div class="insight">tournament_k=3 Â· crossover(Î±=0.5) Â· mutate(Ïƒ=0.05, p=0.3) Â· 20 generations</div>
                </div>
            </div>

            <div class="pipeline-step">
                <div class="step-num">6</div>
                <div class="step-body glass-card">
                    <h3>Checkpoint & Evaluation <span class="tag tag-done">Done</span></h3>
                    <p>The best evolved individual's weights are saved as <code>checkpoint-final</code>
                    (the best <em>evolved</em> adapter). The best distillation checkpoint
                    (lowest eval loss, mid-training) is saved separately as <code>checkpoint-best</code>.
                    <code>test_evolved_model.py</code> loads both, compares PPL / CE loss / fitness against
                    the base model, and prints a side-by-side generation comparison across five prompts.</p>
                    <div class="insight">checkpoint-final &gt; checkpoint-best priority Â· verdict: PASS if evo_ppl â‰¤ base_ppl</div>
                </div>
            </div>

        </div>
    </section>

    <!-- Architecture -->
    <section class="architecture" id="architecture">
        <div class="section-header">
            <h2>GPU Architecture</h2>
            <p>Designed for dual-GPU setups, with Ollama-backed teacher and student split across devices</p>
        </div>

        <div class="arch-diagram">
            <div class="gpu-container">
                <div class="gpu-box teacher glass-card">
                    <h3>GPU 0 â€” Teacher (Ollama)</h3>
                    <p>Remote qwen3.5 via Ollama API</p>
                    <p>Inference only Â· frozen weights</p>
                    <p>Hard-label CE completions</p>
                    <p>No logprobs (API limitation)</p>
                    <div class="gpu-mem">~26 GB VRAM used by Ollama</div>
                </div>

                <div class="arrow-container">
                    <div class="arrow"></div>
                    <div class="arrow-label">Completions<br>(text tokens)</div>
                    <div class="arrow"></div>
                </div>

                <div class="gpu-box student glass-card">
                    <h3>GPU 1 â€” Student</h3>
                    <p>Qwen3-1.7B + LoRA (r=16)</p>
                    <p>Distillation training Â· BF16</p>
                    <p>Evolution fitness eval</p>
                    <p>CPU â†” GPU adapter swaps</p>
                    <div class="gpu-mem">~7 GB peak during training</div>
                </div>
            </div>

            <div class="arch-note">
                <span>âš </span>
                <div>
                    <strong>Current Limitation:</strong> The teacher runs via Ollama's HTTP API rather than
                    directly on GPU 0. Ollama occupies ~26 GB on GPU 0, leaving ~5.5 GB free.
                    The student runs entirely on GPU 1 (32 GB). Evolution population weights live
                    on CPU between evaluations to avoid fragmentation.
                </div>
            </div>
        </div>
    </section>

    <!-- Results -->
    <section id="results">
        <div class="section-header">
            <h2>Training Results</h2>
            <p>Measured on WikiText-2 held-out set Â· evaluated with test_evolved_model.py</p>
        </div>

        <div style="max-width:900px;margin:0 auto 3rem;">
            <table class="runs-table">
                <thead>
                    <tr>
                        <th>Run</th>
                        <th>Steps</th>
                        <th>Gens / Pop</th>
                        <th>Base PPL</th>
                        <th>Evolved PPL</th>
                        <th>Î”PPL</th>
                        <th>Fitness</th>
                        <th>Verdict</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>run_20260224_100235</td>
                        <td>100</td>
                        <td>10 / 4</td>
                        <td class="val-base">154.34</td>
                        <td class="val-good">120.72</td>
                        <td class="val-good">âˆ’33.6</td>
                        <td class="val-good">0.00821</td>
                        <td class="val-good">PASS</td>
                    </tr>
                    <tr>
                        <td>run_20260224_131506</td>
                        <td>200</td>
                        <td>20 / 6</td>
                        <td class="val-base">154.34</td>
                        <td class="val-good">80.28</td>
                        <td class="val-good">âˆ’74.1</td>
                        <td class="val-good">0.01230</td>
                        <td class="val-good">PASS</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section-header" style="margin-top:3rem;">
            <h2>Generation Quality</h2>
            <p>Qualitative comparison â€” base vs evolved on 5 prompts (greedy decode, 60 tokens)</p>
        </div>

        <div class="fixes-grid">
            <div class="fix-card glass-card">
                <div class="fix-title">AI History prompt</div>
                <p><strong>Base:</strong> "The first artificial intelligence systems were developed in the 1950s, and the first AI systems were developed by John McCarthyâ€¦" (repetition)</p>
                <p style="margin-top:0.6rem"><strong>Evolved:</strong> "The first such attempts were made in the 1940s, when researchers like Alan Turing and others began to explore the possibility of machinesâ€¦" (coherent, accurate)</p>
            </div>
            <div class="fix-card glass-card">
                <div class="fix-title">Amazon prompt</div>
                <p><strong>Base:</strong> "a major source of oxygenâ€¦ home to a large number of species" (vague)</p>
                <p style="margin-top:0.6rem"><strong>Evolved:</strong> "home to an incredible variety of species, including over 4000 species of plants, 1000 species of birds, 1000 species of mammals" (specific, factual)</p>
            </div>
            <div class="fix-card glass-card">
                <div class="fix-title">Neural networks prompt</div>
                <p><strong>Base:</strong> "understand the underlying principlesâ€¦ the structure of the network, the role of each component" (generic)</p>
                <p style="margin-top:0.6rem"><strong>Evolved:</strong> "understanding the structure of the network, the role of each layer, and the importance of data preprocessing" (more precise, layer-specific)</p>
            </div>
        </div>
    </section>

    <!-- Bug Fixes / Internals -->
    <section id="fixes">
        <div class="section-header">
            <h2>Internals &amp; Bug Fixes</h2>
            <p>Key discoveries, root causes, and the fixes that made results possible</p>
        </div>

        <div class="fixes-grid">
            <div class="fix-card glass-card">
                <div class="fix-title">
                    <span>1</span> Uniform Soft Targets Corruption
                    <span class="badge">Critical</span>
                </div>
                <p>Ollama returns empty logprobs. KL divergence against uniform soft targets
                is always <code>TÂ² Ã— seq_len Ã— log(vocab)</code> â€” a large, constant positive value
                that actively pushes the student toward uniform predictions, destroying PPL.</p>
                <code>fix: probe logprobs at load time â†’ fall back to hard-label CE only
if teacher_outputs["has_logprobs"] is False</code>
            </div>

            <div class="fix-card glass-card">
                <div class="fix-title">
                    <span>2</span> Eval Never Fired
                    <span class="badge">Critical</span>
                </div>
                <p><code>eval_steps=100</code> default with only 20 training steps meant evaluation
                never ran, so <code>best_eval_loss</code> stayed at <code>inf</code> and
                <code>checkpoint-best</code> was never saved.</p>
                <code>fix: eval_steps = max(1, args.steps // 5)</code>
            </div>

            <div class="fix-card glass-card">
                <div class="fix-title">
                    <span>3</span> Padding Tokens in Loss
                    <span class="badge">Bug</span>
                </div>
                <p>Labels included padding token IDs, causing the model to learn to predict
                padding, inflating cross-entropy loss on padded positions.</p>
                <code>labels[attention_mask == 0] = -100  # mask padding</code>
            </div>

            <div class="fix-card glass-card">
                <div class="fix-title">
                    <span>4</span> PEFT Adapter Key Mismatch
                    <span class="badge">Bug</span>
                </div>
                <p>PEFT saves adapter weights as <code>lora_A.weight</code> but
                <code>PeftModel.state_dict()</code> uses <code>lora_A.default.weight</code>
                (adapter name "default" inserted). All 224 tensors were silently ignored â€” the
                evolved model was identical to the base model.</p>
                <code>k.replace("lora_A.weight", "lora_A.default.weight")</code>
            </div>

            <div class="fix-card glass-card">
                <div class="fix-title">
                    <span>5</span> OOM on Adapter Load
                    <span class="badge">Infrastructure</span>
                </div>
                <p><code>PeftModel.from_pretrained()</code> calls
                <code>safe_load_file(path, device="cuda:1")</code> internally, directly
                allocating on a GPU already holding ~25 GB of Ollama weights.</p>
                <code>cpu_weights = load_file(path, device="cpu")
remapped = {k: v.to(device) for k, v in ...}
model.load_state_dict(remapped, strict=False)</code>
            </div>

            <div class="fix-card glass-card">
                <div class="fix-title">
                    <span>6</span> Wrong Checkpoint Priority
                    <span class="badge">Logic</span>
                </div>
                <p><code>checkpoint-best</code> saves the lowest <em>distillation</em> eval loss
                (mid-training). <code>checkpoint-final</code> saves the best <em>evolved</em>
                individual's LoRA weights. The test script was loading the wrong one first.</p>
                <code>priority: checkpoint-final &gt; checkpoint-best</code>
            </div>

            <div class="fix-card glass-card">
                <div class="fix-title">
                    <span>7</span> Mixed-Device Population Tensors
                    <span class="badge">Gotcha</span>
                </div>
                <p><code>population.initialize_from_model()</code> copies state_dict â€” individual 0's
                tensors are on CUDA. Mutated individuals (1+) land on CPU after
                <code>torch.randn_like()</code>. Calling <code>population.diversity</code> on a
                mixed-device population raises <code>RuntimeError</code>.</p>
                <code>fix: crossover/mutate always clone to CPU first</code>
            </div>

            <div class="fix-card glass-card">
                <div class="fix-title">
                    <span>8</span> Fitness Displays as 0.0000
                    <span class="badge">Display</span>
                </div>
                <p>When PPL &gt; 10,000 (e.g., early in training before any convergence),
                <code>fitness = 1/(1+10000) â‰ˆ 0.0001</code> rounds to four decimal places as
                <code>0.0000</code>. Not a bug â€” just a display artifact of extreme perplexity.</p>
                <code>fitness = 1/(1+ppl)  # rounds at ppl > 9999</code>
            </div>
        </div>
    </section>

    <!-- Tech Stack -->
    <section id="tech">
        <div class="section-header">
            <h2>Technology Stack</h2>
            <p>Built on modern ML frameworks with evolutionary optimization at its core</p>
        </div>

        <div class="tech-grid">
            <div class="glass-card tech-card">
                <span class="tech-icon">ğŸ§¬</span>
                <h3>Evolutionary Optimization</h3>
                <p>Tournament selection, arithmetic crossover, and Gaussian mutation over LoRA A/B weight tensors. Elite preservation keeps top individuals each generation.</p>
            </div>
            <div class="glass-card tech-card">
                <span class="tech-icon">ğŸ“</span>
                <h3>Knowledge Distillation</h3>
                <p>Hard-label cross-entropy distillation from Ollama-backed teacher. Full KD (KL + CE) ready when a logprobs-capable teacher is available.</p>
            </div>
            <div class="glass-card tech-card">
                <span class="tech-icon">ğŸ”§</span>
                <h3>LoRA Adapters (PEFT)</h3>
                <p>Low-rank adaptation on q/k/v/o_proj with r=16, Î±=32. Only ~25 MB per individual â€” enables large population sizes without VRAM explosion.</p>
            </div>
            <div class="glass-card tech-card">
                <span class="tech-icon">âš¡</span>
                <h3>Mixed Precision (BF16)</h3>
                <p>BF16 autocast during forward/backward with GradScaler for stability. Student weights stored in BF16; LoRA adapter weights in float32.</p>
            </div>
            <div class="glass-card tech-card">
                <span class="tech-icon">ğŸ“Š</span>
                <h3>Live Training Monitor</h3>
                <p>watch_training.py parses the training log in real time and renders a Rich terminal dashboard with distillation loss sparklines, evolution fitness trend, and teacher health.</p>
            </div>
            <div class="glass-card tech-card">
                <span class="tech-icon">ğŸ§ª</span>
                <h3>Evolved Model Evaluator</h3>
                <p>test_evolved_model.py runs perplexity comparison (base vs evolved) on WikiText-2 and a side-by-side generation table across configurable prompts.</p>
            </div>
        </div>
    </section>

    <!-- Hardware -->
    <section>
        <div class="section-header">
            <h2>Hardware</h2>
            <p>Tested configuration and scaling targets</p>
        </div>
        <div class="specs-grid">
            <div class="glass-card spec-item">
                <span class="spec-icon">ğŸ–¥</span>
                <div class="spec-info">
                    <h4>Tested On</h4>
                    <p>2Ã— GPU (32 GB each) Â· GPU 0: Ollama teacher Â· GPU 1: student training + evolution eval</p>
                </div>
            </div>
            <div class="glass-card spec-item">
                <span class="spec-icon">ğŸ’»</span>
                <div class="spec-info">
                    <h4>Minimum</h4>
                    <p>1Ã— GPU 16 GB VRAM Â· Ollama on separate host or CPU inference Â· pop-size â‰¤ 4</p>
                </div>
            </div>
            <div class="glass-card spec-item">
                <span class="spec-icon">ğŸ”¥</span>
                <div class="spec-info">
                    <h4>Optimal</h4>
                    <p>2Ã— A100/H100 40â€“80 GB Â· Local teacher with logprobs Â· pop-size 16+ Â· larger models</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Roadmap -->
    <section id="roadmap">
        <div class="section-header">
            <h2>Roadmap</h2>
            <p>What's been built, what's in progress, and what's coming next</p>
        </div>

        <div class="roadmap-grid">
            <div class="glass-card roadmap-card">
                <div class="phase">Phase 1</div>
                <h3 class="done">Foundation âœ“</h3>
                <ul>
                    <li class="done">Population management (Individual, Population)</li>
                    <li class="done">Genetic operations (crossover, mutate, tournament selection)</li>
                    <li class="done">LoRA-only individuals (CPU tensors, 25 MB each)</li>
                    <li class="done">PerplexityFitness evaluation</li>
                    <li class="done">Elite preservation across generations</li>
                </ul>
            </div>

            <div class="glass-card roadmap-card">
                <div class="phase">Phase 2</div>
                <h3 class="done">Distillation âœ“</h3>
                <ul>
                    <li class="done">OllamaTeacher with logprobs probe</li>
                    <li class="done">Hard-label CE fallback (Ollama no logprobs)</li>
                    <li class="done">KDLoss (KL + CE, ready for soft targets)</li>
                    <li class="done">DistillationTrainer with eval, checkpointing</li>
                    <li class="done">Mixed precision BF16, gradient accumulation</li>
                    <li class="done">Label masking for padding tokens</li>
                    <li class="done">watch_training.py live monitor</li>
                    <li class="done">test_evolved_model.py benchmark script</li>
                </ul>
            </div>

            <div class="glass-card roadmap-card">
                <div class="phase">Phase 3</div>
                <h3 class="wip">Soft-Target KD âŸ³</h3>
                <ul>
                    <li class="done">KDLoss architecture in place</li>
                    <li class="wip">Enable logprobs via vLLM or local teacher</li>
                    <li class="wip">Full KL divergence training</li>
                    <li>Temperature sweep for soft targets</li>
                    <li>Feature-level distillation (hidden states)</li>
                </ul>
            </div>

            <div class="glass-card roadmap-card">
                <div class="phase">Phase 4</div>
                <h3 class="next">Neural Pruning</h3>
                <ul>
                    <li>Magnitude-based unstructured pruning</li>
                    <li>Gradient-based saliency pruning</li>
                    <li>Structured channel/head pruning</li>
                    <li>Post-pruning LoRA recovery fine-tuning</li>
                    <li>Compression ratio vs accuracy trade-off</li>
                </ul>
            </div>

            <div class="glass-card roadmap-card">
                <div class="phase">Phase 5</div>
                <h3 class="next">SLERP Crossover</h3>
                <ul>
                    <li>Spherical linear interpolation for weight blending</li>
                    <li>Geodesic paths on weight manifolds</li>
                    <li>Compare SLERP vs arithmetic crossover</li>
                    <li>Adaptive Î± scheduling across generations</li>
                </ul>
            </div>

            <div class="glass-card roadmap-card">
                <div class="phase">Phase 6</div>
                <h3 class="next">Scale &amp; Deploy</h3>
                <ul>
                    <li>Merge LoRA into base model for deployment</li>
                    <li>GGUF / AWQ quantization export</li>
                    <li>Multi-node distributed evolution</li>
                    <li>Larger student models (7B, 13B)</li>
                    <li>Domain-specific datasets</li>
                </ul>
            </div>
        </div>

        <div class="status-grid">
            <div class="status-item"><span class="status-dot active"></span> Completed</div>
            <div class="status-item"><span class="status-dot wip"></span> In Progress</div>
            <div class="status-item"><span class="status-dot next"></span> Planned</div>
        </div>
    </section>

    <!-- Next Steps -->
    <section id="next">
        <div class="section-header">
            <h2>Next Steps to Implement</h2>
            <p>Concrete upcoming improvements, ordered by expected impact</p>
        </div>

        <div class="next-steps-grid">
            <div class="glass-card next-card">
                <div class="priority"><span class="tag tag-wip">High Priority</span></div>
                <h3>Enable Real Soft-Target KD</h3>
                <p>Deploy a local teacher (vLLM, llama.cpp with logprobs, or Hugging Face inference)
                that actually returns per-token log-probabilities. Full KL divergence training
                should significantly accelerate distillation convergence and improve final quality.</p>
                <div class="impact">expected: faster convergence, lower best eval loss, better generation coherence</div>
            </div>

            <div class="glass-card next-card">
                <div class="priority"><span class="tag tag-wip">High Priority</span></div>
                <h3>Larger Population &amp; More Generations</h3>
                <p>Current best run used pop-size=6, 20 generations. The fitness trend
                (â–…â–…â–…â–…â–…â–‡â–‡â–‡â–‡â–‡) shows evolution was still improving at generation 20. Scaling to
                pop-size=16 and 50+ generations should find lower perplexity individuals.</p>
                <div class="impact">pop-size=16, gen=50 â†’ estimated PPL ~65â€“70</div>
            </div>

            <div class="glass-card next-card">
                <div class="priority"><span class="tag tag-next">Medium Priority</span></div>
                <h3>SLERP Crossover</h3>
                <p>Replace arithmetic crossover with spherical linear interpolation
                (<code>slerp(w_a, w_b, Î±)</code>) on LoRA weight tensors. SLERP
                follows the geodesic on the unit sphere rather than the chord, potentially
                producing better-blended children that preserve training dynamics.</p>
                <div class="impact">expected: smoother fitness landscape, less diversity loss</div>
            </div>

            <div class="glass-card next-card">
                <div class="priority"><span class="tag tag-next">Medium Priority</span></div>
                <h3>Adaptive Mutation Rate</h3>
                <p>Implement a 1/5th-success rule or CMA-ES-style mutation strength adaptation.
                When the best fitness stagnates across K generations, increase mutation scale;
                when fitness improves every generation, tighten it for exploitation.</p>
                <div class="impact">expected: escape local optima, faster convergence</div>
            </div>

            <div class="glass-card next-card">
                <div class="priority"><span class="tag tag-next">Medium Priority</span></div>
                <h3>Domain-Specific Training Data</h3>
                <p>Current distillation uses random Ollama completions. Switching to a curated
                domain corpus (math, code, instruction-following) with teacher completions would
                specialize the student rather than general language modeling.</p>
                <div class="impact">targeted fitness â†’ task-specific benchmark improvement</div>
            </div>

            <div class="glass-card next-card">
                <div class="priority"><span class="tag tag-next">Medium Priority</span></div>
                <h3>LoRA â†’ Base Merge &amp; Quantization</h3>
                <p>After evolution completes, merge the best adapter into the base model weights
                (<code>model.merge_and_unload()</code>) then export to GGUF or AWQ for
                deployment. This removes the inference overhead of adapter injection and
                enables edge/mobile deployment.</p>
                <div class="impact">adapter overhead: 0 Â· 4-bit quantized: ~900 MB</div>
            </div>

            <div class="glass-card next-card">
                <div class="priority"><span class="tag tag-next">Lower Priority</span></div>
                <h3>Neural Pruning Integration</h3>
                <p>Add a post-evolution pruning pass: magnitude-pruning on the merged model
                removes low-salience weights, followed by a short LoRA recovery fine-tune.
                The genetic framework can then evolve over pruning thresholds as an
                additional hyperparameter dimension.</p>
                <div class="impact">target: 20â€“30% parameter reduction with &lt;2 PPL regression</div>
            </div>

            <div class="glass-card next-card">
                <div class="priority"><span class="tag tag-next">Lower Priority</span></div>
                <h3>Multi-Objective Fitness</h3>
                <p>Current fitness is <code>1/(1+PPL)</code> only. Add secondary objectives:
                generation speed (tokens/sec), memory footprint, task-specific accuracy
                (MMLU, HumanEval). Use Pareto-front selection or a weighted scalarization
                to evolve toward the efficiency-accuracy frontier.</p>
                <div class="impact">Pareto-optimal models across the speed/quality trade-off</div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="footer-links">
            <a href="#pipeline">How It Works</a>
            <a href="#results">Results</a>
            <a href="#fixes">Internals</a>
            <a href="#roadmap">Roadmap</a>
            <a href="#next">Next Steps</a>
            <a href="https://github.com/genesis-ai/genesis">GitHub</a>
        </div>
        <p class="footer-copyright">Genesis AI Evolution Laboratory Â· MIT License Â· Qwen3-1.7B student Â· qwen3.5 teacher</p>
    </footer>

    <script>
        // Neural network background nodes
        (function() {
            const container = document.getElementById('neuralBg');
            for (let i = 0; i < 35; i++) {
                const node = document.createElement('div');
                node.className = 'neural-node';
                node.style.left = Math.random() * 100 + '%';
                node.style.top = Math.random() * 100 + '%';
                node.style.animationDelay = (Math.random() * 3) + 's';
                node.style.animationDuration = (2 + Math.random() * 2) + 's';
                container.appendChild(node);
            }
        })();

        // Smooth scroll
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) target.scrollIntoView({ behavior: 'smooth' });
            });
        });
    </script>
</body>
</html>
```

<a id="docs-index-md"></a>

#### `docs/index.md`
*2471 bytes Â· ~617 tokens*

```markdown
# Genesis Documentation

Welcome to the Genesis AI Evolution Laboratory documentation. Genesis is a framework for creating efficient AI models using evolutionary algorithms, knowledge distillation, and pruning.

## Overview

Genesis combines three powerful techniques to create optimized AI models:

1. **Evolutionary Algorithms**: Evolve populations of model weights using genetic operations like SLERP-based crossover and adaptive mutation.

2. **Knowledge Distillation**: Transfer knowledge from large teacher models to smaller, more efficient student models.

3. **Model Pruning**: Remove unnecessary weights while maintaining model performance.

## Getting Started

- [Installation](installation.md) - Set up Genesis on your system
- [Quick Start](quickstart.md) - Run your first evolution experiment
- [Architecture](architecture.md) - Understand the system design

## API Reference

- [Core Module](api/core.md) - Evolutionary components
- [Models](api/models.md) - Teacher, Student, and LoRA management
- [Distillation](api/distillation.md) - Knowledge distillation
- [TTS](api/tts.md) - Text-to-Speech evolution

## Tutorials

- [Medical LLM Evolution](tutorials/medical_llm.md) - Train a medical QA model
- [TTS Voice Evolution](tutorials/tts_evolution.md) - Evolve voice characteristics

## Key Features

### Dual-GPU Architecture

Genesis is designed for dual-GPU systems, running the teacher model on GPU 0 and the student model on GPU 1 for maximum efficiency.

```
GPU 0: Teacher Model (frozen, inference only)
GPU 1: Student Model (trained, evolved)
```

### SLERP Crossover

Unlike traditional linear interpolation, Genesis uses Spherical Linear Interpolation (SLERP) for weight crossover, which better preserves the magnitude of neural network weights.

### LoRA Integration

Full support for Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large models with minimal memory overhead.

### Extensible Fitness Functions

Custom fitness evaluators can be implemented by extending the `FitnessEvaluator` base class:

```python
from genesis.core.fitness import FitnessEvaluator, FitnessResult

class MyFitness(FitnessEvaluator):
    def evaluate(self, model, state_dict=None):
        # Your evaluation logic
        return FitnessResult(score=0.95, metrics={"accuracy": 0.95})
```

## Support

- GitHub Issues: Report bugs and feature requests
- Discussions: Ask questions and share ideas

## License

Genesis is released under the MIT License.
```

<a id="genesis-cli-py"></a>

#### `genesis/cli.py`
*8037 bytes Â· ~2,008 tokens*

```python
"""Command-line interface for Genesis AI Evolution Laboratory."""

import argparse
import sys
import logging
from pathlib import Path


def _cmd_run(args: argparse.Namespace) -> int:
    """Run the full evolutionary optimization pipeline."""
    from genesis.config.settings import GenesisConfig
    from genesis.optimizer import EvolutionaryOptimizer

    if args.config:
        config = GenesisConfig.from_yaml(args.config)
    else:
        config = GenesisConfig()

    # CLI overrides
    if args.output_dir:
        config.output_dir = args.output_dir
    if args.generations is not None:
        config.genetic.generations = args.generations
    if args.population_size is not None:
        config.genetic.population_size = args.population_size
    if args.teacher_model:
        config.teacher_model = args.teacher_model
    if args.student_model:
        config.student_model = args.student_model
    if args.no_lora:
        config.use_lora = False

    optimizer = EvolutionaryOptimizer(config=config)

    logging.info("Starting evolutionary optimization...")
    results = optimizer.run()

    print(f"\nEvolution complete.")
    print(f"  Best fitness : {results['best_fitness']:.6f}")
    print(f"  Generations  : {results['generations']}")
    print(f"  Converged    : {results['converged']}")
    print(f"  Output dir   : {config.output_dir}")
    return 0


def _cmd_distill(args: argparse.Namespace) -> int:
    """Run knowledge distillation only."""
    from genesis.config.settings import GenesisConfig
    from genesis.optimizer import EvolutionaryOptimizer

    if args.config:
        config = GenesisConfig.from_yaml(args.config)
    else:
        config = GenesisConfig()

    if args.output_dir:
        config.output_dir = args.output_dir
    if args.steps is not None:
        config.distillation.max_steps = args.steps
    if args.teacher_model:
        config.teacher_model = args.teacher_model
    if args.student_model:
        config.student_model = args.student_model

    optimizer = EvolutionaryOptimizer(config=config)
    optimizer.initialize()

    logging.info("Starting knowledge distillation...")
    results = optimizer.distill(num_steps=args.steps)

    print(f"\nDistillation complete.")
    print(f"  Final loss : {results.get('final_loss', 'N/A')}")
    print(f"  Steps      : {results.get('total_steps', 'N/A')}")
    return 0


def _cmd_prune(args: argparse.Namespace) -> int:
    """Run model pruning only."""
    from genesis.config.settings import GenesisConfig
    from genesis.optimizer import EvolutionaryOptimizer

    if args.config:
        config = GenesisConfig.from_yaml(args.config)
    else:
        config = GenesisConfig()

    if args.output_dir:
        config.output_dir = args.output_dir
    if args.sparsity is not None:
        config.pruning.target_sparsity = args.sparsity
    if args.method:
        config.pruning.pruning_method = args.method
    if args.student_model:
        config.student_model = args.student_model

    optimizer = EvolutionaryOptimizer(config=config)
    optimizer.initialize()

    logging.info("Starting model pruning...")
    stats = optimizer.prune_model(target_sparsity=args.sparsity)

    print(f"\nPruning complete.")
    print(f"  Target sparsity : {config.pruning.target_sparsity:.1%}")
    print(f"  Actual sparsity : {stats.get('actual_sparsity', 0):.1%}")
    print(f"  Params pruned   : {stats.get('pruned_params', 'N/A')}")
    return 0


def _cmd_info(args: argparse.Namespace) -> int:
    """Show hardware and environment information."""
    import torch
    from genesis import __version__
    from genesis.config.hardware import HardwareConfig

    print(f"Genesis AI Evolution Laboratory v{__version__}")
    print(f"  Python  : {sys.version.split()[0]}")
    print(f"  PyTorch : {torch.__version__}")
    print(f"  CUDA    : {'available' if torch.cuda.is_available() else 'not available'}")

    if torch.cuda.is_available():
        print(f"  GPUs    : {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            vram = props.total_memory / 1024 ** 3
            print(f"    [{i}] {props.name} â€” {vram:.1f} GB VRAM")

    hw = HardwareConfig()
    print(f"\n{hw.memory_summary()}")
    return 0


def build_parser() -> argparse.ArgumentParser:
    """Build the top-level argument parser."""
    parser = argparse.ArgumentParser(
        prog="genesis",
        description="Genesis AI Evolution Laboratory â€” evolve efficient AI models.",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Logging verbosity (default: INFO)",
    )

    subparsers = parser.add_subparsers(dest="command", metavar="<command>")
    subparsers.required = True

    # ------------------------------------------------------------------ run --
    run_p = subparsers.add_parser("run", help="Run the full evolutionary optimization pipeline")
    run_p.add_argument("--config", "-c", metavar="FILE", help="Path to YAML config file")
    run_p.add_argument("--output-dir", "-o", metavar="DIR", help="Override output directory")
    run_p.add_argument("--teacher-model", metavar="NAME", help="Teacher model name or path")
    run_p.add_argument("--student-model", metavar="NAME", help="Student model name or path")
    run_p.add_argument("--generations", "-g", type=int, metavar="N", help="Number of generations")
    run_p.add_argument("--population-size", "-p", type=int, metavar="N", help="Population size")
    run_p.add_argument("--no-lora", action="store_true", help="Disable LoRA adapters")
    run_p.set_defaults(func=_cmd_run)

    # --------------------------------------------------------------- distill --
    dist_p = subparsers.add_parser("distill", help="Run knowledge distillation only")
    dist_p.add_argument("--config", "-c", metavar="FILE", help="Path to YAML config file")
    dist_p.add_argument("--output-dir", "-o", metavar="DIR", help="Override output directory")
    dist_p.add_argument("--teacher-model", metavar="NAME", help="Teacher model name or path")
    dist_p.add_argument("--student-model", metavar="NAME", help="Student model name or path")
    dist_p.add_argument("--steps", "-s", type=int, metavar="N", help="Number of training steps")
    dist_p.set_defaults(func=_cmd_distill)

    # ----------------------------------------------------------------- prune --
    prune_p = subparsers.add_parser("prune", help="Prune a model to a target sparsity")
    prune_p.add_argument("--config", "-c", metavar="FILE", help="Path to YAML config file")
    prune_p.add_argument("--output-dir", "-o", metavar="DIR", help="Override output directory")
    prune_p.add_argument("--student-model", metavar="NAME", help="Model to prune")
    prune_p.add_argument("--sparsity", type=float, metavar="F",
                         help="Target sparsity, e.g. 0.3 for 30%%")
    prune_p.add_argument("--method", choices=["magnitude", "gradient", "taylor", "fisher"],
                         help="Pruning saliency method")
    prune_p.set_defaults(func=_cmd_prune)

    # ------------------------------------------------------------------ info --
    info_p = subparsers.add_parser("info", help="Show hardware and environment information")
    info_p.set_defaults(func=_cmd_info)

    return parser


def main() -> None:
    """Entry point for the `genesis` console script."""
    parser = build_parser()
    args = parser.parse_args()

    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%H:%M:%S",
    )

    try:
        exit_code = args.func(args)
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nAborted.", file=sys.stderr)
        sys.exit(1)
    except Exception as exc:
        logging.error(str(exc), exc_info=args.log_level == "DEBUG")
        sys.exit(1)


if __name__ == "__main__":
    main()
```


### ğŸ—‚ï¸ Data Models & Types

<a id="docs-api-models-md"></a>

#### `docs/api/models.md`
*3967 bytes Â· ~991 tokens*

```markdown
# Models Module API Reference

The models module provides wrappers for teacher and student models.

## TeacherModel

### `genesis.models.teacher.TeacherModel`

Wrapper for teacher model in knowledge distillation.

```python
from genesis.models import TeacherModel

teacher = TeacherModel(
    model_name_or_path="meta-llama/Llama-2-7b-hf",
    device="cuda:0",
    dtype=torch.float16,
    load_in_8bit=False,
    load_in_4bit=False,
)

# Load the model
teacher.load()

# Forward pass
outputs = teacher.forward(input_ids, attention_mask)
logits = outputs["logits"]

# Get soft targets
soft_targets = teacher.get_soft_targets(input_ids, temperature=4.0)

# Get hidden states
hidden_states = teacher.get_hidden_states(input_ids, layer_indices=[-1, -2])

# Generate text
generated = teacher.generate(input_ids, max_new_tokens=100)

# Cleanup
teacher.unload()
```

### Methods

#### `load()`
Load the model and tokenizer.

#### `forward(input_ids, attention_mask=None, output_hidden_states=False)`
Forward pass through the model.

#### `get_soft_targets(input_ids, attention_mask=None, temperature=1.0)`
Get soft probability targets for distillation.

#### `get_hidden_states(input_ids, attention_mask=None, layer_indices=None)`
Extract hidden states from specified layers.

#### `generate(input_ids, attention_mask=None, max_new_tokens=100, **kwargs)`
Generate text using the model.

## StudentModel

### `genesis.models.student.StudentModel`

Wrapper for student model with LoRA support.

```python
from genesis.models import StudentModel
from genesis.models.lora_manager import LoRAConfig

lora_config = LoRAConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
)

student = StudentModel(
    model_name_or_path="meta-llama/Llama-2-7b-hf",
    device="cuda:1",
    dtype=torch.float16,
    use_lora=True,
    lora_config=lora_config,
)

student.load()

# Forward pass
outputs = student.forward(input_ids, attention_mask, labels=labels)
loss = outputs["loss"]

# Training step with distillation
losses = student.train_step(
    input_ids,
    attention_mask,
    teacher_logits=teacher_logits,
    labels=labels,
    temperature=4.0,
    alpha=0.5,
)

# Get/set state dict
state = student.get_state_dict(lora_only=True)
student.load_state_dict(state)

# Save model
student.save("./outputs/model", merge_lora=False)
```

## LoRAManager

### `genesis.models.lora_manager.LoRAManager`

Manager for LoRA adapters supporting evolutionary operations.

```python
from genesis.models import LoRAManager, LoRAConfig

config = LoRAConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
)

manager = LoRAManager(model, config)

# Get LoRA weights
lora_state = manager.get_lora_state_dict()

# Set LoRA weights
manager.set_lora_state_dict(lora_state)

# Interpolate between two LoRA states
blended = manager.interpolate_lora(state1, state2, ratio=0.5)

# Merge multiple LoRA states
merged = manager.merge_multiple_lora([state1, state2, state3], weights=[0.5, 0.3, 0.2])

# Compute similarity
similarity = manager.compute_lora_similarity(state1, state2)

# Add noise for mutation
noisy = manager.add_noise_to_lora(lora_state, noise_scale=0.01)

# Save/load LoRA weights
manager.save_lora("./lora_weights.pt")
manager.load_lora("./lora_weights.pt")
```

### LoRAConfig

Configuration for LoRA adapters.

```python
@dataclass
class LoRAConfig:
    r: int = 16                    # LoRA rank
    lora_alpha: int = 32           # LoRA alpha
    lora_dropout: float = 0.05     # Dropout probability
    target_modules: list = ["q_proj", "v_proj"]
    bias: str = "none"
    task_type: str = "CAUSAL_LM"
```

### Helper Function

```python
from genesis.models.lora_manager import create_lora_config_for_model

# Create optimized config for specific model types
config = create_lora_config_for_model("llama", r=16, alpha=32)
# Returns config with target_modules optimized for Llama architecture
```
```

<a id="genesis-models-__init__-py"></a>

#### `genesis/models/__init__.py`
*691 bytes Â· ~172 tokens*

```python
"""Model handling for Genesis."""

__all__ = [
    "TeacherModel",
    "StudentModel",
    "LoRAManager",
    "LoRAConfig",
]


def __getattr__(name):
    if name == "TeacherModel":
        from genesis.models.teacher import TeacherModel
        return TeacherModel
    if name == "StudentModel":
        from genesis.models.student import StudentModel
        return StudentModel
    if name == "LoRAManager":
        from genesis.models.lora_manager import LoRAManager
        return LoRAManager
    if name == "LoRAConfig":
        from genesis.models.lora_manager import LoRAConfig
        return LoRAConfig
    raise AttributeError(f"module 'genesis.models' has no attribute {name!r}")
```

<a id="genesis-models-lora_manager-py"></a>

#### `genesis/models/lora_manager.py`
*10560 bytes Â· ~2,640 tokens*

```python
"""LoRA adapter management for Genesis."""

from dataclasses import dataclass, field
from typing import Any, Optional
import torch
import torch.nn as nn
from copy import deepcopy
import logging

logger = logging.getLogger(__name__)


@dataclass
class LoRAConfig:
    """Configuration for LoRA adapters."""

    r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    target_modules: list[str] = field(
        default_factory=lambda: ["q_proj", "v_proj", "k_proj", "o_proj"]
    )
    bias: str = "none"
    task_type: str = "CAUSAL_LM"
    modules_to_save: Optional[list[str]] = None

    def to_peft_config(self):
        """Convert to PEFT LoraConfig."""
        from peft import LoraConfig as PeftLoraConfig, TaskType

        task_type_map = {
            "CAUSAL_LM": TaskType.CAUSAL_LM,
            "SEQ_2_SEQ_LM": TaskType.SEQ_2_SEQ_LM,
            "SEQ_CLS": TaskType.SEQ_CLS,
            "TOKEN_CLS": TaskType.TOKEN_CLS,
        }

        return PeftLoraConfig(
            r=self.r,
            lora_alpha=self.lora_alpha,
            lora_dropout=self.lora_dropout,
            target_modules=self.target_modules,
            bias=self.bias,
            task_type=task_type_map.get(self.task_type, TaskType.CAUSAL_LM),
            modules_to_save=self.modules_to_save,
        )


class LoRAManager:
    """
    Manager for LoRA adapters supporting evolutionary operations.

    Handles extraction, manipulation, and merging of LoRA weights
    for use in genetic algorithms.
    """

    def __init__(
        self,
        model: nn.Module,
        config: Optional[LoRAConfig] = None,
    ):
        """
        Initialize LoRA manager.

        Args:
            model: PEFT model with LoRA adapters
            config: LoRA configuration
        """
        self.model = model
        self.config = config or LoRAConfig()
        self._lora_modules: dict[str, nn.Module] = {}
        self._discover_lora_modules()

    def _discover_lora_modules(self) -> None:
        """Discover all LoRA modules in the model."""
        for name, module in self.model.named_modules():
            # Check for PEFT LoRA layers
            if hasattr(module, "lora_A") and hasattr(module, "lora_B"):
                self._lora_modules[name] = module
            # Alternative: check for Linear layers with LoRA weights
            elif "lora_" in name.lower():
                self._lora_modules[name] = module

        logger.info(f"Discovered {len(self._lora_modules)} LoRA modules")

    def get_lora_state_dict(self) -> dict[str, torch.Tensor]:
        """
        Extract only LoRA parameters from model.

        Returns:
            State dictionary containing only LoRA weights
        """
        lora_state = {}

        for name, param in self.model.named_parameters():
            if "lora_" in name.lower() or any(
                target in name for target in self.config.target_modules
            ):
                if param.requires_grad:
                    lora_state[name] = param.data.clone()

        return lora_state

    def set_lora_state_dict(
        self,
        state_dict: dict[str, torch.Tensor],
        strict: bool = False,
    ) -> None:
        """
        Load LoRA parameters into model.

        Args:
            state_dict: LoRA state dictionary
            strict: Whether to require exact key matching
        """
        model_state = self.model.state_dict()

        for key, value in state_dict.items():
            if key in model_state:
                model_state[key] = value
            elif not strict:
                logger.warning(f"Key {key} not found in model state dict")
            else:
                raise KeyError(f"Key {key} not found in model state dict")

        self.model.load_state_dict(model_state)

    def clone_lora_weights(self) -> dict[str, torch.Tensor]:
        """Create a deep copy of LoRA weights."""
        return deepcopy(self.get_lora_state_dict())

    def interpolate_lora(
        self,
        state_dict1: dict[str, torch.Tensor],
        state_dict2: dict[str, torch.Tensor],
        ratio: float = 0.5,
    ) -> dict[str, torch.Tensor]:
        """
        Linear interpolation between two LoRA state dicts.

        Args:
            state_dict1: First LoRA state dict
            state_dict2: Second LoRA state dict
            ratio: Interpolation ratio (0 = state_dict1, 1 = state_dict2)

        Returns:
            Interpolated LoRA state dict
        """
        result = {}

        for key in state_dict1.keys():
            if key in state_dict2:
                w1 = state_dict1[key]
                w2 = state_dict2[key]
                result[key] = (1 - ratio) * w1 + ratio * w2
            else:
                result[key] = state_dict1[key]

        return result

    def merge_multiple_lora(
        self,
        state_dicts: list[dict[str, torch.Tensor]],
        weights: Optional[list[float]] = None,
    ) -> dict[str, torch.Tensor]:
        """
        Merge multiple LoRA state dicts with weights.

        Args:
            state_dicts: List of LoRA state dictionaries
            weights: Optional weights for each state dict

        Returns:
            Merged LoRA state dict
        """
        if not state_dicts:
            raise ValueError("At least one state dict required")

        if weights is None:
            weights = [1.0 / len(state_dicts)] * len(state_dicts)

        assert len(weights) == len(state_dicts)

        result = {}
        keys = set(state_dicts[0].keys())

        for key in keys:
            tensors = [sd.get(key) for sd in state_dicts if key in sd]
            if len(tensors) == len(state_dicts):
                merged = sum(w * t for w, t in zip(weights, tensors))
                result[key] = merged
            else:
                result[key] = state_dicts[0][key]

        return result

    def compute_lora_similarity(
        self,
        state_dict1: dict[str, torch.Tensor],
        state_dict2: dict[str, torch.Tensor],
    ) -> float:
        """
        Compute cosine similarity between two LoRA state dicts.

        Args:
            state_dict1: First LoRA state dict
            state_dict2: Second LoRA state dict

        Returns:
            Average cosine similarity across all parameters
        """
        similarities = []

        for key in state_dict1.keys():
            if key in state_dict2:
                v1 = state_dict1[key].flatten().float()
                v2 = state_dict2[key].flatten().float()

                cos_sim = torch.nn.functional.cosine_similarity(
                    v1.unsqueeze(0),
                    v2.unsqueeze(0),
                ).item()
                similarities.append(cos_sim)

        return sum(similarities) / len(similarities) if similarities else 0.0

    def get_lora_magnitude(self, state_dict: Optional[dict[str, torch.Tensor]] = None) -> float:
        """
        Compute total magnitude of LoRA weights.

        Args:
            state_dict: Optional state dict (uses current model if None)

        Returns:
            L2 norm of all LoRA parameters
        """
        if state_dict is None:
            state_dict = self.get_lora_state_dict()

        total_norm = 0.0
        for tensor in state_dict.values():
            total_norm += tensor.float().norm().item() ** 2

        return total_norm**0.5

    def scale_lora_weights(
        self,
        state_dict: dict[str, torch.Tensor],
        scale: float,
    ) -> dict[str, torch.Tensor]:
        """
        Scale all LoRA weights by a factor.

        Args:
            state_dict: LoRA state dict
            scale: Scaling factor

        Returns:
            Scaled state dict
        """
        return {key: value * scale for key, value in state_dict.items()}

    def add_noise_to_lora(
        self,
        state_dict: dict[str, torch.Tensor],
        noise_scale: float = 0.01,
    ) -> dict[str, torch.Tensor]:
        """
        Add Gaussian noise to LoRA weights.

        Args:
            state_dict: LoRA state dict
            noise_scale: Standard deviation of noise

        Returns:
            Noisy state dict
        """
        result = {}
        for key, value in state_dict.items():
            noise = torch.randn_like(value) * noise_scale
            result[key] = value + noise
        return result

    def freeze_base_model(self) -> None:
        """Freeze all non-LoRA parameters."""
        for name, param in self.model.named_parameters():
            if "lora_" not in name.lower():
                param.requires_grad = False

    def unfreeze_all(self) -> None:
        """Unfreeze all parameters."""
        for param in self.model.parameters():
            param.requires_grad = True

    def get_trainable_param_count(self) -> tuple[int, int]:
        """
        Get trainable parameter statistics.

        Returns:
            Tuple of (trainable_params, total_params)
        """
        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in self.model.parameters())
        return trainable, total

    def save_lora(self, path: str) -> None:
        """Save LoRA weights to file."""
        state_dict = self.get_lora_state_dict()
        torch.save(state_dict, path)
        logger.info(f"LoRA weights saved to {path}")

    def load_lora(self, path: str) -> None:
        """Load LoRA weights from file."""
        state_dict = torch.load(path, map_location="cpu", weights_only=False)
        self.set_lora_state_dict(state_dict)
        logger.info(f"LoRA weights loaded from {path}")


def create_lora_config_for_model(
    model_type: str,
    r: int = 16,
    alpha: int = 32,
) -> LoRAConfig:
    """
    Create a LoRA config optimized for specific model types.

    Args:
        model_type: Type of model ('llama', 'mistral', 'gpt2', 'bert')
        r: LoRA rank
        alpha: LoRA alpha

    Returns:
        LoRAConfig instance
    """
    target_modules_map = {
        "llama": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "mistral": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "gpt2": ["c_attn", "c_proj", "c_fc"],
        "bert": ["query", "key", "value", "dense"],
        "t5": ["q", "k", "v", "o", "wi", "wo"],
    }

    target_modules = target_modules_map.get(model_type.lower(), ["q_proj", "v_proj"])

    return LoRAConfig(
        r=r,
        lora_alpha=alpha,
        target_modules=target_modules,
    )
```

<a id="genesis-models-ollama_teacher-py"></a>

#### `genesis/models/ollama_teacher.py`
*20017 bytes Â· ~4,967 tokens*

```python
"""Teacher model backed by Ollama for inference-only knowledge distillation.

Instead of loading a large model locally, this teacher delegates all
inference to an Ollama server. Since all Qwen3 models share the same
tokenizer (vocab_size=151936), soft targets from the Ollama teacher
are directly usable in token-level KD loss with a Qwen3 student.

Two classes are provided:
  - OllamaTeacher  â€” synchronous (one request at a time)
  - AsyncOllamaTeacher â€” async batch fetching via aiohttp, eliminating
    the GPU starvation caused by the synchronous sequential HTTP loop.
"""

import asyncio
import math
import logging
import time
from typing import Optional

import torch
import requests

logger = logging.getLogger(__name__)


class _OllamaConfig:
    """Minimal config shim so teacher.model.config.vocab_size works."""

    def __init__(self, vocab_size: int, model_name: str):
        self.vocab_size = vocab_size
        self.model_type = "ollama"
        self._model_name = model_name


class _OllamaModelShim:
    """Shim so teacher.model.parameters() / .training work as expected."""

    training = False

    def __init__(self, config: _OllamaConfig):
        self.config = config

    def parameters(self):
        return iter([])

    def named_parameters(self):
        return iter([])

    def eval(self):
        """No-op â€” remote model is always in inference mode."""
        return self

    def train(self, mode: bool = True):
        """No-op â€” remote model cannot be put in training mode."""
        return self


class OllamaTeacher:
    """
    Teacher model backed by a local (or remote) Ollama server.

    All Qwen3 variants share the same BPE tokenizer, so soft targets
    produced by, e.g., ``qwen3.5`` on Ollama can be used directly
    for KD loss against a Qwen3-1.7B student that uses the same vocab.

    Args:
        model_name: Ollama model tag (e.g. ``"qwen3.5"``, ``"qwen3:32b"``).
        base_url: Ollama server base URL.
        tokenizer_path: Local path or HF repo ID for the shared tokenizer
            (used to decode ``input_ids`` â†’ text before sending to Ollama).
        vocab_size: Shared vocabulary size (151936 for all Qwen3 models).
        top_logprobs: Number of top-logprob tokens to request per position.
    """

    def __init__(
        self,
        model_name: str = "qwen3.5",
        base_url: str = "http://localhost:11434",
        tokenizer_path: Optional[str] = None,
        vocab_size: int = 151936,
        top_logprobs: int = 20,
        fallback_model: Optional[str] = None,
    ):
        self.model_name = model_name
        self.base_url = base_url.rstrip("/")
        self._tokenizer_path = tokenizer_path
        self._vocab_size = vocab_size
        self.top_logprobs = top_logprobs
        self.fallback_model = fallback_model

        self._tokenizer = None
        self._logprobs_supported: bool = False  # set by load() after probing
        # Expose a .model attribute so teacher.model.config.vocab_size works
        self.model = _OllamaModelShim(
            _OllamaConfig(vocab_size=vocab_size, model_name=model_name)
        )

    # ------------------------------------------------------------------
    # Lifecycle
    # ------------------------------------------------------------------

    def load(self) -> None:
        """Verify Ollama connectivity, probe logprobs support, and load the tokenizer."""
        try:
            r = requests.get(f"{self.base_url}/api/tags", timeout=10)
            r.raise_for_status()
            models = [m["name"] for m in r.json().get("models", [])]
            logger.info(f"Ollama server reachable. Available models: {models}")
            if not any(self.model_name in m for m in models):
                logger.warning(
                    f"Model '{self.model_name}' not in Ollama. "
                    f"Run: ollama pull {self.model_name}"
                )
        except requests.RequestException as e:
            raise RuntimeError(
                f"Cannot reach Ollama server at {self.base_url}. "
                f"Start it with: ollama serve\nError: {e}"
            )

        if self._tokenizer_path:
            from transformers import AutoTokenizer

            self._tokenizer = AutoTokenizer.from_pretrained(self._tokenizer_path)
            logger.info(f"Tokenizer loaded from {self._tokenizer_path}")

        self._logprobs_supported = self._probe_logprobs()
        if self._logprobs_supported:
            logger.info("Ollama logprobs: SUPPORTED â€” KD soft-target loss enabled.")
        else:
            logger.warning(
                "Ollama logprobs: NOT SUPPORTED â€” KD soft-target loss will be "
                "disabled. Training will use hard-label cross-entropy only."
            )

    def _probe_logprobs(self) -> bool:
        """Send a tiny test request and check whether logprobs are returned."""
        try:
            resp = requests.post(
                f"{self.base_url}/v1/completions",
                json={
                    "model": self.model_name,
                    "prompt": "Hi",
                    "max_tokens": 1,
                    "logprobs": True,
                    "top_logprobs": 1,
                    "stream": False,
                },
                timeout=30,
            )
            resp.raise_for_status()
            data = resp.json()
            choices = data.get("choices", [{}])
            lp = (choices[0].get("logprobs") or {}) if choices else {}
            return bool(lp.get("top_logprobs") or lp.get("token_logprobs"))
        except Exception:
            return False

    def unload(self) -> None:
        """Release local resources (tokenizer)."""
        self._tokenizer = None

    # ------------------------------------------------------------------
    # Internal helpers
    # ------------------------------------------------------------------

    def _ids_to_text(self, input_ids: torch.Tensor) -> list[str]:
        if self._tokenizer is None:
            raise RuntimeError(
                "Tokenizer not loaded. Pass tokenizer_path= to OllamaTeacher."
            )
        return self._tokenizer.batch_decode(input_ids, skip_special_tokens=True)

    def _build_distribution(
        self,
        top_logprobs: dict,  # {token_str: logprob}
        temperature: float,
    ) -> torch.Tensor:
        """
        Convert Ollama top-k logprobs to a full-vocab probability vector.

        Known top-k tokens receive their temperature-scaled probabilities;
        the residual mass is spread uniformly over the remaining vocab.
        """
        result = torch.zeros(self._vocab_size)

        if not top_logprobs:
            return torch.full((self._vocab_size,), 1.0 / self._vocab_size)

        # Temperature-scale in log space then softmax over the top-k set
        scaled = {tok: lp / max(temperature, 1e-6) for tok, lp in top_logprobs.items()}
        max_lp = max(scaled.values())
        exp_vals = {tok: math.exp(lp - max_lp) for tok, lp in scaled.items()}
        denom = sum(exp_vals.values())

        known_prob_sum = 0.0
        for tok_str, prob in exp_vals.items():
            prob_norm = prob / denom
            if self._tokenizer is not None:
                tok_ids = self._tokenizer.encode(tok_str, add_special_tokens=False)
                if tok_ids and 0 <= tok_ids[0] < self._vocab_size:
                    result[tok_ids[0]] += prob_norm
                    known_prob_sum += prob_norm

        # Spread residual mass uniformly
        residual = max(0.0, 1.0 - known_prob_sum)
        result += residual / self._vocab_size

        return result

    # ------------------------------------------------------------------
    # Teacher interface
    # ------------------------------------------------------------------

    def get_soft_targets(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        temperature: float = 1.0,
    ) -> torch.Tensor:
        """
        Return soft-target distributions for each token position.

        Decodes ``input_ids`` to text, sends the prompt to Ollama requesting
        ``top_logprobs`` per generated token, and reconstructs an approximate
        full-vocabulary distribution.

        Returns:
            Tensor of shape ``(batch, seq_len, vocab_size)``.
        """
        texts = self._ids_to_text(input_ids)
        batch_size, seq_len = input_ids.shape
        device = input_ids.device

        # Pre-fill with uniform distribution so any uncovered position is valid
        uniform_val = 1.0 / self._vocab_size
        soft_targets = torch.full((batch_size, seq_len, self._vocab_size), uniform_val)

        for i, text in enumerate(texts):
            # Try primary model, then fallback (if configured), then uniform
            models_to_try = [self.model_name]
            if self.fallback_model:
                models_to_try.append(self.fallback_model)

            success = False
            for model_name in models_to_try:
                if success:
                    break
                backoff = 5.0
                for attempt in range(3):
                    try:
                        response = requests.post(
                            f"{self.base_url}/v1/completions",
                            json={
                                "model": model_name,
                                "prompt": text,
                                "max_tokens": seq_len,
                                "temperature": temperature,
                                "logprobs": self.top_logprobs,
                                "stream": False,
                            },
                            timeout=120,
                        )
                        if response.status_code == 429:
                            if self.fallback_model and model_name != self.fallback_model:
                                logger.warning(
                                    f"Ollama 429 on '{model_name}'. Switching to fallback '{self.fallback_model}'."
                                )
                            else:
                                retry_after = float(response.headers.get("Retry-After", backoff))
                                wait = max(retry_after, backoff)
                                logger.warning(
                                    f"Ollama 429 on '{model_name}'. Retrying in {wait:.1f}s (attempt {attempt+1}/3)."
                                )
                                time.sleep(wait)
                                backoff = min(backoff * 2, 60.0)
                            break  # move to next model (or next attempt)
                        response.raise_for_status()
                        data = response.json()

                        choices = data.get("choices", [{}])
                        if choices:
                            logprobs_data = choices[0].get("logprobs", {}) or {}
                            token_logprobs = logprobs_data.get("top_logprobs", []) or []
                            for t, tok_lp in enumerate(token_logprobs[:seq_len]):
                                if tok_lp:
                                    dist = self._build_distribution(tok_lp, temperature)
                                    if dist.sum() > 0:
                                        soft_targets[i, t] = dist
                        success = True
                        break  # success â€” no need to try fallback

                    except requests.RequestException as e:
                        if attempt < 2:
                            logger.warning(f"Ollama request to '{model_name}' failed: {e}. Retrying in {backoff:.1f}s.")
                            time.sleep(backoff)
                            backoff = min(backoff * 2, 60.0)
                        else:
                            logger.warning(f"Ollama request to '{model_name}' failed after 3 attempts: {e}.")

            if not success:
                logger.warning("All teacher models exhausted. Using uniform distribution for this sample.")
                # soft_targets[i] already uniform from pre-fill

        return soft_targets.to(device)

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> dict:
        """
        Forward pass via Ollama, returning a logits-compatible dict.

        When the Ollama server does not return logprobs (``has_logprobs=False``
        in the returned dict) the caller should skip the KD term and rely only
        on hard-label cross-entropy, because using a uniform soft-target
        distribution would actively corrupt the student model.
        """
        if not self._logprobs_supported:
            return {"logits": None, "has_logprobs": False}

        soft = self.get_soft_targets(input_ids, attention_mask, temperature=1.0)
        # Detect if all positions are still uniform (every API call failed)
        uniform_val = 1.0 / self._vocab_size
        is_uniform = (soft - uniform_val).abs().max().item() < 1e-7
        if is_uniform:
            return {"logits": None, "has_logprobs": False}

        logits = torch.log(soft.clamp(min=1e-9))
        return {"logits": logits.to(input_ids.device), "has_logprobs": True}

    def generate(self, prompt: str, max_tokens: int = 256, **kwargs) -> str:
        """Plain text generation via Ollama."""
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "think": False,   # disable Qwen3 thinking mode so response is non-empty
                "options": {"num_predict": max_tokens},
            },
            timeout=120,
        )
        response.raise_for_status()
        data = response.json()
        text = data.get("response", "")
        # Fallback: some Ollama versions nest output differently
        if not text and "message" in data:
            text = data["message"].get("content", "")
        return text

    def get_config(self) -> dict:
        """Return config compatible with TeacherModel.get_config()."""
        return {
            "model_name": self.model_name,
            "base_url": self.base_url,
            "vocab_size": self._vocab_size,
            "num_parameters": 0,   # remote model
            "dtype": "remote",
        }


# â”€â”€ Async teacher â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AsyncOllamaTeacher(OllamaTeacher):
    """Async-batch variant of OllamaTeacher.

    The synchronous OllamaTeacher issues one HTTP request per batch, meaning
    the GPU sits completely idle while waiting for the network round-trip.
    This class fires all texts in a batch as *concurrent* aiohttp requests,
    letting the GPU work on the previous batch while the next one is in-flight.

    Usage â€” drop-in replacement::

        teacher = AsyncOllamaTeacher(model_name="qwen3.5:cloud", base_url=...)
        teacher.load()
        # forward() signature identical to OllamaTeacher
        outputs = teacher.forward(input_ids, attention_mask)

    Requirements: ``pip install aiohttp``
    """

    def _fetch_completions_async(
        self,
        texts: list[str],
        seq_len: int,
        temperature: float,
    ) -> list[dict | None]:
        """Fire all texts concurrently and return raw Ollama JSON responses."""
        try:
            import aiohttp
        except ImportError:
            raise ImportError(
                "AsyncOllamaTeacher requires aiohttp: pip install aiohttp"
            )

        payload_template = {
            "model": self.model_name,
            "max_tokens": seq_len,
            "temperature": temperature,
            "logprobs": True,
            "top_logprobs": self.top_logprobs,
            "stream": False,
        }

        async def _fetch_one(session: "aiohttp.ClientSession", text: str) -> dict | None:
            payload = {**payload_template, "prompt": text}
            try:
                async with session.post(
                    f"{self.base_url}/v1/completions",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120),
                ) as resp:
                    if resp.status == 200:
                        return await resp.json()
                    logger.warning("AsyncOllamaTeacher: status %d for text %.40r", resp.status, text)
            except Exception as exc:
                logger.warning("AsyncOllamaTeacher: request error: %s", exc)
            return None

        async def _fetch_all() -> list[dict | None]:
            connector = aiohttp.TCPConnector(limit=len(texts))
            async with aiohttp.ClientSession(connector=connector) as session:
                tasks = [_fetch_one(session, t) for t in texts]
                return await asyncio.gather(*tasks)

        # Run in the current event loop if one exists, otherwise create a new one.
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # We're inside an already-running loop (e.g. Jupyter).
                # Use a thread-based executor to avoid nest_asyncio dependency.
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
                    future = pool.submit(asyncio.run, _fetch_all())
                    return future.result()
            return loop.run_until_complete(_fetch_all())
        except RuntimeError:
            return asyncio.run(_fetch_all())

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        output_hidden_states: bool = False,
    ) -> dict:
        """Async-batched forward pass â€” identical return contract to OllamaTeacher.

        All texts in the batch are dispatched as concurrent HTTP requests.
        When logprobs are not supported by the server the method degrades
        gracefully to ``{"logits": None, "has_logprobs": False}``.
        """
        if not self._logprobs_supported:
            return {"logits": None, "has_logprobs": False}

        texts = self._ids_to_text(input_ids)
        batch_size, seq_len = input_ids.shape
        temperature = 1.0

        responses = self._fetch_completions_async(texts, seq_len, temperature)

        # Parse responses into a sparse prob tensor [B, seq_len, vocab_size]
        all_probs: list[torch.Tensor] = []
        any_valid = False

        for resp in responses:
            probs = torch.zeros(seq_len, self._vocab_size, dtype=torch.float32)
            if resp is not None:
                choices = resp.get("choices", [{}])
                lp_data = (choices[0].get("logprobs") or {}) if choices else {}
                top_lp = lp_data.get("top_logprobs", [])
                for pos, token_dict in enumerate(top_lp[:seq_len]):
                    for token_str, lp_val in (token_dict or {}).items():
                        tok_ids = self._tokenizer.encode(
                            token_str, add_special_tokens=False
                        )
                        if tok_ids and tok_ids[0] < self._vocab_size:
                            probs[pos, tok_ids[0]] += math.exp(lp_val)
                    # Renormalise this position
                    s = probs[pos].sum()
                    if s > 0:
                        probs[pos] /= s
                        any_valid = True
            all_probs.append(probs)

        if not any_valid:
            return {"logits": None, "has_logprobs": False}

        logits_tensor = torch.stack(all_probs, dim=0)  # [B, seq_len, vocab]
        return {"logits": logits_tensor, "has_logprobs": True}
```

<a id="genesis-models-student-py"></a>

#### `genesis/models/student.py`
*11176 bytes Â· ~2,794 tokens*

```python
"""Student model wrapper for knowledge distillation."""

from typing import Any, Optional
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel
from peft import get_peft_model, LoraConfig, TaskType
import logging

from genesis.models.lora_manager import LoRAManager, LoRAConfig as GenesisLoRAConfig

logger = logging.getLogger(__name__)


class StudentModel:
    """
    Wrapper for student model in knowledge distillation.

    The student model is a smaller or modified version of the teacher that
    learns from soft targets. It supports LoRA adapters for efficient training.
    """

    def __init__(
        self,
        model_name_or_path: str,
        device: str = "cuda:1",
        dtype: torch.dtype = torch.float16,
        use_lora: bool = True,
        lora_config: Optional[GenesisLoRAConfig] = None,
        load_in_8bit: bool = False,
        load_in_4bit: bool = False,
        trust_remote_code: bool = False,
    ):
        """
        Initialize the student model.

        Args:
            model_name_or_path: HuggingFace model name or local path
            device: Device to load model on
            dtype: Model dtype
            use_lora: Whether to use LoRA adapters
            lora_config: LoRA configuration
            load_in_8bit: Use 8-bit quantization
            load_in_4bit: Use 4-bit quantization
            trust_remote_code: Trust remote code from HuggingFace
        """
        self.model_name_or_path = model_name_or_path
        self.device = device
        self.dtype = dtype
        self.use_lora = use_lora
        self.lora_config = lora_config or GenesisLoRAConfig()
        self._model: Optional[PreTrainedModel] = None
        self._tokenizer = None
        self._lora_manager: Optional[LoRAManager] = None
        self.load_in_8bit = load_in_8bit
        self.load_in_4bit = load_in_4bit
        self.trust_remote_code = trust_remote_code

    def load(self) -> None:
        """Load the student model and optionally apply LoRA."""
        logger.info(f"Loading student model: {self.model_name_or_path}")

        # Determine quantization config
        quantization_config = None
        if self.load_in_4bit or self.load_in_8bit:
            from transformers import BitsAndBytesConfig

            quantization_config = BitsAndBytesConfig(
                load_in_8bit=self.load_in_8bit,
                load_in_4bit=self.load_in_4bit,
            )

        # Load base model
        self._model = AutoModelForCausalLM.from_pretrained(
            self.model_name_or_path,
            dtype=self.dtype,
            device_map=self.device if quantization_config else None,
            quantization_config=quantization_config,
            trust_remote_code=self.trust_remote_code,
        )

        if quantization_config is None:
            self._model = self._model.to(self.device)

        # Apply LoRA if enabled
        if self.use_lora:
            self._apply_lora()

        # Load tokenizer
        self._tokenizer = AutoTokenizer.from_pretrained(
            self.model_name_or_path,
            trust_remote_code=self.trust_remote_code,
        )

        if self._tokenizer.pad_token is None:
            self._tokenizer.pad_token = self._tokenizer.eos_token

        logger.info(f"Student model loaded on {self.device}")
        self._log_trainable_params()

    def _apply_lora(self) -> None:
        """Apply LoRA adapters to the model."""
        peft_config = LoraConfig(
            r=self.lora_config.r,
            lora_alpha=self.lora_config.lora_alpha,
            lora_dropout=self.lora_config.lora_dropout,
            target_modules=self.lora_config.target_modules,
            bias=self.lora_config.bias,
            task_type=TaskType.CAUSAL_LM,
        )

        self._model = get_peft_model(self._model, peft_config)
        self._lora_manager = LoRAManager(self._model, self.lora_config)

        logger.info("LoRA adapters applied")

    def _log_trainable_params(self) -> None:
        """Log trainable parameter statistics."""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        logger.info(
            f"Trainable params: {trainable_params:,} / {total_params:,} "
            f"({100 * trainable_params / total_params:.2f}%)"
        )

    @property
    def model(self) -> PreTrainedModel:
        """Get the underlying model."""
        if self._model is None:
            raise RuntimeError("Model not loaded. Call load() first.")
        return self._model

    @property
    def tokenizer(self):
        """Get the tokenizer."""
        if self._tokenizer is None:
            raise RuntimeError("Tokenizer not loaded. Call load() first.")
        return self._tokenizer

    @property
    def lora_manager(self) -> Optional[LoRAManager]:
        """Get LoRA manager if available."""
        return self._lora_manager

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        output_hidden_states: bool = False,
    ) -> dict[str, Any]:
        """
        Forward pass through the student model.

        Args:
            input_ids: Input token IDs
            attention_mask: Attention mask
            labels: Optional labels for computing loss
            output_hidden_states: Whether to output hidden states

        Returns:
            Dictionary containing logits, loss, and optionally hidden states
        """
        input_ids = input_ids.to(self.device)
        if attention_mask is not None:
            attention_mask = attention_mask.to(self.device)
        if labels is not None:
            labels = labels.to(self.device)

        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            output_hidden_states=output_hidden_states,
        )

        result = {"logits": outputs.logits}

        if hasattr(outputs, "loss") and outputs.loss is not None:
            result["loss"] = outputs.loss

        if output_hidden_states:
            result["hidden_states"] = outputs.hidden_states

        return result

    def train_step(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        teacher_logits: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        temperature: float = 4.0,
        alpha: float = 0.5,
    ) -> dict[str, torch.Tensor]:
        """
        Perform a single training step with distillation.

        Args:
            input_ids: Input token IDs
            attention_mask: Attention mask
            teacher_logits: Soft targets from teacher model
            labels: Hard labels (optional)
            temperature: Temperature for soft targets
            alpha: Weight for distillation loss

        Returns:
            Dictionary with total loss and component losses
        """
        self.model.train()

        # Forward pass
        outputs = self.forward(input_ids, attention_mask, labels, output_hidden_states=False)
        student_logits = outputs["logits"]

        total_loss = torch.tensor(0.0, device=self.device)
        loss_dict = {}

        # Distillation loss
        if teacher_logits is not None:
            teacher_logits = teacher_logits.to(self.device)

            # KL divergence loss
            student_soft = torch.log_softmax(student_logits / temperature, dim=-1)
            teacher_soft = torch.softmax(teacher_logits / temperature, dim=-1)

            kd_loss = torch.nn.functional.kl_div(
                student_soft,
                teacher_soft,
                reduction="batchmean",
            ) * (temperature**2)

            loss_dict["kd_loss"] = kd_loss
            total_loss = total_loss + alpha * kd_loss

        # Hard label loss
        if labels is not None and "loss" in outputs:
            hard_loss = outputs["loss"]
            loss_dict["hard_loss"] = hard_loss
            total_loss = total_loss + (1 - alpha) * hard_loss

        loss_dict["total_loss"] = total_loss
        return loss_dict

    def get_state_dict(self, lora_only: bool = True) -> dict[str, torch.Tensor]:
        """
        Get model state dictionary.

        Args:
            lora_only: If True and using LoRA, return only LoRA parameters

        Returns:
            State dictionary
        """
        if self.use_lora and lora_only and self._lora_manager:
            return self._lora_manager.get_lora_state_dict()
        return self.model.state_dict()

    def load_state_dict(
        self,
        state_dict: dict[str, torch.Tensor],
        strict: bool = False,
    ) -> None:
        """
        Load state dictionary into model.

        Args:
            state_dict: State dictionary to load
            strict: Whether to require strict key matching
        """
        self.model.load_state_dict(state_dict, strict=strict)

    def merge_lora(self) -> None:
        """Merge LoRA weights into base model."""
        if self.use_lora and hasattr(self.model, "merge_and_unload"):
            self._model = self.model.merge_and_unload()
            self.use_lora = False
            logger.info("LoRA weights merged into base model")

    def save(self, path: str, merge_lora: bool = False) -> None:
        """
        Save model to disk.

        Args:
            path: Save path
            merge_lora: Whether to merge LoRA before saving
        """
        if merge_lora and self.use_lora:
            self.merge_lora()

        self.model.save_pretrained(path)
        self.tokenizer.save_pretrained(path)
        logger.info(f"Model saved to {path}")

    def get_config(self) -> dict:
        """Get model configuration."""
        config = {
            "model_name_or_path": self.model_name_or_path,
            "device": self.device,
            "dtype": str(self.dtype),
            "use_lora": self.use_lora,
            "num_parameters": sum(p.numel() for p in self.model.parameters()),
            "trainable_parameters": sum(
                p.numel() for p in self.model.parameters() if p.requires_grad
            ),
        }

        if self.use_lora:
            config["lora_config"] = {
                "r": self.lora_config.r,
                "lora_alpha": self.lora_config.lora_alpha,
                "target_modules": self.lora_config.target_modules,
            }

        return config

    def unload(self) -> None:
        """Unload model from memory."""
        if self._model is not None:
            del self._model
            self._model = None
        if self._tokenizer is not None:
            del self._tokenizer
            self._tokenizer = None
        if self._lora_manager is not None:
            del self._lora_manager
            self._lora_manager = None

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        logger.info("Student model unloaded")

    def __del__(self):
        """Cleanup on deletion."""
        self.unload()
```

<a id="genesis-models-teacher-py"></a>

#### `genesis/models/teacher.py`
*7836 bytes Â· ~1,958 tokens*

```python
"""Teacher model wrapper for knowledge distillation."""

from typing import Any, Optional
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel
import logging

logger = logging.getLogger(__name__)


class TeacherModel:
    """
    Wrapper for teacher model in knowledge distillation.

    The teacher model is typically a larger, well-trained model that provides
    soft targets for training the student model. It runs on a dedicated GPU
    and produces logits/hidden states for distillation.
    """

    def __init__(
        self,
        model_name_or_path: str,
        device: str = "cuda:0",
        dtype: torch.dtype = torch.float16,
        load_in_8bit: bool = False,
        load_in_4bit: bool = False,
        trust_remote_code: bool = False,
    ):
        """
        Initialize the teacher model.

        Args:
            model_name_or_path: HuggingFace model name or local path
            device: Device to load model on
            dtype: Model dtype (float16, bfloat16, float32)
            load_in_8bit: Use 8-bit quantization
            load_in_4bit: Use 4-bit quantization
            trust_remote_code: Trust remote code from HuggingFace
        """
        self.model_name_or_path = model_name_or_path
        self.device = device
        self.dtype = dtype
        self._model: Optional[PreTrainedModel] = None
        self._tokenizer = None
        self.load_in_8bit = load_in_8bit
        self.load_in_4bit = load_in_4bit
        self.trust_remote_code = trust_remote_code

    def load(self) -> None:
        """Load the teacher model and tokenizer."""
        logger.info(f"Loading teacher model: {self.model_name_or_path}")

        # Determine quantization config
        quantization_config = None
        if self.load_in_4bit or self.load_in_8bit:
            from transformers import BitsAndBytesConfig

            quantization_config = BitsAndBytesConfig(
                load_in_8bit=self.load_in_8bit,
                load_in_4bit=self.load_in_4bit,
            )

        # Load model â€” use device_map="auto" to spread large models across
        # all available GPUs automatically (e.g. Qwen3-32B on dual RTX 5090)
        self._model = AutoModelForCausalLM.from_pretrained(
            self.model_name_or_path,
            torch_dtype=self.dtype,
            device_map="auto",
            quantization_config=quantization_config,
            trust_remote_code=self.trust_remote_code,
        )

        # Set to eval mode (teacher is never trained)
        self._model.eval()

        # Load tokenizer
        self._tokenizer = AutoTokenizer.from_pretrained(
            self.model_name_or_path,
            trust_remote_code=self.trust_remote_code,
        )

        if self._tokenizer.pad_token is None:
            self._tokenizer.pad_token = self._tokenizer.eos_token

        logger.info(f"Teacher model loaded on {self.device}")

    @property
    def model(self) -> PreTrainedModel:
        """Get the underlying model."""
        if self._model is None:
            raise RuntimeError("Model not loaded. Call load() first.")
        return self._model

    @property
    def tokenizer(self):
        """Get the tokenizer."""
        if self._tokenizer is None:
            raise RuntimeError("Tokenizer not loaded. Call load() first.")
        return self._tokenizer

    @torch.no_grad()
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        output_hidden_states: bool = False,
    ) -> dict[str, Any]:
        """
        Forward pass through the teacher model.

        Args:
            input_ids: Input token IDs
            attention_mask: Attention mask
            output_hidden_states: Whether to output hidden states

        Returns:
            Dictionary containing logits and optionally hidden states
        """
        input_ids = input_ids.to(self.device)
        if attention_mask is not None:
            attention_mask = attention_mask.to(self.device)

        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=output_hidden_states,
        )

        result = {"logits": outputs.logits}

        if output_hidden_states:
            result["hidden_states"] = outputs.hidden_states

        return result

    @torch.no_grad()
    def get_soft_targets(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        temperature: float = 1.0,
    ) -> torch.Tensor:
        """
        Get soft probability targets from teacher model.

        Args:
            input_ids: Input token IDs
            attention_mask: Attention mask
            temperature: Temperature for softmax scaling

        Returns:
            Soft probability distribution over vocabulary
        """
        outputs = self.forward(input_ids, attention_mask)
        logits = outputs["logits"]

        # Apply temperature scaling
        soft_targets = torch.softmax(logits / temperature, dim=-1)

        return soft_targets

    @torch.no_grad()
    def get_hidden_states(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        layer_indices: Optional[list[int]] = None,
    ) -> list[torch.Tensor]:
        """
        Get hidden states from specified layers.

        Args:
            input_ids: Input token IDs
            attention_mask: Attention mask
            layer_indices: Indices of layers to extract (negative indexing supported)

        Returns:
            List of hidden state tensors
        """
        outputs = self.forward(input_ids, attention_mask, output_hidden_states=True)
        hidden_states = outputs["hidden_states"]

        if layer_indices is None:
            return list(hidden_states)

        return [hidden_states[i] for i in layer_indices]

    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        max_new_tokens: int = 100,
        **kwargs,
    ) -> torch.Tensor:
        """
        Generate text using the teacher model.

        Args:
            input_ids: Input token IDs
            attention_mask: Attention mask
            max_new_tokens: Maximum tokens to generate
            **kwargs: Additional generation arguments

        Returns:
            Generated token IDs
        """
        input_ids = input_ids.to(self.device)
        if attention_mask is not None:
            attention_mask = attention_mask.to(self.device)

        return self.model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            **kwargs,
        )

    def get_config(self) -> dict:
        """Get model configuration."""
        return {
            "model_name_or_path": self.model_name_or_path,
            "device": self.device,
            "dtype": str(self.dtype),
            "num_parameters": sum(p.numel() for p in self.model.parameters()),
            "config": self.model.config.to_dict() if hasattr(self.model, "config") else {},
        }

    def unload(self) -> None:
        """Unload model from memory."""
        if self._model is not None:
            del self._model
            self._model = None
        if self._tokenizer is not None:
            del self._tokenizer
            self._tokenizer = None

        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception:
            pass

        logger.info("Teacher model unloaded")

    def __del__(self):
        """Cleanup on deletion."""
        try:
            self.unload()
        except Exception:
            pass
```


### ğŸ”Œ Routes & Handlers

<a id="docs-api-core-md"></a>

#### `docs/api/core.md`
*3875 bytes Â· ~968 tokens*

```markdown
# Core Module API Reference

The core module provides evolutionary algorithm components.

## Genetics

### `genesis.core.genetics`

#### `slerp(t, v0, v1, epsilon=1e-8)`

Spherical Linear Interpolation between two tensors.

**Parameters:**
- `t` (float): Interpolation factor (0 = v0, 1 = v1)
- `v0` (Tensor): First tensor
- `v1` (Tensor): Second tensor
- `epsilon` (float): Small value to prevent division by zero

**Returns:** Interpolated tensor

**Example:**
```python
from genesis.core.genetics import slerp

child_weights = slerp(0.5, parent1_weights, parent2_weights)
```

#### `crossover(parent1_state, parent2_state, crossover_rate=0.7, method="slerp", slerp_ratio=0.5)`

Perform crossover between two parent state dictionaries.

**Parameters:**
- `parent1_state` (dict): First parent state dict
- `parent2_state` (dict): Second parent state dict
- `crossover_rate` (float): Probability of crossover
- `method` (str): "slerp", "uniform", or "single_point"
- `slerp_ratio` (float): Interpolation ratio for SLERP

**Returns:** Child state dictionary

#### `mutate(state_dict, mutation_rate=0.1, mutation_scale=0.01, mutation_prob_per_weight=0.1, method="gaussian")`

Apply mutation to a state dictionary.

**Parameters:**
- `state_dict` (dict): Model state dictionary
- `mutation_rate` (float): Overall mutation probability
- `mutation_scale` (float): Scale of Gaussian noise
- `mutation_prob_per_weight` (float): Per-weight mutation probability
- `method` (str): "gaussian", "uniform", or "adaptive"

**Returns:** Mutated state dictionary

#### `class Genetics`

Manager class for genetic operations.

```python
genetics = Genetics(
    crossover_rate=0.7,
    mutation_rate=0.1,
    mutation_scale=0.01,
    slerp_ratio=0.5,
    adaptive_mutation=True,
)

child_state = genetics.create_offspring(parent1, parent2)
genetics.step_generation()  # For adaptive mutation decay
```

## Population

### `genesis.core.population`

#### `class Individual`

Represents an individual in the population.

**Attributes:**
- `id` (str): Unique identifier
- `state_dict` (dict): Model weights
- `fitness` (float): Fitness score
- `generation` (int): Generation number
- `parent_ids` (list): IDs of parents

#### `class Population`

Manages a population of individuals.

```python
population = Population(
    size=20,
    genetics=genetics,
    elite_size=2,
)

# Initialize from model
population.initialize_from_model(model, perturbation_scale=0.01)

# Evaluate fitness
population.evaluate(fitness_fn)

# Evolve to next generation
population.evolve()

# Access individuals
best = population.best
avg_fitness = population.average_fitness
diversity = population.diversity
```

## Fitness

### `genesis.core.fitness`

#### `class FitnessEvaluator`

Abstract base class for fitness evaluators.

```python
class CustomFitness(FitnessEvaluator):
    def evaluate(self, model, state_dict=None) -> FitnessResult:
        if state_dict:
            model.load_state_dict(state_dict)
        score = compute_score(model)
        return FitnessResult(score=score)
```

#### Built-in Evaluators

- `PerplexityFitness`: For language models (lower perplexity = higher fitness)
- `AccuracyFitness`: For classification tasks
- `QAFitness`: For question-answering tasks
- `CompositeFitness`: Combine multiple evaluators

## Selection

### `genesis.core.selection`

#### `class SelectionStrategy`

Base class for selection strategies.

#### Built-in Strategies

- `ElitismSelection`: Preserve top individuals
- `TournamentSelection`: Tournament-based selection
- `RouletteWheelSelection`: Fitness-proportionate selection
- `RankSelection`: Rank-based selection
- `BoltzmannSelection`: Temperature-controlled selection

```python
from genesis.core.selection import TournamentSelection

selection = TournamentSelection(tournament_size=3)
selected = selection.select(population, num_to_select=10)
```
```

<a id="docs-api-distillation-md"></a>

#### `docs/api/distillation.md`
*4009 bytes Â· ~1,002 tokens*

```markdown
# Distillation Module API Reference

The distillation module provides knowledge distillation functionality.

## KD Loss Functions

### `genesis.distillation.kd_loss`

#### `kl_divergence_loss(student_logits, teacher_logits, temperature=4.0, reduction="batchmean")`

Compute KL divergence loss between student and teacher logits.

```python
from genesis.distillation.kd_loss import kl_divergence_loss

loss = kl_divergence_loss(
    student_logits,      # [batch, seq_len, vocab]
    teacher_logits,      # [batch, seq_len, vocab]
    temperature=4.0,     # Softmax temperature
    reduction="batchmean"
)
```

#### `soft_target_loss(student_logits, teacher_logits, temperature=4.0, attention_mask=None)`

Compute soft target cross-entropy loss.

#### `feature_distillation_loss(student_hidden, teacher_hidden, projection=None, loss_type="mse")`

Compute feature distillation loss between hidden states.

```python
from genesis.distillation.kd_loss import feature_distillation_loss

loss = feature_distillation_loss(
    student_hidden,    # [batch, seq_len, hidden_dim]
    teacher_hidden,    # [batch, seq_len, hidden_dim]
    projection=None,   # Optional projection layer
    loss_type="mse"    # "mse", "cosine", or "l1"
)
```

### KDLoss Class

Comprehensive knowledge distillation loss module.

```python
from genesis.distillation import KDLoss

kd_loss = KDLoss(
    temperature=4.0,
    alpha=0.5,                        # Weight for KD vs hard loss
    use_feature_distillation=True,
    feature_weight=0.1,
    feature_layers=[-1, -2, -3],
    use_attention_distillation=False,
    attention_weight=0.1,
)

losses = kd_loss(
    student_logits=student_logits,
    teacher_logits=teacher_logits,
    hard_labels=labels,
    student_hidden_states=student_hidden,
    teacher_hidden_states=teacher_hidden,
    attention_mask=attention_mask,
)

total_loss = losses["total_loss"]
kd_loss_value = losses["kd_loss"]
hard_loss_value = losses["hard_loss"]
```

### ProgressiveKDLoss

Knowledge distillation with curriculum learning.

```python
from genesis.distillation.kd_loss import ProgressiveKDLoss

progressive_loss = ProgressiveKDLoss(
    initial_temperature=10.0,
    final_temperature=1.0,
    initial_alpha=0.9,
    final_alpha=0.1,
    total_steps=10000,
)

# Update temperature and alpha each step
progressive_loss.step()
```

## Distillation Trainer

### `genesis.distillation.trainer.DistillationTrainer`

Training loop with dual-GPU synchronization.

```python
from genesis.distillation import DistillationTrainer, TrainingConfig

config = TrainingConfig(
    learning_rate=2e-5,
    weight_decay=0.01,
    max_steps=1000,
    warmup_steps=100,
    gradient_accumulation_steps=4,
    max_grad_norm=1.0,
    logging_steps=10,
    eval_steps=100,
    save_steps=500,
    mixed_precision="fp16",
    temperature=4.0,
    alpha=0.5,
    output_dir="./outputs",
)

trainer = DistillationTrainer(
    teacher=teacher_model,
    student=student_model,
    train_dataloader=train_loader,
    eval_dataloader=eval_loader,
    config=config,
    kd_loss=custom_kd_loss,  # Optional custom loss
)

# Run training
results = trainer.train(
    num_steps=1000,
    callback=lambda r: print(f"Step {r['step']}: loss={r['loss']:.4f}")
)

# Evaluate
eval_results = trainer.evaluate()

# Save/load checkpoint
trainer._save_checkpoint("best")
trainer.load_checkpoint("./checkpoints/checkpoint-best")

# Get training state
state = trainer.get_training_state()
```

### TrainingConfig

Configuration for distillation training.

```python
@dataclass
class TrainingConfig:
    learning_rate: float = 2e-5
    weight_decay: float = 0.01
    max_steps: int = 1000
    warmup_steps: int = 100
    gradient_accumulation_steps: int = 4
    max_grad_norm: float = 1.0
    logging_steps: int = 10
    eval_steps: int = 100
    save_steps: int = 500
    mixed_precision: str = "fp16"  # fp16, bf16, fp32
    temperature: float = 4.0
    alpha: float = 0.5
    output_dir: str = "./outputs"
    save_total_limit: int = 3
```
```

<a id="docs-api-tts-md"></a>

#### `docs/api/tts.md`
*4700 bytes Â· ~1,175 tokens*

```markdown
# TTS Module API Reference

The TTS module provides components for evolving Text-to-Speech models.

## TTSChild

### `genesis.tts.tts_child.TTSChild`

TTS model child for evolutionary optimization.

```python
from genesis.tts import TTSChild, TTSConfig

config = TTSConfig(
    model_type="tacotron2",  # tacotron2, fastspeech2, vits
    style_dim=128,
    speaker_dim=256,
    num_speakers=1,
    sample_rate=22050,
    n_mel_channels=80,
    hop_length=256,
)

child = TTSChild(config=config)

# Load model weights
child.load_model("./model_checkpoint.pt", device="cuda")

# Synthesize speech
output = child.synthesize(
    text="Hello, world!",
    speaker_id=0,
    style_idx=None,  # Use average style
    device="cuda",
)
mel = output["mel_spectrogram"]

# Genetic operations
mutated_child = child.mutate(mutation_rate=0.1, mutation_scale=0.01)
crossed_child = child.crossover(other_child)
cloned_child = child.clone()

# Save/load
child.save("./tts_child.pt")
loaded_child = TTSChild.load("./tts_child.pt")
```

### TTSConfig

```python
@dataclass
class TTSConfig:
    model_type: str = "tacotron2"
    style_dim: int = 128
    speaker_dim: int = 256
    num_speakers: int = 1
    sample_rate: int = 22050
    n_mel_channels: int = 80
    hop_length: int = 256
    win_length: int = 1024
    n_fft: int = 1024
    mutation_rate: float = 0.1
    mutation_scale: float = 0.01
```

## Style Evolution

### `genesis.tts.style_evolution.StyleEvolution`

Evolutionary optimization of TTS style tokens.

```python
from genesis.tts import StyleEvolution

evolution = StyleEvolution(
    style_dim=128,
    num_tokens=10,
    population_size=20,
    elite_size=2,
    mutation_rate=0.1,
    mutation_scale=0.05,
    crossover_rate=0.7,
)

# Initialize population
evolution.initialize_population(
    base_tokens=initial_tokens,  # Optional
    perturbation_scale=0.1,
)

# Evolution loop
for generation in range(100):
    # Evaluate fitness for each individual
    for idx in range(evolution.population_size):
        tokens = evolution.get_individual(idx)
        fitness = evaluate_style(tokens)
        evolution.set_fitness(idx, fitness)

    # Get best
    best_tokens, best_fitness = evolution.get_best()
    print(f"Gen {generation}: Best fitness = {best_fitness}")

    # Evolve
    evolution.evolve()

# Save/load state
evolution.save_state("./evolution_state.pt")
evolution.load_state("./evolution_state.pt")
```

### StyleToken

```python
@dataclass
class StyleToken:
    embedding: torch.Tensor
    name: str = ""
    fitness: float = 0.0
    metadata: dict = field(default_factory=dict)
```

### MultiStyleEvolution

Evolve multiple style aspects simultaneously.

```python
from genesis.tts.style_evolution import MultiStyleEvolution

multi_evolution = MultiStyleEvolution(
    style_configs={
        "prosody": {"dim": 64, "num_tokens": 5},
        "emotion": {"dim": 32, "num_tokens": 8},
        "speaker": {"dim": 128, "num_tokens": 4},
    },
    population_size=20,
)

multi_evolution.initialize_all()

# Get combined style from multiple evolutions
combined_style = multi_evolution.get_combined_style({
    "prosody": 0,
    "emotion": 5,
    "speaker": 2,
})
```

## MCD Fitness

### `genesis.tts.mcd_fitness`

Mel Cepstral Distortion based fitness evaluation.

#### `compute_mcd(reference_mel, synthesized_mel, reduction="mean")`

Compute MCD between reference and synthesized mel spectrograms.

```python
from genesis.tts.mcd_fitness import compute_mcd

mcd = compute_mcd(
    reference_mel,      # [batch, mel_dim, time]
    synthesized_mel,    # [batch, mel_dim, time]
    reduction="mean"    # "mean", "sum", "none"
)
```

### MCDFitness Class

```python
from genesis.tts import MCDFitness

fitness_evaluator = MCDFitness(
    reference_mels=list_of_reference_mels,
    target_mcd=5.0,               # Target MCD value
    weight_naturalness=0.5,
    weight_similarity=0.5,
    device="cuda",
)

# Add more references
fitness_evaluator.add_reference(new_mel)

# Evaluate single mel
result = fitness_evaluator.evaluate(synthesized_mel)
fitness = result["fitness"]
mcd = result["mcd"]
naturalness = result["naturalness_fitness"]
similarity = result["similarity_fitness"]

# Batch evaluation
results = fitness_evaluator.batch_evaluate(list_of_mels)

# Rank population
rankings = fitness_evaluator.rank_population(synthesized_mels)
# Returns: [(index, fitness), ...] sorted by fitness
```

### Additional Metrics

```python
from genesis.tts.mcd_fitness import compute_f0_rmse, compute_vuv_error

# F0 RMSE (fundamental frequency)
f0_rmse = compute_f0_rmse(reference_f0, synthesized_f0)

# Voiced/Unvoiced error rate
vuv_error = compute_vuv_error(reference_f0, synthesized_f0)
```
```


### ğŸ§  Core Logic

<a id="genesis-core-__init__-py"></a>

#### `genesis/core/__init__.py`
*566 bytes Â· ~141 tokens*

```python
"""Core evolutionary components for Genesis."""

from genesis.core.genetics import Genetics, slerp, crossover, mutate
from genesis.core.population import Population, Individual
from genesis.core.fitness import FitnessEvaluator, FitnessResult
from genesis.core.selection import SelectionStrategy, ElitismSelection, TournamentSelection

__all__ = [
    "Genetics",
    "slerp",
    "crossover",
    "mutate",
    "Population",
    "Individual",
    "FitnessEvaluator",
    "FitnessResult",
    "SelectionStrategy",
    "ElitismSelection",
    "TournamentSelection",
]
```

<a id="genesis-core-fitness-py"></a>

#### `genesis/core/fitness.py`
*11136 bytes Â· ~2,784 tokens*

```python
"""Fitness evaluation functions for evolutionary optimization."""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Callable, Optional
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import logging

logger = logging.getLogger(__name__)


@dataclass
class FitnessResult:
    """Container for fitness evaluation results."""

    score: float
    metrics: dict[str, float] = field(default_factory=dict)
    metadata: dict[str, Any] = field(default_factory=dict)

    def __float__(self) -> float:
        return self.score


class FitnessEvaluator(ABC):
    """Abstract base class for fitness evaluators."""

    @abstractmethod
    def evaluate(
        self,
        model: nn.Module,
        state_dict: Optional[dict[str, torch.Tensor]] = None,
    ) -> FitnessResult:
        """
        Evaluate fitness of a model.

        Args:
            model: Model to evaluate
            state_dict: Optional state dict to load before evaluation

        Returns:
            FitnessResult containing score and metrics
        """
        pass

    def __call__(
        self,
        model: nn.Module,
        state_dict: Optional[dict[str, torch.Tensor]] = None,
    ) -> FitnessResult:
        return self.evaluate(model, state_dict)


class PerplexityFitness(FitnessEvaluator):
    """Fitness based on language model perplexity (lower is better, converted to fitness)."""

    def __init__(
        self,
        dataloader: DataLoader,
        device: str = "cuda",
        max_samples: Optional[int] = None,
    ):
        self.dataloader = dataloader
        self.device = device
        self.max_samples = max_samples

    @torch.no_grad()
    def evaluate(
        self,
        model: nn.Module,
        state_dict: Optional[dict[str, torch.Tensor]] = None,
    ) -> FitnessResult:
        if state_dict is not None:
            model.load_state_dict(state_dict)

        model.eval()
        model.to(self.device)

        total_loss = 0.0
        total_tokens = 0
        samples_processed = 0

        for batch in self.dataloader:
            if self.max_samples and samples_processed >= self.max_samples:
                break

            input_ids = batch["input_ids"].to(self.device)
            attention_mask = batch.get("attention_mask", torch.ones_like(input_ids)).to(self.device)
            labels = batch.get("labels", input_ids).to(self.device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
            )

            loss = outputs.loss
            num_tokens = attention_mask.sum().item()

            total_loss += loss.item() * num_tokens
            total_tokens += num_tokens
            samples_processed += input_ids.size(0)

        avg_loss = total_loss / total_tokens if total_tokens > 0 else float("inf")
        perplexity = torch.exp(torch.tensor(avg_loss)).item()

        # Convert perplexity to fitness (lower perplexity = higher fitness)
        # Using negative log to convert to maximization problem
        fitness = 1.0 / (1.0 + perplexity)

        return FitnessResult(
            score=fitness,
            metrics={
                "perplexity": perplexity,
                "loss": avg_loss,
                "tokens_evaluated": total_tokens,
            },
        )


class AccuracyFitness(FitnessEvaluator):
    """Fitness based on classification accuracy."""

    def __init__(
        self,
        dataloader: DataLoader,
        device: str = "cuda",
        max_samples: Optional[int] = None,
    ):
        self.dataloader = dataloader
        self.device = device
        self.max_samples = max_samples

    @torch.no_grad()
    def evaluate(
        self,
        model: nn.Module,
        state_dict: Optional[dict[str, torch.Tensor]] = None,
    ) -> FitnessResult:
        if state_dict is not None:
            model.load_state_dict(state_dict)

        model.eval()
        model.to(self.device)

        correct = 0
        total = 0
        samples_processed = 0

        for batch in self.dataloader:
            if self.max_samples and samples_processed >= self.max_samples:
                break

            input_ids = batch["input_ids"].to(self.device)
            attention_mask = batch.get("attention_mask", torch.ones_like(input_ids)).to(self.device)
            labels = batch["labels"].to(self.device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            # Get predictions
            predictions = logits.argmax(dim=-1)

            # For sequence classification, compare directly
            # For token classification, flatten
            if predictions.dim() > 1:
                mask = attention_mask.bool()
                predictions = predictions[mask]
                labels = labels[mask]

            correct += (predictions == labels).sum().item()
            total += labels.numel()
            samples_processed += input_ids.size(0)

        accuracy = correct / total if total > 0 else 0.0

        return FitnessResult(
            score=accuracy,
            metrics={
                "accuracy": accuracy,
                "correct": correct,
                "total": total,
            },
        )


class QAFitness(FitnessEvaluator):
    """Fitness for question-answering tasks."""

    def __init__(
        self,
        dataloader: DataLoader,
        tokenizer: Any,
        device: str = "cuda",
        max_samples: Optional[int] = None,
        max_new_tokens: int = 50,
    ):
        self.dataloader = dataloader
        self.tokenizer = tokenizer
        self.device = device
        self.max_samples = max_samples
        self.max_new_tokens = max_new_tokens

    @torch.no_grad()
    def evaluate(
        self,
        model: nn.Module,
        state_dict: Optional[dict[str, torch.Tensor]] = None,
    ) -> FitnessResult:
        if state_dict is not None:
            model.load_state_dict(state_dict)

        model.eval()
        model.to(self.device)

        exact_match = 0
        f1_scores = []
        samples_processed = 0

        for batch in self.dataloader:
            if self.max_samples and samples_processed >= self.max_samples:
                break

            input_ids = batch["input_ids"].to(self.device)
            attention_mask = batch.get("attention_mask", torch.ones_like(input_ids)).to(self.device)
            target_texts = batch.get("target_text", [])

            # Generate answers
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=self.max_new_tokens,
                do_sample=False,
            )

            # Decode predictions
            predictions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)

            for pred, target in zip(predictions, target_texts):
                pred_normalized = self._normalize(pred)
                target_normalized = self._normalize(target)

                # Exact match
                if pred_normalized == target_normalized:
                    exact_match += 1

                # F1 score
                f1 = self._compute_f1(pred_normalized, target_normalized)
                f1_scores.append(f1)

            samples_processed += input_ids.size(0)

        em_score = exact_match / samples_processed if samples_processed > 0 else 0.0
        avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0

        # Combined fitness
        fitness = 0.5 * em_score + 0.5 * avg_f1

        return FitnessResult(
            score=fitness,
            metrics={
                "exact_match": em_score,
                "f1": avg_f1,
                "samples_evaluated": samples_processed,
            },
        )

    def _normalize(self, text: str) -> str:
        """Normalize text for comparison."""
        return " ".join(text.lower().split())

    def _compute_f1(self, pred: str, target: str) -> float:
        """Compute F1 score between prediction and target."""
        pred_tokens = set(pred.split())
        target_tokens = set(target.split())

        if not pred_tokens or not target_tokens:
            return float(pred_tokens == target_tokens)

        common = pred_tokens & target_tokens
        precision = len(common) / len(pred_tokens)
        recall = len(common) / len(target_tokens)

        if precision + recall == 0:
            return 0.0

        return 2 * precision * recall / (precision + recall)


class CompositeFitness(FitnessEvaluator):
    """Combine multiple fitness evaluators with weights."""

    def __init__(
        self,
        evaluators: list[FitnessEvaluator],
        weights: Optional[list[float]] = None,
    ):
        self.evaluators = evaluators
        self.weights = weights or [1.0 / len(evaluators)] * len(evaluators)
        assert len(self.weights) == len(self.evaluators)

    def evaluate(
        self,
        model: nn.Module,
        state_dict: Optional[dict[str, torch.Tensor]] = None,
    ) -> FitnessResult:
        results = []
        for evaluator in self.evaluators:
            result = evaluator.evaluate(model, state_dict)
            results.append(result)

        # Weighted sum of scores
        total_score = sum(w * r.score for w, r in zip(self.weights, results))

        # Combine metrics
        combined_metrics = {}
        for i, result in enumerate(results):
            for key, value in result.metrics.items():
                combined_metrics[f"evaluator_{i}_{key}"] = value

        return FitnessResult(
            score=total_score,
            metrics=combined_metrics,
            metadata={"evaluator_scores": [r.score for r in results]},
        )


class CustomFitness(FitnessEvaluator):
    """Wrapper for custom fitness functions."""

    def __init__(self, fitness_fn: Callable[[nn.Module], float]):
        self.fitness_fn = fitness_fn

    def evaluate(
        self,
        model: nn.Module,
        state_dict: Optional[dict[str, torch.Tensor]] = None,
    ) -> FitnessResult:
        if state_dict is not None:
            model.load_state_dict(state_dict)

        score = self.fitness_fn(model)
        return FitnessResult(score=score)


def create_fitness_evaluator(
    fitness_type: str,
    dataloader: DataLoader,
    device: str = "cuda",
    **kwargs,
) -> FitnessEvaluator:
    """
    Factory function to create fitness evaluators.

    Args:
        fitness_type: Type of fitness ('perplexity', 'accuracy', 'qa')
        dataloader: DataLoader for evaluation data
        device: Device to evaluate on
        **kwargs: Additional arguments for the evaluator

    Returns:
        FitnessEvaluator instance
    """
    evaluators = {
        "perplexity": PerplexityFitness,
        "accuracy": AccuracyFitness,
        "qa": QAFitness,
    }

    if fitness_type not in evaluators:
        raise ValueError(f"Unknown fitness type: {fitness_type}")

    return evaluators[fitness_type](dataloader=dataloader, device=device, **kwargs)
```

<a id="genesis-core-genetics-py"></a>

#### `genesis/core/genetics.py`
*14943 bytes Â· ~3,650 tokens*

```python
"""Genetic operations for evolutionary optimization."""

from typing import Optional, Union
import torch
import torch.nn as nn
import numpy as np
from copy import deepcopy


def slerp(
    t: float,
    v0: torch.Tensor,
    v1: torch.Tensor,
    epsilon: float = 1e-8,
) -> torch.Tensor:
    """
    Spherical Linear Interpolation (SLERP) between two tensors.

    SLERP interpolates along the shortest arc on a hypersphere,
    making it ideal for blending model weights.

    Args:
        t: Interpolation factor (0 = v0, 1 = v1)
        v0: First tensor
        v1: Second tensor
        epsilon: Small value to prevent division by zero

    Returns:
        Interpolated tensor
    """
    # Flatten tensors
    v0_flat = v0.flatten().float()
    v1_flat = v1.flatten().float()

    # Normalize
    v0_norm = v0_flat / (torch.norm(v0_flat) + epsilon)
    v1_norm = v1_flat / (torch.norm(v1_flat) + epsilon)

    # Compute angle between vectors
    dot = torch.clamp(torch.dot(v0_norm, v1_norm), -1.0, 1.0)

    # Handle both nearly-identical (dot â†’ +1) and antiparallel (dot â†’ -1) vectors.
    # In both cases sin(theta) â†’ 0, causing division by zero in the SLERP formula.
    # Fall back to LERP, which is the correct limit in both situations.
    if torch.abs(dot) > 0.9995:
        result = (1.0 - t) * v0_flat + t * v1_flat
    else:
        theta = torch.acos(dot)
        sin_theta = torch.sin(theta)
        result = (
            torch.sin((1.0 - t) * theta) / sin_theta * v0_flat
            + torch.sin(t * theta) / sin_theta * v1_flat
        )

    # Preserve original magnitude (average of both)
    original_magnitude = (torch.norm(v0_flat) + torch.norm(v1_flat)) / 2
    result = result / (torch.norm(result) + epsilon) * original_magnitude

    return result.view(v0.shape).to(v0.dtype)


def crossover(
    parent1_state: dict[str, torch.Tensor],
    parent2_state: dict[str, torch.Tensor],
    crossover_rate: float = 0.7,
    method: str = "slerp",
    slerp_ratio: float = 0.5,
    ties_density: float = 0.2,
) -> dict[str, torch.Tensor]:
    """
    Perform crossover between two parent state dictionaries.

    Args:
        parent1_state: State dict of first parent
        parent2_state: State dict of second parent
        crossover_rate: Probability of crossover occurring
        method: Crossover method ('slerp', 'uniform', 'single_point')
        slerp_ratio: Interpolation ratio for SLERP (0.5 = equal blend)

    Returns:
        Child state dictionary
    """
    child_state = {}

    # Check if crossover should occur
    if np.random.random() > crossover_rate:
        # No crossover, return copy of first parent
        return deepcopy(parent1_state)

    keys = list(parent1_state.keys())

    if method == "ties":
        return ties_crossover(parent1_state, parent2_state, density=ties_density)

    elif method == "slerp":
        # SLERP-based crossover â€” performed on CPU to avoid GPU OOM
        for key in keys:
            if key in parent2_state:
                p1 = parent1_state[key].cpu()
                p2 = parent2_state[key].cpu()
                if p1.shape == p2.shape:
                    # Add small random variation to slerp_ratio
                    t = slerp_ratio + np.random.uniform(-0.1, 0.1)
                    t = np.clip(t, 0.0, 1.0)
                    child_state[key] = slerp(t, p1, p2)
                else:
                    child_state[key] = p1.clone()
            else:
                child_state[key] = parent1_state[key].cpu().clone()

    elif method == "uniform":
        # Uniform crossover: randomly select each parameter from either parent
        for key in keys:
            if key in parent2_state and np.random.random() < 0.5:
                child_state[key] = parent2_state[key].cpu().clone()
            else:
                child_state[key] = parent1_state[key].cpu().clone()

    elif method == "single_point":
        # Single-point crossover: split at random point
        crossover_point = np.random.randint(0, len(keys))
        for i, key in enumerate(keys):
            if i < crossover_point:
                child_state[key] = parent1_state[key].cpu().clone()
            else:
                if key in parent2_state:
                    child_state[key] = parent2_state[key].cpu().clone()
                else:
                    child_state[key] = parent1_state[key].cpu().clone()

    else:
        raise ValueError(f"Unknown crossover method: {method} (choices: ties, slerp, uniform, single_point)")

    return child_state


def ties_crossover(
    parent1_state: dict[str, torch.Tensor],
    parent2_state: dict[str, torch.Tensor],
    density: float = 0.2,
) -> dict[str, torch.Tensor]:
    """
    TIES-Merging crossover (Yadav et al., 2023).

    Plain weight averaging destroys knowledge because parameters from different
    fine-tunes often have conflicting signs (sign interference).  TIES resolves
    this in three steps for every tensor:

      1. TRIM   â€” zero out the (1-density) fraction of weights with the
                  smallest absolute value, keeping only the most salient ones.
      2. ELECT  â€” decide the dominant sign per element by majority vote
                  (sign of the trimmed sum).
      3. MERGE  â€” average only the weights that agree with the elected sign;
                  weights that conflict are treated as zero.

    Args:
        parent1_state: LoRA state dict of the first parent (CPU tensors).
        parent2_state: LoRA state dict of the second parent (CPU tensors).
        density: Fraction of weights to keep after trimming (0.2 = top 20%).

    Returns:
        Child state dict (CPU tensors, same dtypes as parent1).
    """
    child_state: dict[str, torch.Tensor] = {}

    for key in parent1_state:
        p1 = parent1_state[key].cpu()
        if key not in parent2_state:
            child_state[key] = p1.clone()
            continue

        p2 = parent2_state[key].cpu()
        if p1.shape != p2.shape:
            child_state[key] = p1.clone()
            continue

        orig_dtype = p1.dtype
        p1f, p2f = p1.float(), p2.float()

        # â”€â”€ 1. TRIM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Keep top-`density` fraction by absolute magnitude for each parent.
        n = p1f.numel()
        k1 = max(1, int(n * density))
        k2 = max(1, int(n * density))

        flat1, flat2 = p1f.flatten(), p2f.flatten()
        # kthvalue gives the k-th *smallest*, so numel-k+1 gives the k-th largest.
        thresh1 = torch.kthvalue(flat1.abs(), n - k1 + 1).values
        thresh2 = torch.kthvalue(flat2.abs(), n - k2 + 1).values

        p1_trim = torch.where(p1f.abs() >= thresh1, p1f, torch.zeros_like(p1f))
        p2_trim = torch.where(p2f.abs() >= thresh2, p2f, torch.zeros_like(p2f))

        # â”€â”€ 2. ELECT SIGN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # The elected sign is the sign of the element-wise sum of trimmed weights.
        # sign(0) = 0, but we need a non-zero direction; use p1_trim as tiebreak.
        elected = torch.sign(p1_trim + p2_trim)
        # Where the sum is exactly zero, fall back to parent-1's sign.
        tiebreak = torch.sign(p1_trim)
        elected = torch.where(elected == 0, tiebreak, elected)

        # â”€â”€ 3. DISJOINT MERGE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Average only the weights that agree with the elected sign.
        p1_aligned = torch.where(torch.sign(p1_trim) == elected, p1_trim, torch.zeros_like(p1f))
        p2_aligned = torch.where(torch.sign(p2_trim) == elected, p2_trim, torch.zeros_like(p2f))

        count = (p1_aligned != 0).float() + (p2_aligned != 0).float()
        merged = (p1_aligned + p2_aligned) / count.clamp(min=1.0)

        child_state[key] = merged.to(orig_dtype)

    return child_state


def mutate(
    state_dict: dict[str, torch.Tensor],
    mutation_rate: float = 0.1,
    mutation_scale: float = 0.01,
    mutation_prob_per_weight: float = 0.1,
    method: str = "gaussian",
) -> dict[str, torch.Tensor]:
    """
    Apply mutation to a state dictionary.

    Args:
        state_dict: Model state dictionary
        mutation_rate: Overall probability of mutation occurring
        mutation_scale: Scale of Gaussian noise
        mutation_prob_per_weight: Probability of mutating each weight
        method: Mutation method ('gaussian', 'uniform', 'adaptive')

    Returns:
        Mutated state dictionary
    """
    mutated_state = {}

    # Check if mutation should occur at all
    if np.random.random() > mutation_rate:
        return deepcopy(state_dict)

    _float_dtypes = (torch.float16, torch.float32, torch.float64, torch.bfloat16)
    for key, param in state_dict.items():
        # Always operate on CPU to avoid GPU OOM during population evolution
        cpu_param = param.cpu()
        if cpu_param.dtype in _float_dtypes:
            if method == "gaussian":
                # Create mutation mask
                mask = torch.rand_like(cpu_param.float()) < mutation_prob_per_weight
                noise = torch.randn_like(cpu_param.float()) * mutation_scale

                # Apply mutation only where mask is True
                mutated_param = cpu_param.float() + noise * mask.float()
                mutated_state[key] = mutated_param.to(cpu_param.dtype)

            elif method == "uniform":
                mask = torch.rand_like(cpu_param.float()) < mutation_prob_per_weight
                noise = (torch.rand_like(cpu_param.float()) - 0.5) * 2 * mutation_scale
                mutated_param = cpu_param.float() + noise * mask.float()
                mutated_state[key] = mutated_param.to(cpu_param.dtype)

            elif method == "adaptive":
                # Scale mutation by parameter magnitude
                param_scale = torch.abs(cpu_param.float()).mean() + 1e-8
                mask = torch.rand_like(cpu_param.float()) < mutation_prob_per_weight
                noise = torch.randn_like(cpu_param.float()) * mutation_scale * param_scale
                mutated_param = cpu_param.float() + noise * mask.float()
                mutated_state[key] = mutated_param.to(cpu_param.dtype)

            else:
                raise ValueError(f"Unknown mutation method: {method}")
        else:
            mutated_state[key] = cpu_param.clone()

    return mutated_state


class Genetics:
    """Manager class for genetic operations."""

    def __init__(
        self,
        crossover_rate: float = 0.7,
        mutation_rate: float = 0.1,
        mutation_scale: float = 0.01,
        slerp_ratio: float = 0.5,
        crossover_method: str = "slerp",
        mutation_method: str = "gaussian",
        adaptive_mutation: bool = True,
        mutation_decay: float = 0.95,
        min_mutation_rate: float = 0.01,
        ties_density: float = 0.2,
    ):
        self.crossover_rate = crossover_rate
        self.mutation_rate = mutation_rate
        self.mutation_scale = mutation_scale
        self.slerp_ratio = slerp_ratio
        self.crossover_method = crossover_method
        self.mutation_method = mutation_method
        self.adaptive_mutation = adaptive_mutation
        self.mutation_decay = mutation_decay
        self.min_mutation_rate = min_mutation_rate
        self.ties_density = ties_density

        self._generation = 0

    def create_offspring(
        self,
        parent1: Union[nn.Module, dict[str, torch.Tensor]],
        parent2: Union[nn.Module, dict[str, torch.Tensor]],
    ) -> dict[str, torch.Tensor]:
        """
        Create offspring from two parents using crossover and mutation.

        Args:
            parent1: First parent model or state dict
            parent2: Second parent model or state dict

        Returns:
            Child state dictionary
        """
        # Extract state dicts if needed
        p1_state = parent1.state_dict() if isinstance(parent1, nn.Module) else parent1
        p2_state = parent2.state_dict() if isinstance(parent2, nn.Module) else parent2

        # Perform crossover
        child_state = crossover(
            p1_state,
            p2_state,
            crossover_rate=self.crossover_rate,
            method=self.crossover_method,
            slerp_ratio=self.slerp_ratio,
            ties_density=self.ties_density,
        )

        # Apply mutation
        current_mutation_rate = self._get_current_mutation_rate()
        child_state = mutate(
            child_state,
            mutation_rate=current_mutation_rate,
            mutation_scale=self.mutation_scale,
            method=self.mutation_method,
        )

        return child_state

    def _get_current_mutation_rate(self) -> float:
        """Get mutation rate for current generation (with decay)."""
        if not self.adaptive_mutation:
            return self.mutation_rate

        decayed_rate = self.mutation_rate * (self.mutation_decay**self._generation)
        return max(decayed_rate, self.min_mutation_rate)

    def step_generation(self) -> None:
        """Advance to next generation (for adaptive mutation)."""
        self._generation += 1

    def reset(self) -> None:
        """Reset genetics state."""
        self._generation = 0

    @property
    def generation(self) -> int:
        """Current generation number."""
        return self._generation


def blend_state_dicts(
    state_dicts: list[dict[str, torch.Tensor]],
    weights: Optional[list[float]] = None,
) -> dict[str, torch.Tensor]:
    """
    Blend multiple state dictionaries with optional weights.

    Args:
        state_dicts: List of state dictionaries to blend
        weights: Optional weights for each state dict (must sum to 1)

    Returns:
        Blended state dictionary
    """
    if len(state_dicts) == 0:
        raise ValueError("At least one state dict required")

    if len(state_dicts) == 1:
        return deepcopy(state_dicts[0])

    # Default to equal weights
    if weights is None:
        weights = [1.0 / len(state_dicts)] * len(state_dicts)

    assert len(weights) == len(state_dicts), "Weights must match number of state dicts"
    assert abs(sum(weights) - 1.0) < 1e-6, "Weights must sum to 1"

    blended_state = {}
    keys = list(state_dicts[0].keys())

    for key in keys:
        tensors = [sd[key] for sd in state_dicts if key in sd]
        if len(tensors) == len(state_dicts):
            blended = sum(w * t.float() for w, t in zip(weights, tensors))
            blended_state[key] = blended.to(state_dicts[0][key].dtype)
        else:
            blended_state[key] = state_dicts[0][key].clone()

    return blended_state
```

<a id="genesis-core-population-py"></a>

#### `genesis/core/population.py`
*10533 bytes Â· ~2,633 tokens*

```python
"""Population management for evolutionary optimization."""

from dataclasses import dataclass, field
from typing import Any, Callable, Optional
import torch
import torch.nn as nn
import numpy as np
from copy import deepcopy
import uuid
import logging

from genesis.core.genetics import Genetics, crossover, mutate

logger = logging.getLogger(__name__)


@dataclass
class Individual:
    """Represents an individual in the population."""

    id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    state_dict: dict[str, torch.Tensor] = field(default_factory=dict)
    fitness: float = 0.0
    metadata: dict[str, Any] = field(default_factory=dict)
    generation: int = 0
    parent_ids: list[str] = field(default_factory=list)

    def __post_init__(self):
        if not self.metadata:
            self.metadata = {}

    def clone(self) -> "Individual":
        """Create a deep copy of this individual."""
        return Individual(
            id=str(uuid.uuid4())[:8],
            state_dict=deepcopy(self.state_dict),
            fitness=self.fitness,
            metadata=deepcopy(self.metadata),
            generation=self.generation,
            parent_ids=[self.id],
        )

    def __repr__(self) -> str:
        return f"Individual(id={self.id}, fitness={self.fitness:.4f}, gen={self.generation})"


class Population:
    """Manages a population of individuals for evolutionary optimization."""

    def __init__(
        self,
        size: int,
        genetics: Optional[Genetics] = None,
        elite_size: int = 2,
    ):
        """
        Initialize population.

        Args:
            size: Population size
            genetics: Genetics instance for genetic operations
            elite_size: Number of elite individuals to preserve
        """
        self.size = size
        self.genetics = genetics or Genetics()
        self.elite_size = elite_size
        self._individuals: list[Individual] = []
        self._generation = 0
        self._best_fitness_history: list[float] = []
        self._avg_fitness_history: list[float] = []

    def initialize_from_model(
        self,
        model: nn.Module,
        perturbation_scale: float = 0.01,
    ) -> None:
        """
        Initialize population from a base model.

        Args:
            model: Base model to create population from
            perturbation_scale: Scale of random perturbations
        """
        base_state = deepcopy(model.state_dict())
        self._individuals = []

        for i in range(self.size):
            if i == 0:
                # First individual is unperturbed base model
                individual = Individual(
                    state_dict=deepcopy(base_state),
                    generation=0,
                    metadata={"origin": "base"},
                )
            else:
                # Add random perturbations
                perturbed_state = mutate(
                    base_state,
                    mutation_rate=1.0,  # Always mutate for initialization
                    mutation_scale=perturbation_scale,
                    mutation_prob_per_weight=0.5,
                )
                individual = Individual(
                    state_dict=perturbed_state,
                    generation=0,
                    metadata={"origin": "perturbed"},
                )
            self._individuals.append(individual)

        logger.info(f"Initialized population with {self.size} individuals")

    def initialize_from_state_dicts(
        self,
        state_dicts: list[dict[str, torch.Tensor]],
    ) -> None:
        """
        Initialize population from existing state dictionaries.

        Args:
            state_dicts: List of state dictionaries
        """
        self._individuals = []
        for i, state_dict in enumerate(state_dicts):
            individual = Individual(
                state_dict=deepcopy(state_dict),
                generation=0,
                metadata={"origin": f"provided_{i}"},
            )
            self._individuals.append(individual)

        # Fill remaining slots with mutations if needed
        while len(self._individuals) < self.size:
            base = np.random.choice(self._individuals)
            mutated_state = mutate(
                base.state_dict,
                mutation_rate=1.0,
                mutation_scale=0.01,
            )
            individual = Individual(
                state_dict=mutated_state,
                generation=0,
                parent_ids=[base.id],
                metadata={"origin": "mutation"},
            )
            self._individuals.append(individual)

    def evaluate(
        self,
        fitness_fn: Callable[[dict[str, torch.Tensor]], float],
    ) -> None:
        """
        Evaluate fitness of all individuals.

        Args:
            fitness_fn: Function that takes state_dict and returns fitness score
        """
        for individual in self._individuals:
            individual.fitness = fitness_fn(individual.state_dict)

        # Sort by fitness (descending)
        self._individuals.sort(key=lambda x: x.fitness, reverse=True)

        # Update history
        self._best_fitness_history.append(self.best.fitness)
        self._avg_fitness_history.append(self.average_fitness)

        logger.info(
            f"Generation {self._generation}: "
            f"Best={self.best.fitness:.4f}, "
            f"Avg={self.average_fitness:.4f}"
        )

    def evolve(self) -> None:
        """
        Create next generation through selection, crossover, and mutation.
        """
        new_individuals = []

        # Elitism: preserve top individuals
        elites = self._individuals[: self.elite_size]
        for elite in elites:
            elite_copy = elite.clone()
            elite_copy.generation = self._generation + 1
            elite_copy.metadata["origin"] = "elite"
            new_individuals.append(elite_copy)

        # Create offspring for remaining slots
        while len(new_individuals) < self.size:
            # Tournament selection for parents
            parent1 = self._tournament_select()
            parent2 = self._tournament_select()

            # Create offspring
            child_state = self.genetics.create_offspring(
                parent1.state_dict,
                parent2.state_dict,
            )

            child = Individual(
                state_dict=child_state,
                generation=self._generation + 1,
                parent_ids=[parent1.id, parent2.id],
                metadata={"origin": "offspring"},
            )
            new_individuals.append(child)

        self._individuals = new_individuals
        self._generation += 1
        self.genetics.step_generation()

    def _tournament_select(self, tournament_size: int = 3) -> Individual:
        """Select individual using tournament selection."""
        actual_size = min(tournament_size, len(self._individuals))
        tournament = np.random.choice(self._individuals, size=actual_size, replace=False)
        return max(tournament, key=lambda x: x.fitness)

    @property
    def individuals(self) -> list[Individual]:
        """Return all individuals."""
        return self._individuals

    @property
    def best(self) -> Individual:
        """Return best individual by fitness."""
        return max(self._individuals, key=lambda x: x.fitness)

    @property
    def worst(self) -> Individual:
        """Return worst individual by fitness."""
        return min(self._individuals, key=lambda x: x.fitness)

    @property
    def average_fitness(self) -> float:
        """Return average fitness of population."""
        return sum(ind.fitness for ind in self._individuals) / len(self._individuals)

    @property
    def fitness_std(self) -> float:
        """Return standard deviation of fitness."""
        fitnesses = [ind.fitness for ind in self._individuals]
        return float(np.std(fitnesses))

    @property
    def generation(self) -> int:
        """Current generation number."""
        return self._generation

    @property
    def diversity(self) -> float:
        """
        Compute population diversity based on parameter variance.

        Returns:
            Average standard deviation across all parameters
        """
        if len(self._individuals) < 2:
            return 0.0

        keys = list(self._individuals[0].state_dict.keys())
        total_std = 0.0
        count = 0

        for key in keys:
            params = torch.stack([ind.state_dict[key].float() for ind in self._individuals])
            total_std += params.std(dim=0).mean().item()
            count += 1

        return total_std / count if count > 0 else 0.0

    def get_state(self) -> dict[str, Any]:
        """Get population state for checkpointing."""
        return {
            "generation": self._generation,
            "size": self.size,
            "elite_size": self.elite_size,
            "individuals": [
                {
                    "id": ind.id,
                    "state_dict": ind.state_dict,
                    "fitness": ind.fitness,
                    "metadata": ind.metadata,
                    "generation": ind.generation,
                    "parent_ids": ind.parent_ids,
                }
                for ind in self._individuals
            ],
            "best_fitness_history": self._best_fitness_history,
            "avg_fitness_history": self._avg_fitness_history,
        }

    def load_state(self, state: dict[str, Any]) -> None:
        """Load population state from checkpoint."""
        self._generation = state["generation"]
        self.size = state["size"]
        self.elite_size = state["elite_size"]
        self._best_fitness_history = state["best_fitness_history"]
        self._avg_fitness_history = state["avg_fitness_history"]

        self._individuals = []
        for ind_state in state["individuals"]:
            individual = Individual(
                id=ind_state["id"],
                state_dict=ind_state["state_dict"],
                fitness=ind_state["fitness"],
                metadata=ind_state["metadata"],
                generation=ind_state["generation"],
                parent_ids=ind_state["parent_ids"],
            )
            self._individuals.append(individual)

    def __len__(self) -> int:
        return len(self._individuals)

    def __getitem__(self, idx: int) -> Individual:
        return self._individuals[idx]

    def __iter__(self):
        return iter(self._individuals)
```

<a id="genesis-core-selection-py"></a>

#### `genesis/core/selection.py`
*10101 bytes Â· ~2,525 tokens*

```python
"""Selection strategies for evolutionary optimization."""

from abc import ABC, abstractmethod
from typing import Optional
import numpy as np
import logging

from genesis.core.population import Individual

logger = logging.getLogger(__name__)


class SelectionStrategy(ABC):
    """Abstract base class for selection strategies."""

    @abstractmethod
    def select(
        self,
        population: list[Individual],
        num_to_select: int,
    ) -> list[Individual]:
        """
        Select individuals from the population.

        Args:
            population: List of individuals to select from
            num_to_select: Number of individuals to select

        Returns:
            List of selected individuals
        """
        pass

    def __call__(
        self,
        population: list[Individual],
        num_to_select: int,
    ) -> list[Individual]:
        return self.select(population, num_to_select)


class ElitismSelection(SelectionStrategy):
    """
    Elitism selection: always select the top N individuals by fitness.

    This ensures the best solutions are preserved across generations.
    """

    def __init__(self, preserve_ratio: float = 0.1):
        """
        Args:
            preserve_ratio: Ratio of population to preserve as elites
        """
        self.preserve_ratio = preserve_ratio

    def select(
        self,
        population: list[Individual],
        num_to_select: int,
    ) -> list[Individual]:
        # Sort by fitness descending
        sorted_pop = sorted(population, key=lambda x: x.fitness, reverse=True)

        # Select top individuals
        return sorted_pop[:num_to_select]


class TournamentSelection(SelectionStrategy):
    """
    Tournament selection: randomly sample k individuals and select the best.

    This provides a balance between selection pressure and diversity.
    """

    def __init__(self, tournament_size: int = 3):
        """
        Args:
            tournament_size: Number of individuals in each tournament
        """
        self.tournament_size = tournament_size

    def select(
        self,
        population: list[Individual],
        num_to_select: int,
    ) -> list[Individual]:
        selected = []

        for _ in range(num_to_select):
            # Randomly select tournament participants
            tournament_indices = np.random.choice(
                len(population),
                size=min(self.tournament_size, len(population)),
                replace=False,
            )
            tournament = [population[i] for i in tournament_indices]

            # Select winner (highest fitness)
            winner = max(tournament, key=lambda x: x.fitness)
            selected.append(winner)

        return selected


class RouletteWheelSelection(SelectionStrategy):
    """
    Roulette wheel (fitness proportionate) selection.

    Probability of selection is proportional to fitness.
    """

    def __init__(self, scaling: str = "linear"):
        """
        Args:
            scaling: Fitness scaling method ('linear', 'rank', 'sigma')
        """
        self.scaling = scaling

    def select(
        self,
        population: list[Individual],
        num_to_select: int,
    ) -> list[Individual]:
        fitnesses = np.array([ind.fitness for ind in population])

        # Handle negative fitness values
        if fitnesses.min() < 0:
            fitnesses = fitnesses - fitnesses.min() + 1e-8

        # Apply scaling
        if self.scaling == "rank":
            # Rank-based scaling
            ranks = np.argsort(np.argsort(fitnesses)) + 1
            probabilities = ranks / ranks.sum()
        elif self.scaling == "sigma":
            # Sigma scaling
            mean = fitnesses.mean()
            std = fitnesses.std() + 1e-8
            scaled = 1 + (fitnesses - mean) / (2 * std)
            scaled = np.maximum(scaled, 0)
            probabilities = scaled / scaled.sum()
        else:
            # Linear scaling (default)
            probabilities = fitnesses / fitnesses.sum()

        # Select individuals
        indices = np.random.choice(
            len(population),
            size=num_to_select,
            replace=True,
            p=probabilities,
        )

        return [population[i] for i in indices]


class RankSelection(SelectionStrategy):
    """
    Rank-based selection: selection probability based on rank, not raw fitness.

    This reduces the dominance of very fit individuals.
    """

    def __init__(self, selection_pressure: float = 2.0):
        """
        Args:
            selection_pressure: Controls selection intensity (1.0 = uniform, 2.0 = linear)
        """
        self.selection_pressure = selection_pressure

    def select(
        self,
        population: list[Individual],
        num_to_select: int,
    ) -> list[Individual]:
        n = len(population)

        # Sort by fitness
        sorted_pop = sorted(population, key=lambda x: x.fitness, reverse=True)

        # Compute rank-based probabilities (linear ranking)
        sp = self.selection_pressure
        probabilities = np.array([
            (2 - sp + 2 * (sp - 1) * (n - 1 - i) / (n - 1)) / n
            for i in range(n)
        ])
        probabilities = probabilities / probabilities.sum()

        # Select individuals
        indices = np.random.choice(n, size=num_to_select, replace=True, p=probabilities)

        return [sorted_pop[i] for i in indices]


class TruncationSelection(SelectionStrategy):
    """
    Truncation selection: only the top fraction of the population reproduces.

    Simple but effective for strong selection pressure.
    """

    def __init__(self, truncation_rate: float = 0.5):
        """
        Args:
            truncation_rate: Fraction of population eligible for selection
        """
        self.truncation_rate = truncation_rate

    def select(
        self,
        population: list[Individual],
        num_to_select: int,
    ) -> list[Individual]:
        # Sort by fitness
        sorted_pop = sorted(population, key=lambda x: x.fitness, reverse=True)

        # Truncate
        cutoff = max(1, int(len(sorted_pop) * self.truncation_rate))
        eligible = sorted_pop[:cutoff]

        # Randomly select from eligible
        indices = np.random.choice(len(eligible), size=num_to_select, replace=True)

        return [eligible[i] for i in indices]


class BoltzmannSelection(SelectionStrategy):
    """
    Boltzmann selection: uses temperature-controlled softmax probabilities.

    High temperature = more exploration, low temperature = more exploitation.
    """

    def __init__(
        self,
        initial_temperature: float = 10.0,
        min_temperature: float = 0.1,
        cooling_rate: float = 0.95,
    ):
        """
        Args:
            initial_temperature: Starting temperature
            min_temperature: Minimum temperature
            cooling_rate: Temperature decay per generation
        """
        self.temperature = initial_temperature
        self.min_temperature = min_temperature
        self.cooling_rate = cooling_rate

    def select(
        self,
        population: list[Individual],
        num_to_select: int,
    ) -> list[Individual]:
        fitnesses = np.array([ind.fitness for ind in population])

        # Normalize fitness for numerical stability
        fitnesses_norm = (fitnesses - fitnesses.mean()) / (fitnesses.std() + 1e-8)

        # Boltzmann probabilities
        exp_values = np.exp(fitnesses_norm / self.temperature)
        probabilities = exp_values / exp_values.sum()

        # Select individuals
        indices = np.random.choice(
            len(population),
            size=num_to_select,
            replace=True,
            p=probabilities,
        )

        return [population[i] for i in indices]

    def cool_down(self) -> None:
        """Reduce temperature for next generation."""
        self.temperature = max(
            self.min_temperature,
            self.temperature * self.cooling_rate,
        )


class SteadyStateSelection(SelectionStrategy):
    """
    Steady-state selection: replace only a few individuals each generation.

    Maintains population diversity by making gradual changes.
    """

    def __init__(
        self,
        replacement_rate: float = 0.2,
        parent_selection: Optional[SelectionStrategy] = None,
    ):
        """
        Args:
            replacement_rate: Fraction of population to replace
            parent_selection: Strategy for selecting parents
        """
        self.replacement_rate = replacement_rate
        self.parent_selection = parent_selection or TournamentSelection()

    def select(
        self,
        population: list[Individual],
        num_to_select: int,
    ) -> list[Individual]:
        # Determine how many to replace
        num_to_replace = max(1, int(len(population) * self.replacement_rate))

        # Sort by fitness
        sorted_pop = sorted(population, key=lambda x: x.fitness, reverse=True)

        # Keep the best individuals
        num_to_keep = len(population) - num_to_replace
        survivors = sorted_pop[:num_to_keep]

        # Select parents for new offspring
        parents = self.parent_selection.select(sorted_pop, num_to_replace)

        return survivors + parents


def create_selection_strategy(
    strategy_type: str,
    **kwargs,
) -> SelectionStrategy:
    """
    Factory function to create selection strategies.

    Args:
        strategy_type: Type of selection strategy
        **kwargs: Additional arguments for the strategy

    Returns:
        SelectionStrategy instance
    """
    strategies = {
        "elitism": ElitismSelection,
        "tournament": TournamentSelection,
        "roulette": RouletteWheelSelection,
        "rank": RankSelection,
        "truncation": TruncationSelection,
        "boltzmann": BoltzmannSelection,
        "steady_state": SteadyStateSelection,
    }

    if strategy_type not in strategies:
        raise ValueError(f"Unknown selection strategy: {strategy_type}")

    return strategies[strategy_type](**kwargs)
```


### ğŸ“¦ Source

<a id="gitignore"></a>

#### `.gitignore`
*1822 bytes Â· ~455 tokens*

```
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff
instance/
.webassets-cache

# Scrapy stuff
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
Pipfile.lock

# PEP 582
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# VS Code
.vscode/

# PyCharm
.idea/

# Model files
*.pt
*.pth
*.bin
*.safetensors
*.ckpt

# Large data files
*.zip
*.tar.gz
*.tar
*.h5
*.hdf5

# Genesis specific
checkpoints/
outputs/
logs/
wandb/
mlruns/
ray_results/
*.npy
*.npz
data/raw/
data/processed/

# Experiment outputs
experiments/*/outputs/
experiments/*/logs/
experiments/*/checkpoints/

# TensorBoard logs
runs/

# Audio files (TTS)
*.wav
*.mp3
*.flac
*.ogg

# Temp files
*.tmp
*.temp
.DS_Store
Thumbs.db
```

<a id="license"></a>

#### `LICENSE`
*1069 bytes Â· ~267 tokens*

```
MIT License

Copyright (c) 2024 Genesis Team

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

<a id="lab-grade-update-plan-md"></a>

#### `Lab-grade-update-plan.md`
*11094 bytes Â· ~2,757 tokens*

```markdown
Plan de AcciÃ³n: Genesis Research-Grade
ğŸ”´ FASE 1: SoluciÃ³n de Bugs MatemÃ¡ticos (Hotfixes CrÃ­ticos)
Objetivo: Evitar colapsos silenciosos por NaNs y eliminar el "ruido entrÃ³pico" que destruye el aprendizaje del estudiante.

1.1 DivisiÃ³n por cero en SLERP
Si dos individuos genÃ©ticos apuntan en direcciones opuestas (antiparalelos), la funciÃ³n falla catastrÃ³ficamente.
SoluciÃ³n: Remplaza la funciÃ³n en genesis/core/genetics.py para usar un fallback a interpolaciÃ³n lineal (LERP) en los lÃ­mites.

Python

# Reemplazar en genesis/core/genetics.py
def slerp(t: float, v0: torch.Tensor, v1: torch.Tensor, epsilon: float = 1e-8) -> torch.Tensor:
    v0_flat, v1_flat = v0.flatten().float(), v1.flatten().float()

    v0_norm = v0_flat / (torch.norm(v0_flat) + epsilon)
    v1_norm = v1_flat / (torch.norm(v1_flat) + epsilon)

    dot = torch.clamp(torch.sum(v0_norm * v1_norm), -1.0, 1.0)
    
    # FIX: Manejar vectores colineales (idÃ©nticos) o diametralmente opuestos
    if torch.abs(dot) > 0.9995:
        # Fallback suave a LERP para evitar divisiÃ³n por cero
        result = (1.0 - t) * v0_flat + t * v1_flat
    else:
        theta = torch.acos(dot)
        sin_theta = torch.sin(theta)
        result = (torch.sin((1.0 - t) * theta) / sin_theta) * v0_flat + \
                 (torch.sin(t * theta) / sin_theta) * v1_flat

    # Mantener la magnitud estable
    original_magnitude = (torch.norm(v0_flat) + torch.norm(v1_flat)) / 2.0
    result = result / (torch.norm(result) + epsilon) * original_magnitude

    return result.view(v0.shape).to(v0.dtype)
1.2 DestilaciÃ³n Top-K Libre de Ruido
Ollama devuelve las probabilidades del Top-20, pero el cÃ³digo actual rellena uniformemente los 151,000 tokens restantes. Esto hace que el modelo intente aprender "ruido plano", degradando su lenguaje.
SoluciÃ³n: Aislar el KL Divergence exclusivamente a los tokens devueltos.

Python

# AÃ±adir a genesis/distillation/kd_loss.py
def topk_kl_divergence_loss(
    student_logits: torch.Tensor,
    teacher_probs: torch.Tensor, # Probabilidades ya suavizadas del maestro
    temperature: float = 4.0
) -> torch.Tensor:
    """Calcula el KL Divergence aislando exclusivamente el Top-K del maestro (sin ruido)."""
    # MÃ¡scara para operar solo donde el maestro tiene confianza (> 0)
    mask = (teacher_probs > 0).float()
    
    student_log_probs = torch.nn.functional.log_softmax(student_logits / temperature, dim=-1)
    teacher_log_probs = torch.log(teacher_probs + 1e-8) # Epsilon para evitar log(0)
    
    # D_KL = P * (log(P) - log(Q))
    kl_div = teacher_probs * (teacher_log_probs - student_log_probs)
    
    # Aplicar mÃ¡scara y hacer la media solo sobre los elementos vÃ¡lidos
    masked_loss = (kl_div * mask).sum(dim=-1)
    loss = masked_loss.sum() / torch.clamp(mask.sum(), min=1.0)
    
    return loss * (temperature**2)
ğŸŸ¡ FASE 2: Avances ArquitectÃ³nicos (NeuroevoluciÃ³n SOTA)
Objetivo: Adoptar mÃ©todos modernos para evitar que la red se degrade al mezclar pesos y acelerar la convergencia.

2.1 Crossover TIES-Merging (ResoluciÃ³n de Interferencia)
Cruzar adaptadores LoRA directamente (incluso con SLERP) destruye el conocimiento debido a la interferencia de signos. Usaremos el algoritmo acadÃ©mico TIES (Trim, Elect Sign, Merge).

Python

# AÃ±adir a genesis/core/genetics.py y usarlo como mÃ©todo en config.yaml (crossover_method: "ties")
def ties_crossover(parent1_state: dict, parent2_state: dict, density: float = 0.2) -> dict:
    """TIES-Merging: Cruce genÃ©tico que previene la destrucciÃ³n de los pesos."""
    child_state = {}
    for key in parent1_state.keys():
        if key not in parent2_state:
            child_state[key] = parent1_state[key].clone()
            continue
            
        p1, p2 = parent1_state[key].cpu(), parent2_state[key].cpu()
        
        # 1. TRIM (Nos quedamos solo con los pesos mÃ¡s fuertes por magnitud)
        k1 = max(1, int(p1.numel() * density))
        k2 = max(1, int(p2.numel() * density))
        t1 = torch.kthvalue(p1.abs().flatten(), p1.numel() - k1 + 1).values
        t2 = torch.kthvalue(p2.abs().flatten(), p2.numel() - k2 + 1).values
        
        p1_trim = torch.where(p1.abs() >= t1, p1, torch.zeros_like(p1))
        p2_trim = torch.where(p2.abs() >= t2, p2, torch.zeros_like(p2))
        
        # 2. ELECT SIGN (VotaciÃ³n de direcciÃ³n matemÃ¡tica para evitar colisiones)
        sign_mask = torch.sign(p1_trim + p2_trim)
        
        # 3. DISJOINT MERGE (Promediar solo los pesos que apoyan la direcciÃ³n ganadora)
        p1_aligned = torch.where(torch.sign(p1_trim) == sign_mask, p1_trim, torch.zeros_like(p1))
        p2_aligned = torch.where(torch.sign(p2_trim) == sign_mask, p2_trim, torch.zeros_like(p2))
        
        count = (p1_aligned != 0).float() + (p2_aligned != 0).float()
        child_state[key] = (p1_aligned + p2_aligned) / count.clamp(min=1.0)
        child_state[key] = child_state[key].to(parent1_state[key].dtype)
        
    return child_state
2.2 EvoluciÃ³n Lamarckiana (MemÃ©tica)
Los algoritmos genÃ©ticos puros son malos para el ajuste fino. Permitiremos que cada "hijo" haga unos cuantos pasos de descenso de gradiente antes de ser evaluado, heredando lo aprendido a sus genes.

Python

# Modificar el mÃ©todo `_evaluate_population` en genesis/optimizer.py
    def _evaluate_population(self) -> None:
        for individual in self._population.individuals:
            self.student.load_state_dict(individual.state_dict, strict=False)
            
            # --- MICRO-ENTRENAMIENTO LAMARCKIANO ---
            memetic_steps = getattr(self.config.genetic, 'memetic_steps', 5)
            if memetic_steps > 0 and self.train_dataloader:
                self.student.model.train()
                optimizer = torch.optim.AdamW(
                    [p for p in self.student.model.parameters() if p.requires_grad], lr=5e-5
                )
                
                batch_iter = iter(self.train_dataloader)
                for _ in range(memetic_steps):
                    try:
                        batch = next(batch_iter)
                        optimizer.zero_grad()
                        # Entrenamiento rÃ¡pido usando etiquetas duras
                        outputs = self.student.forward(
                            batch["input_ids"], attention_mask=batch.get("attention_mask"), 
                            labels=batch.get("labels", batch["input_ids"])
                        )
                        outputs["loss"].backward()
                        optimizer.step()
                    except StopIteration:
                        break
                        
                # El individuo actualiza su "genoma" con lo que aprendiÃ³ en vida
                individual.state_dict = self.student.get_state_dict(lora_only=True)
            
            # --- EVALUACIÃ“N DE FITNESS ---
            self.student.model.eval()
            result = self._fitness_evaluator.evaluate(self.student.model)
            individual.fitness = result.score
ğŸŸ£ FASE 3: EliminaciÃ³n de Cuellos de Botella I/O y Hardware
Objetivo: Exprimir tus GPUs, reducir VRAM e impedir que PyTorch espere a la red.

3.1 I/O AsÃ­ncrono para el Maestro Ollama
El bucle for sÃ­ncrono actual causa inaniciÃ³n de GPU (0% de uso).

Python

# AÃ±adir a genesis/models/ollama_teacher.py
import asyncio
import aiohttp

class AsyncOllamaTeacher(OllamaTeacher):
    async def _fetch_single(self, session, text, seq_len, temperature):
        payload = {
            "model": self.model_name, "prompt": text, "max_tokens": seq_len,
            "temperature": temperature, "logprobs": self.top_logprobs, "stream": False
        }
        try:
            async with session.post(f"{self.base_url}/v1/completions", json=payload) as resp:
                if resp.status == 200: return await resp.json()
        except: pass
        return None

    async def _fetch_batch(self, texts, seq_len, temperature):
        async with aiohttp.ClientSession() as session:
            tasks = [self._fetch_single(session, t, seq_len, temperature) for t in texts]
            return await asyncio.gather(*tasks)

    # Reemplazar tu get_soft_targets() actual por esto:
    def get_soft_targets(self, input_ids: torch.Tensor, attention_mask=None, temperature=1.0):
        texts = self._ids_to_text(input_ids)
        
        try: loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
        responses = loop.run_until_complete(self._fetch_batch(texts, input_ids.shape[1], temperature))
        # ... (Tu lÃ³gica de formatear respuestas va aquÃ­, asegurando usar tensores en ceros)
3.2 Poda (Pruning) 2:4 Acelerada por Hardware (NVIDIA Tensor Cores)
Multiplicar la matriz por 0 (lo que hace actualmente tu Pruner) no ahorra memoria VRAM. Las RTX 5090 soportan matrices dispersas nativas (SparseSemiStructured).

Python

# AÃ±adir a genesis/pruning/pruner.py
    def apply_nvidia_2_4_sparsity(self) -> dict[str, float]:
        """Aplica compresiÃ³n fÃ­sica 2:4 usando Tensor Cores (Ahorra 50% de RAM real)."""
        try:
            from torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor
            SparseSemiStructuredTensor._FORCE_INT32_PTX = True
        except ImportError:
            logger.error("Se requiere PyTorch 2.1+ para Sparsidad 2:4")
            return {}

        logger.info("Colapsando matrices densas a Sparsidad 2:4...")
        total_params, pruned_params = 0, 0
        
        with torch.no_grad():
            for name, module in self.model.named_modules():
                # Aplicar al base model, ignorar LoRA
                if isinstance(module, torch.nn.Linear) and "lora_" not in name:
                    weight = module.weight
                    
                    # Hardware requirement: dimensiones mÃºltiplos de 16
                    if weight.shape[0] % 8 == 0 and weight.shape[1] % 16 == 0:
                        total_params += weight.numel()
                        
                        # 1. Crear mÃ¡scara 2:4 por magnitud
                        reshaped = weight.abs().view(-1, 4)
                        _, indices = torch.topk(reshaped, k=2, dim=1, largest=False)
                        mask = torch.ones_like(reshaped)
                        mask.scatter_(1, indices, 0)
                        mask = mask.view_as(weight)
                        
                        # 2. Comprimir FÃ­sicamente en VRAM
                        weight.data *= mask
                        try:
                            module.weight = torch.nn.Parameter(to_sparse_semi_structured(weight.data))
                            module.weight.requires_grad = False
                            pruned_params += weight.numel() // 2
                        except Exception as e:
                            logger.warning(f"Fallback a densa en {name}: {e}")

        torch.cuda.empty_cache()
        return {"actual_sparsity": pruned_params / max(1, total_params), "method": "nvidia_2:4"}
```

<a id="genesis_plan-md"></a>

#### `genesis_plan.md`
*12973 bytes Â· ~3,210 tokens*

```markdown
ğŸ§¬ Proyecto GÃ©nesis: Laboratorio de EvoluciÃ³n y DestilaciÃ³n de IA Local (Dual 5090 Edition)

Objetivo: Crear modelos de IA altamente eficientes mediante tÃ©cnicas de selecciÃ³n natural (Algoritmos GenÃ©ticos), poda sinÃ¡ptica (Pruning) y transferencia de conocimiento (Distillation), ejecutado en un entorno de multi-GPU de nivel entusiasta.

ğŸ–¥ï¸ Arquitectura de Hardware (El Ecosistema Gemelo)

Con dos RTX 5090, eliminamos el cuello de botella de la memoria del "Estudiante". Ahora podemos paralelizar la inferencia del Maestro o doblar la velocidad de entrenamiento de la poblaciÃ³n.

Componente

Rol en el Laboratorio

Tarea EspecÃ­fica

NVIDIA RTX 5090 (GPU 0)

El Maestro (The Teacher)

Ejecuta modelos masivos (Llama-3-70B o incluso 405B cuantizado en 4-bit) para generar Soft Targets de mÃ¡xima calidad.

NVIDIA RTX 5090 (GPU 1)

El Coliseo (The Coliseum)

Ejecuta el entrenamiento de la poblaciÃ³n de "Hijos" en paralelo masivo. Gracias a los 32GB+ VRAM, puedes cargar lotes mucho mayores o evaluar mÃºltiples hijos simultÃ¡neamente.

CPU (Ryzen)

El Orquestador

Ejecuta el Algoritmo GenÃ©tico (Ray/DEAP) y gestiona el flujo de datos entre las dos GPUs vÃ­a PCIe.

ğŸ”¬ Enfoque 1: LLM MÃ©dico (El Cerebro)

Meta: Crear un modelo de 7B/8B parÃ¡metros capaz de razonar sobre notas clÃ­nicas con la precisiÃ³n de un modelo de 70B+, aprovechando el ancho de banda masivo de las 5090.

Fase A: PreparaciÃ³n del "Cuerpo" (Pruning)

Antes de evolucionar, necesitamos un cuerpo Ã¡gil.

Herramienta: LLM-Pruner o SparseGPT.

AcciÃ³n: Eliminar el 20% de las capas menos activas del modelo base (ej. BioMistral-7B).

Ventaja Dual-GPU: Puedes calcular la importancia de los pesos (saliency maps) en la GPU 1 mientras la GPU 0 valida la integridad del modelo en tiempo real.

Fase B: El Ciclo Evolutivo (NeuroevoluciÃ³n de LoRAs)

1. PoblaciÃ³n Inicial (GeneraciÃ³n 0)

Creamos 10-20 adaptadores LoRA distintos con diferentes semillas y rangos (Rank 8, 16, 32).

2. EvaluaciÃ³n Paralela (Fitness Function)

Estrategia: Data Parallelism. Dividimos el dataset de validaciÃ³n en dos. La mitad se evalÃºa en GPU 0 (cuando no actÃºa de maestro) y la otra en GPU 1 para duplicar la velocidad de evaluaciÃ³n.

Dataset: PubMedQA o notas mÃ©dicas anonimizadas.

3. ReproducciÃ³n (Cruce Padre-Hijo)

Usaremos Mergekit con la tÃ©cnica SLERP o TIES.

ğŸ§¬ GenÃ©tica de CÃ³digo: La fusiÃ³n de matrices es una operaciÃ³n intensiva en memoria. Con la 5090, puedes fusionar modelos sin tener que descargarlos a la RAM del sistema (CPU offloading), acelerando el proceso x10.

4. DestilaciÃ³n (El Maestro EnseÃ±a)

El modelo resultante se refina. AquÃ­ usaremos la GPU 0 dedicada exclusivamente a inferir el modelo gigante (Teacher) y la GPU 1 dedicada exclusivamente a ajustar los pesos del alumno (Student), sincronizando solo los gradientes/logits.

ğŸ› ï¸ CÃ³digo de ImplementaciÃ³n Completo (Script Maestro)

Este es el cÃ³digo completo para orquestar la evoluciÃ³n y destilaciÃ³n entre las dos GPUs.

import torch
import torch.nn as nn
import copy
import random
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, LoraConfig, get_peft_model

# --- CONFIGURACIÃ“N DE HARDWARE (DUAL RTX 5090) ---
# GPU 0: Maestro (Teacher) - Inferencia pesada
# GPU 1: Coliseo (Student/Population) - Entrenamiento y EvaluaciÃ³n masiva
TEACHER_DEVICE = "cuda:0"
STUDENT_DEVICE = "cuda:1"

# ConfiguraciÃ³n GenÃ©tica
POPULATION_SIZE = 10
GENERATIONS = 5
MUTATION_RATE = 0.1
ELITISM_COUNT = 2  # Los 2 mejores pasan intactos

class EvolutionaryOptimizer:
    def __init__(self, base_model_name, teacher_model_name):
        print(f"ğŸš€ Iniciando Laboratorio Evolutivo en Dual GPU...")
        
        # 1. Cargar el Maestro en GPU 0 (FP16 para velocidad)
        print(f"Loading Teacher on {TEACHER_DEVICE}...")
        self.teacher = AutoModelForCausalLM.from_pretrained(
            teacher_model_name, 
            torch_dtype=torch.float16,
            device_map=TEACHER_DEVICE
        )
        self.teacher.eval() # El maestro solo evalÃºa/enseÃ±a

        # 2. Cargar el Modelo Base del Estudiante en GPU 1
        # Nota: Este modelo base se comparte, lo que cambia son los adaptadores LoRA
        print(f"Loading Student Base on {STUDENT_DEVICE}...")
        self.student_base = AutoModelForCausalLM.from_pretrained(
            base_model_name, 
            torch_dtype=torch.bfloat16, 
            device_map=STUDENT_DEVICE
        )
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)

        # 3. Inicializar PoblaciÃ³n (Lista de Configs/Pesos de LoRA)
        self.population = self._initialize_population()

    def _initialize_population(self):
        """Crea la generaciÃ³n 0 con variaciones aleatorias de LoRA."""
        pop = []
        for i in range(POPULATION_SIZE):
            # Variar rangos para diversidad genÃ©tica
            rank = random.choice([8, 16, 32])
            config = LoraConfig(
                r=rank,
                lora_alpha=rank*2,
                target_modules=["q_proj", "v_proj"],
                lora_dropout=0.05,
                bias="none",
                task_type="CAUSAL_LM"
            )
            # Creamos un modelo temporal para inicializar pesos y guardarlos en RAM/Disco
            temp_model = get_peft_model(self.student_base, config)
            state_dict = {k: v.cpu() for k, v in temp_model.peft_config['default'].items() if 'lora' in k} 
            # Nota: En un caso real, guardamos el state_dict de los pesos LoRA, no el modelo entero
            pop.append({'config': config, 'weights': temp_model.state_dict(), 'id': i})
            print(f"  -> Individuo {i} creado (Rank {rank})")
        return pop

    def _slerp(self, t1, t2, val):
        """
        Spherical Linear Interpolation: Cruce geomÃ©trico de tensores.
        Mejor que el promedio simple para redes neuronales.
        """
        # SimplificaciÃ³n para tensores: interpolaciÃ³n lineal si son 1D, esfÃ©rica si son matrices
        return (1 - val) * t1 + val * t2

    def crossover(self, parent_a, parent_b):
        """Crea un hijo mezclando los pesos de dos padres."""
        child_weights = {}
        # Asumimos arquitecturas compatibles (mismo rank) para simplificar
        # Si tienen ranks distintos, se requiere padding (lÃ³gica avanzada omitida)
        
        mix_ratio = random.uniform(0.3, 0.7)
        
        for key in parent_a['weights']:
            if key in parent_b['weights']:
                w_a = parent_a['weights'][key].to(STUDENT_DEVICE)
                w_b = parent_b['weights'][key].to(STUDENT_DEVICE)
                
                # Cruce SLERP
                w_child = self._slerp(w_a, w_b, mix_ratio)
                
                # MutaciÃ³n
                if random.random() < MUTATION_RATE:
                    noise = torch.randn_like(w_child) * 0.02
                    w_child += noise
                
                child_weights[key] = w_child.cpu()
            else:
                child_weights[key] = parent_a['weights'][key] # Heredar del padre A por defecto

        return {'config': parent_a['config'], 'weights': child_weights, 'id': -1}

    def evaluate_fitness(self, individual, validation_dataset):
        """
        EvalÃºa quÃ© tan bueno es un individuo.
        Usa GPU 1 para inferencia rÃ¡pida del estudiante.
        """
        # Cargar pesos en el modelo base (LÃ³gica simplificada)
        # self.student_base.set_adapter(...) 
        
        loss_accum = 0
        with torch.no_grad():
            for batch in validation_dataset:
                inputs = self.tokenizer(batch['text'], return_tensors="pt").to(STUDENT_DEVICE)
                outputs = self.student_base(**inputs, labels=inputs["input_ids"])
                loss_accum += outputs.loss.item()
        
        # Fitness = 1 / Loss (Menor loss es mejor fitness)
        return 1.0 / (loss_accum / len(validation_dataset) + 1e-6)

    def distillation_step(self, student_adapter, batch_data):
        """
        Fase de Entrenamiento: El Maestro (GPU 0) enseÃ±a al Estudiante (GPU 1).
        """
        inputs = self.tokenizer(batch_data, return_tensors="pt")
        
        # 1. Inferencia del Maestro (GPU 0)
        input_teacher = inputs.to(TEACHER_DEVICE)
        with torch.no_grad():
            teacher_logits = self.teacher(**input_teacher).logits
        
        # Mover logits del maestro a la GPU del estudiante para calcular loss
        teacher_logits = teacher_logits.to(STUDENT_DEVICE)
        
        # 2. Entrenamiento del Estudiante (GPU 1)
        input_student = inputs.to(STUDENT_DEVICE)
        # Activar adaptador del estudiante...
        student_outputs = self.student_base(**input_student)
        student_logits = student_outputs.logits
        
        # 3. Calcular Loss (KL Divergence + Cross Entropy)
        temperature = 2.0
        loss_kd = nn.functional.kl_div(
            nn.functional.log_softmax(student_logits / temperature, dim=-1),
            nn.functional.softmax(teacher_logits / temperature, dim=-1),
            reduction='batchmean'
        ) * (temperature ** 2)
        
        return loss_kd

    def run_evolution(self, dataset):
        """Bucle principal de evoluciÃ³n."""
        for gen in range(GENERATIONS):
            print(f"\n--- GeneraciÃ³n {gen} ---")
            
            # 1. Evaluar Fitness
            scores = []
            for ind in self.population:
                fitness = self.evaluate_fitness(ind, dataset)
                scores.append((fitness, ind))
            
            # Ordenar por mejor fitness
            scores.sort(key=lambda x: x[0], reverse=True)
            print(f"Top Fitness: {scores[0][0]:.4f}")
            
            # 2. SelecciÃ³n (Elitismo)
            survivors = [s[1] for s in scores[:ELITISM_COUNT]]
            
            # 3. ReproducciÃ³n
            while len(survivors) < POPULATION_SIZE:
                # Torneo simple para elegir padres
                parent_a = random.choice(scores[:5])[1]
                parent_b = random.choice(scores[:5])[1]
                
                child = self.crossover(parent_a, parent_b)
                child['id'] = len(survivors) + (gen * 100)
                survivors.append(child)
            
            self.population = survivors
            
        print("EvoluciÃ³n completada. Guardando el mejor modelo...")

# --- EJECUCIÃ“N ---
if __name__ == "__main__":
    # Datos dummy para probar
    dummy_dataset = [{'text': "El paciente presenta fiebre alta."}]
    
    lab = EvolutionaryOptimizer(
        base_model_name="BioMistral/BioMistral-7B", 
        teacher_model_name="meta-llama/Meta-Llama-3-70B"
    )
    
    lab.run_evolution(dummy_dataset)


ğŸ—£ï¸ Enfoque 2: TTS (La Voz)

Meta: Sintetizar voz a velocidades sobrehumanas para entrenar generaciones en minutos.

Fase A: Pruning Estructural

TÃ©cnica: Channel Pruning.

Mejora Dual: Puedes probar diferentes % de pruning simultÃ¡neamente en cada GPU para encontrar el "Sweet Spot" de calidad/velocidad.

Fase B: EvoluciÃ³n de Estilo (Style Tokens)

PoblaciÃ³n Masiva: Con 48GB+ de VRAM, carga 50+ instancias pequeÃ±as de TTS.

Fitness: CÃ¡lculo de MCD en paralelo.

ğŸ› ï¸ CÃ³digo de ImplementaciÃ³n (Python - TTS)

import torch.nn as nn
import random
import copy

class TTS_Child(nn.Module):
    def __init__(self, parent_genome=None):
        super().__init__()
        self.encoder = self.inherit_genes(parent_genome, component="encoder")
        self.decoder = self.inherit_genes(parent_genome, component="decoder")
        
    def inherit_genes(self, parent, component):
        if parent:
            return copy.deepcopy(getattr(parent, component))
        else:
            return nn.LSTM(512, 256, num_layers=random.choice([2, 4]))

def survival_of_the_fittest_parallel(population, target_audio):
    """
    VersiÃ³n paralelizada para Dual GPU.
    """
    mid = len(population) // 2
    group_0 = population[:mid] # Para GPU 0
    group_1 = population[mid:] # Para GPU 1
    
    # PseudocÃ³digo de lÃ³gica paralela...
    return [] 


ğŸ“… Hoja de Ruta de ImplementaciÃ³n

Semana 1: Infraestructura

Instalar Ubuntu 22.04 / 24.04 LTS.

Configurar Drivers NVIDIA 550+ y CUDA 12.x.

Instalar librerÃ­as clave: Mergekit, PEFT, Ray Tune, Deepspeed.

Semana 2: El Experimento LLM (MÃ©dico)

Descargar Llama-3-8B y Llama-3-70B.

Aplicar SparseGPT al 8B.

Ejecutar el script EvolutionaryOptimizer.

Semana 3: El Experimento TTS

Entrenar modelo base VITS pequeÃ±o.

Aplicar pruning y destilaciÃ³n.

âš ï¸ Consideraciones TÃ©cnicas Actualizadas

NVLink: Si tus 5090 lo soportan, actÃ­valo. Si no, asegura una buena ventilaciÃ³n; dos tarjetas de 600W juntas generarÃ¡n mucho calor.

Fuente de AlimentaciÃ³n (PSU): MÃ­nimo 1600W Titanium/Platinum para manejar picos transitorios.

Model Sharding: Usa Pipeline Parallelism para modelos >24GB que requieran inferencia rÃ¡pida.
```

<a id="docs-installation-md"></a>

#### `docs/installation.md`
*2518 bytes Â· ~629 tokens*

```markdown
# Installation Guide

This guide walks you through installing Genesis and its dependencies.

## Requirements

### System Requirements

- **Python**: 3.9 or higher
- **CUDA**: 11.7 or higher (for GPU support)
- **Operating System**: Linux (recommended), macOS, or Windows

### Hardware Requirements

| Configuration | GPUs | VRAM | Use Case |
|--------------|------|------|----------|
| Minimum | 1x | 16GB | Small models, testing |
| Recommended | 2x | 24GB each | Full dual-GPU mode |
| Optimal | 2x A100/H100 | 40-80GB each | Large models |

## Installation Methods

### Method 1: From Source (Recommended)

```bash
# Clone the repository
git clone https://github.com/genesis-ai/genesis.git
cd genesis

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
.\venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Install Genesis in development mode
pip install -e .
```

### Method 2: Using pip

```bash
pip install genesis-ai
```

### Method 3: Using conda

```bash
conda create -n genesis python=3.10
conda activate genesis
pip install -r requirements.txt
pip install -e .
```

## Dependencies

### Core Dependencies

```
torch>=2.0.0
transformers>=4.36.0
peft>=0.7.0
accelerate>=0.25.0
datasets>=2.16.0
```

### Optional Dependencies

For TTS experiments:
```bash
pip install librosa soundfile pyworld
```

For distributed training:
```bash
pip install ray[tune] deepspeed
```

For documentation:
```bash
pip install mkdocs mkdocs-material mkdocstrings[python]
```

## Verifying Installation

Run the following to verify your installation:

```python
import genesis
print(f"Genesis version: {genesis.__version__}")

# Check GPU availability
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Number of GPUs: {torch.cuda.device_count()}")

# Test basic import
from genesis import EvolutionaryOptimizer, GenesisConfig
print("Installation successful!")
```

## Troubleshooting

### CUDA Out of Memory

If you encounter OOM errors:
1. Reduce `batch_size` in config
2. Enable gradient checkpointing
3. Use smaller population size
4. Enable 8-bit quantization for teacher model

### Import Errors

Ensure you've installed all dependencies:
```bash
pip install -r requirements.txt
```

### Permission Errors

On Linux, you may need to:
```bash
chmod +x scripts/*.sh
```

## Next Steps

- [Quick Start Guide](quickstart.md) - Run your first experiment
- [Architecture Overview](architecture.md) - Understand the system
```

<a id="docs-quickstart-md"></a>

#### `docs/quickstart.md`
*4169 bytes Â· ~1,042 tokens*

```markdown
# Quick Start Guide

Get started with Genesis in just a few minutes.

## Basic Usage

### 1. Create Configuration

```python
from genesis import GenesisConfig

config = GenesisConfig(
    project_name="my_experiment",
    teacher_model="gpt2",  # Use a small model for testing
    use_lora=True,
    genetic=dict(
        population_size=10,
        generations=20,
        mutation_rate=0.1,
    ),
)
```

### 2. Prepare Data

```python
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from genesis.data.datasets import DatasetLoader

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Create dataloader
dataset_loader = DatasetLoader(
    dataset_name="wikitext",
    tokenizer=tokenizer,
    max_length=256,
)

train_loader = dataset_loader.get_dataloader(batch_size=8)
```

### 3. Run Evolution

```python
from genesis import EvolutionaryOptimizer

optimizer = EvolutionaryOptimizer(
    config=config,
    train_dataloader=train_loader,
    eval_dataloader=train_loader,  # Use same for demo
)

# Run evolution
results = optimizer.run()

print(f"Best fitness: {results['best_fitness']:.4f}")
print(f"Generations: {results['generations']}")
```

### 4. Save and Use the Model

```python
# The best model is automatically saved
# Load it for inference
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("./outputs/best_model")
```

## Configuration Options

### Genetic Algorithm

```python
config = GenesisConfig(
    genetic=dict(
        population_size=20,      # Number of individuals
        generations=50,          # Evolution iterations
        mutation_rate=0.1,       # Mutation probability
        crossover_rate=0.7,      # Crossover probability
        elite_size=2,            # Top individuals to preserve
        tournament_size=3,       # Tournament selection size
        slerp_ratio=0.5,         # SLERP interpolation ratio
    ),
)
```

### Knowledge Distillation

```python
config = GenesisConfig(
    distillation=dict(
        temperature=4.0,         # Softmax temperature
        alpha=0.5,               # Distillation vs hard loss weight
        learning_rate=2e-5,      # Learning rate
        max_steps=1000,          # Training steps
    ),
)
```

### Pruning

```python
config = GenesisConfig(
    pruning=dict(
        target_sparsity=0.3,     # Remove 30% of weights
        pruning_method="magnitude",  # magnitude, gradient, taylor
        structured=False,        # Element-wise vs structured
    ),
)
```

## Full Example

```python
import torch
from transformers import AutoTokenizer
from genesis import EvolutionaryOptimizer, GenesisConfig
from genesis.data.datasets import create_dataloader

# Configuration
config = GenesisConfig(
    project_name="quickstart_demo",
    teacher_model="gpt2",
    use_lora=True,
    teacher_device="cuda:0",
    student_device="cuda:0",  # Use same GPU if only one available
    genetic=dict(
        population_size=8,
        generations=10,
        mutation_rate=0.1,
    ),
    distillation=dict(
        temperature=4.0,
        max_steps=100,
    ),
)

# Prepare tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Create dataloader
train_loader = create_dataloader(
    dataset_name="wikitext",
    tokenizer=tokenizer,
    batch_size=4,
    split="train",
    max_samples=500,
)

# Create and run optimizer
optimizer = EvolutionaryOptimizer(
    config=config,
    train_dataloader=train_loader,
    eval_dataloader=train_loader,
)

results = optimizer.run()

# Optional: Prune the model
prune_stats = optimizer.prune_model(target_sparsity=0.2)

# Optional: Final distillation
distill_results = optimizer.distill(num_steps=100)

print("Evolution complete!")
print(f"Best fitness: {results['best_fitness']:.4f}")
print(f"Final sparsity: {prune_stats['actual_sparsity']:.2%}")
```

## Next Steps

- [Architecture Overview](architecture.md) - Deep dive into the system
- [Medical LLM Tutorial](tutorials/medical_llm.md) - Real-world example
- [API Reference](api/core.md) - Complete API documentation
```

<a id="genesis-__init__-py"></a>

#### `genesis/__init__.py`
*830 bytes Â· ~207 tokens*

```python
"""
Genesis AI Evolution Laboratory

A framework for creating efficient AI models using evolutionary algorithms,
pruning, and knowledge distillation on dual GPUs.
"""

__version__ = "0.1.0"
__author__ = "Genesis Team"

__all__ = [
    "EvolutionaryOptimizer",
    "GenesisConfig",
    "HardwareConfig",
    "__version__",
]


def __getattr__(name):
    if name == "EvolutionaryOptimizer":
        from genesis.optimizer import EvolutionaryOptimizer
        return EvolutionaryOptimizer
    if name in ("GenesisConfig", "HardwareConfig"):
        if name == "GenesisConfig":
            from genesis.config.settings import GenesisConfig
            return GenesisConfig
        from genesis.config.hardware import HardwareConfig
        return HardwareConfig
    raise AttributeError(f"module 'genesis' has no attribute {name!r}")
```

<a id="genesis-optimizer-py"></a>

#### `genesis/optimizer.py`
*20186 bytes Â· ~5,005 tokens*

```python
"""Main EvolutionaryOptimizer class for Genesis."""

from typing import Any, Callable, Optional
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import logging
from pathlib import Path

from genesis.config.settings import GenesisConfig
from genesis.config.hardware import HardwareConfig
from genesis.core.genetics import Genetics
from genesis.core.population import Population, Individual
from genesis.core.fitness import FitnessEvaluator, create_fitness_evaluator
from genesis.core.selection import SelectionStrategy, TournamentSelection, ElitismSelection
from genesis.models.teacher import TeacherModel
from genesis.models.student import StudentModel
from genesis.distillation.trainer import DistillationTrainer, TrainingConfig
from genesis.distillation.kd_loss import KDLoss
from genesis.pruning.pruner import Pruner, PruningConfig
from genesis.utils.logging import setup_logging, ProgressLogger, TrainingLogger
from genesis.utils.checkpointing import CheckpointManager
from genesis.utils.metrics import FitnessTracker, MetricsTracker

logger = logging.getLogger(__name__)


class EvolutionaryOptimizer:
    """
    Main orchestrator for evolutionary optimization of AI models.

    Combines genetic algorithms, knowledge distillation, and pruning
    to create efficient AI models using dual-GPU acceleration.
    """

    def __init__(
        self,
        config: Optional[GenesisConfig] = None,
        teacher_model: Optional[TeacherModel] = None,
        student_model: Optional[StudentModel] = None,
        train_dataloader: Optional[DataLoader] = None,
        eval_dataloader: Optional[DataLoader] = None,
    ):
        """
        Initialize the evolutionary optimizer.

        Args:
            config: Genesis configuration
            teacher_model: Pre-initialized teacher model
            student_model: Pre-initialized student model
            train_dataloader: Training data loader
            eval_dataloader: Evaluation data loader
        """
        self.config = config or GenesisConfig()

        # Set up logging
        self.logger = setup_logging(
            log_level="INFO",
            log_dir=self.config.log_dir,
            name="genesis",
        )

        # Hardware configuration
        self.hardware = HardwareConfig(
            teacher_device=self.config.teacher_device,
            student_device=self.config.student_device,
        )

        # Models
        self.teacher = teacher_model
        self.student = student_model

        # Data
        self.train_dataloader = train_dataloader
        self.eval_dataloader = eval_dataloader

        # Core components (initialized lazily)
        self._genetics: Optional[Genetics] = None
        self._population: Optional[Population] = None
        self._fitness_evaluator: Optional[FitnessEvaluator] = None
        self._selection_strategy: Optional[SelectionStrategy] = None
        self._distillation_trainer: Optional[DistillationTrainer] = None
        self._pruner: Optional[Pruner] = None

        # Tracking
        self.fitness_tracker = FitnessTracker()
        self.metrics_tracker = MetricsTracker()
        self.checkpoint_manager = CheckpointManager(
            checkpoint_dir=self.config.checkpoint_dir,
            max_checkpoints=5,
        )

        # Training logger
        self.training_logger = TrainingLogger(
            log_dir=self.config.log_dir,
            use_tensorboard=self.config.use_tensorboard,
            use_wandb=self.config.use_wandb,
            wandb_project=self.config.wandb_project,
        )

        # State
        self._initialized = False
        self._current_generation = 0

    def initialize(self) -> None:
        """Initialize all components."""
        logger.info("Initializing Genesis Evolutionary Optimizer...")

        # Log hardware info
        logger.info(self.hardware.memory_summary())

        # Initialize genetics
        self._genetics = Genetics(
            crossover_rate=self.config.genetic.crossover_rate,
            mutation_rate=self.config.genetic.mutation_rate,
            mutation_scale=self.config.genetic.mutation_scale,
            slerp_ratio=self.config.genetic.slerp_ratio,
            adaptive_mutation=self.config.genetic.adaptive_mutation,
            mutation_decay=self.config.genetic.mutation_decay,
            min_mutation_rate=self.config.genetic.min_mutation_rate,
        )

        # Initialize population
        self._population = Population(
            size=self.config.genetic.population_size,
            genetics=self._genetics,
            elite_size=self.config.genetic.elite_size,
        )

        # Initialize selection strategy
        self._selection_strategy = TournamentSelection(
            tournament_size=self.config.genetic.tournament_size,
        )

        # Load models if not provided
        if self.teacher is None:
            self.teacher = TeacherModel(
                model_name_or_path=self.config.teacher_model,
                device=self.hardware.teacher_device,
            )
            self.teacher.load()

        if self.student is None:
            student_model = self.config.student_model or self.config.teacher_model
            self.student = StudentModel(
                model_name_or_path=student_model,
                device=self.hardware.student_device,
                use_lora=self.config.use_lora,
            )
            self.student.load()

        # Initialize population from student model
        self._population.initialize_from_model(
            self.student.model,
            perturbation_scale=self.config.genetic.mutation_scale,
        )

        self._initialized = True
        logger.info("Initialization complete")

    def set_fitness_evaluator(self, evaluator: FitnessEvaluator) -> None:
        """Set custom fitness evaluator."""
        self._fitness_evaluator = evaluator

    def set_selection_strategy(self, strategy: SelectionStrategy) -> None:
        """Set custom selection strategy."""
        self._selection_strategy = strategy

    def run(
        self,
        num_generations: Optional[int] = None,
        callback: Optional[Callable[[dict], None]] = None,
    ) -> dict[str, Any]:
        """
        Run the evolutionary optimization.

        Args:
            num_generations: Number of generations (overrides config)
            callback: Optional callback after each generation

        Returns:
            Dictionary with optimization results
        """
        if not self._initialized:
            self.initialize()

        num_generations = num_generations or self.config.genetic.generations
        progress_logger = ProgressLogger(num_generations)

        logger.info(f"Starting evolution for {num_generations} generations")

        for generation in range(num_generations):
            self._current_generation = generation

            # Evaluate population
            self._evaluate_population()

            # Log progress
            best = self._population.best
            avg_fitness = self._population.average_fitness
            diversity = self._population.diversity

            progress_logger.log_generation(
                generation=generation,
                best_fitness=best.fitness,
                avg_fitness=avg_fitness,
                diversity=diversity,
            )

            self.fitness_tracker.update(
                generation=generation,
                best_fitness=best.fitness,
                avg_fitness=avg_fitness,
                diversity=diversity,
            )

            # Log to tensorboard/wandb
            self.training_logger.log_metrics(
                {
                    "best_fitness": best.fitness,
                    "avg_fitness": avg_fitness,
                    "diversity": diversity,
                    "mutation_rate": self._genetics._get_current_mutation_rate(),
                },
                step=generation,
                prefix="evolution",
            )

            # Callback
            if callback:
                callback({
                    "generation": generation,
                    "best": best,
                    "avg_fitness": avg_fitness,
                    "diversity": diversity,
                })

            # Check for early stopping
            if self.fitness_tracker.is_converged():
                logger.info(f"Converged at generation {generation}")
                break

            # Checkpoint
            if generation % self.config.save_every_n_generations == 0:
                self._save_checkpoint(generation)

            # Evolve to next generation
            self._population.evolve()

            # Optional: Distillation refinement
            if generation % self.config.eval_every_n_generations == 0:
                self._refine_best_individual()

        # Final evaluation and save
        self._evaluate_population()
        best = self._population.best

        # Save best model
        self._save_best_model(best)

        # Close loggers
        self.training_logger.close()

        results = {
            "best_fitness": best.fitness,
            "best_individual": best,
            "generations": self._current_generation + 1,
            "fitness_history": self.fitness_tracker.get_summary(),
            "converged": self.fitness_tracker.is_converged(),
        }

        logger.info(f"Evolution complete. Best fitness: {best.fitness:.4f}")
        return results

    def _evaluate_population(self) -> None:
        """Evaluate fitness of all individuals.

        If ``config.genetic.memetic_steps > 0`` each individual performs that
        many gradient steps on the training data *before* fitness evaluation
        and writes the updated weights back to its genome (Lamarckian / memetic
        algorithm).  This lets gradient descent exploit the local neighbourhood
        of each candidate, dramatically accelerating convergence compared to
        pure genetic search.
        """
        if self._fitness_evaluator is None:
            if self.eval_dataloader is not None:
                self._fitness_evaluator = create_fitness_evaluator(
                    fitness_type="perplexity",
                    dataloader=self.eval_dataloader,
                    device=self.hardware.student_device,
                    max_samples=self.config.eval_samples,
                )
            else:
                raise ValueError("Fitness evaluator or eval_dataloader required")

        memetic_steps = getattr(self.config.genetic, "memetic_steps", 0)

        def fitness_fn(state_dict: dict[str, torch.Tensor]) -> float:
            # Load genome into student model
            self.student.load_state_dict(state_dict, strict=False)

            # â”€â”€ Lamarckian micro-training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if memetic_steps > 0 and self.train_dataloader is not None:
                self.student.model.train()
                optimizer = torch.optim.AdamW(
                    [p for p in self.student.model.parameters() if p.requires_grad],
                    lr=5e-5,
                )
                batch_iter = iter(self.train_dataloader)
                for _ in range(memetic_steps):
                    try:
                        batch = next(batch_iter)
                    except StopIteration:
                        break

                    input_ids = batch["input_ids"].to(self.hardware.student_device)
                    attention_mask = batch.get("attention_mask")
                    if attention_mask is not None:
                        attention_mask = attention_mask.to(self.hardware.student_device)
                    labels = batch.get("labels", input_ids).to(self.hardware.student_device)

                    optimizer.zero_grad()
                    out = self.student.model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels,
                    )
                    out.loss.backward()
                    torch.nn.utils.clip_grad_norm_(
                        self.student.model.parameters(), max_norm=1.0
                    )
                    optimizer.step()

                # Write the locally-improved weights back into the individual's
                # genome so the next generation inherits the gradient refinement.
                updated = self.student.get_state_dict(lora_only=True)
                state_dict.update({k: v.cpu() for k, v in updated.items()})

            # â”€â”€ Fitness evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            self.student.model.eval()
            result = self._fitness_evaluator.evaluate(self.student.model)
            return result.score

        self._population.evaluate(fitness_fn)

    def _refine_best_individual(self) -> None:
        """Refine best individual with distillation."""
        if self.train_dataloader is None:
            return

        best = self._population.best
        logger.info("Refining best individual with distillation...")

        # Load best state into student
        self.student.load_state_dict(best.state_dict, strict=False)

        # Create distillation trainer
        trainer_config = TrainingConfig(
            learning_rate=self.config.distillation.learning_rate,
            max_steps=100,  # Short refinement
            temperature=self.config.distillation.temperature,
            alpha=self.config.distillation.alpha,
        )

        trainer = DistillationTrainer(
            teacher=self.teacher,
            student=self.student,
            train_dataloader=self.train_dataloader,
            config=trainer_config,
        )

        # Train briefly
        trainer.train(num_steps=100)

        # Update best individual's state using the full model state dict so it
        # stays consistent with how the rest of the population is stored.
        best.state_dict = self.student.get_state_dict(lora_only=False)

    def _save_checkpoint(self, generation: int) -> None:
        """Save evolution checkpoint."""
        checkpoint_path = Path(self.config.checkpoint_dir) / f"evolution_gen{generation}.pt"

        state = {
            "generation": generation,
            "population_state": self._population.get_state(),
            "genetics_generation": self._genetics.generation,
            "fitness_tracker": {
                "best_history": self.fitness_tracker.best_fitness_history,
                "avg_history": self.fitness_tracker.avg_fitness_history,
            },
            "config": self.config.to_dict(),
        }

        torch.save(state, checkpoint_path)
        logger.info(f"Checkpoint saved: {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str) -> None:
        """Load evolution checkpoint."""
        state = torch.load(checkpoint_path, weights_only=False)

        self._current_generation = state["generation"]
        self._population.load_state(state["population_state"])
        self._genetics._generation = state["genetics_generation"]

        self.fitness_tracker.best_fitness_history = state["fitness_tracker"]["best_history"]
        self.fitness_tracker.avg_fitness_history = state["fitness_tracker"]["avg_history"]

        logger.info(f"Checkpoint loaded from {checkpoint_path}")

    def _save_best_model(self, individual: Individual) -> None:
        """Save the best evolved model."""
        output_path = Path(self.config.output_dir) / "best_model"
        output_path.mkdir(parents=True, exist_ok=True)

        # Load state into student and save
        self.student.load_state_dict(individual.state_dict, strict=False)
        self.student.save(str(output_path))

        # Save metadata
        metadata = {
            "fitness": individual.fitness,
            "generation": individual.generation,
            "parent_ids": individual.parent_ids,
            "config": self.config.to_dict(),
        }

        import json

        with open(output_path / "metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"Best model saved to {output_path}")

    def prune_model(
        self,
        target_sparsity: Optional[float] = None,
    ) -> dict[str, float]:
        """
        Prune the best evolved model.

        Args:
            target_sparsity: Target sparsity (overrides config)

        Returns:
            Pruning statistics
        """
        if self._pruner is None:
            pruning_config = PruningConfig(
                target_sparsity=target_sparsity or self.config.pruning.target_sparsity,
                pruning_method=self.config.pruning.pruning_method,
                structured=self.config.pruning.structured,
                granularity=self.config.pruning.granularity,
                block_size=self.config.pruning.block_size,
                iterative_steps=self.config.pruning.iterative_steps,
                initial_sparsity=self.config.pruning.initial_sparsity,
                final_sparsity=self.config.pruning.final_sparsity,
                pruning_schedule=self.config.pruning.pruning_schedule,
                skip_layers=self.config.pruning.skip_layers,
                layer_sparsity_overrides=self.config.pruning.layer_sparsity_overrides,
            )
            self._pruner = Pruner(
                model=self.student.model,
                config=pruning_config,
                dataloader=self.train_dataloader,
                device=self.hardware.student_device,
            )

        stats = self._pruner.prune()
        logger.info(f"Model pruned to {stats['actual_sparsity']:.2%} sparsity")
        return stats

    def distill(
        self,
        num_steps: Optional[int] = None,
    ) -> dict[str, Any]:
        """
        Run full knowledge distillation training.

        Args:
            num_steps: Number of training steps

        Returns:
            Training results
        """
        if self.train_dataloader is None:
            raise ValueError("train_dataloader required for distillation")

        trainer_config = TrainingConfig(
            learning_rate=self.config.distillation.learning_rate,
            max_steps=num_steps or self.config.distillation.max_steps,
            warmup_steps=self.config.distillation.warmup_steps,
            temperature=self.config.distillation.temperature,
            alpha=self.config.distillation.alpha,
            output_dir=self.config.output_dir,
        )

        self._distillation_trainer = DistillationTrainer(
            teacher=self.teacher,
            student=self.student,
            train_dataloader=self.train_dataloader,
            eval_dataloader=self.eval_dataloader,
            config=trainer_config,
        )

        results = self._distillation_trainer.train()
        return results

    @property
    def best_individual(self) -> Optional[Individual]:
        """Get current best individual."""
        if self._population is not None:
            return self._population.best
        return None

    @property
    def population(self) -> Optional[Population]:
        """Get current population."""
        return self._population

    @property
    def current_generation(self) -> int:
        """Get current generation number."""
        return self._current_generation


def main():
    """Example usage of EvolutionaryOptimizer."""
    # Create configuration
    config = GenesisConfig(
        project_name="genesis_example",
        teacher_model="gpt2",  # Use small model for example
        use_lora=True,
        genetic=dict(
            population_size=10,
            generations=5,
            mutation_rate=0.1,
        ),
    )

    # Create optimizer
    optimizer = EvolutionaryOptimizer(config=config)

    # Note: In real usage, you would provide dataloaders
    # optimizer.initialize()
    # results = optimizer.run()

    print("Genesis Evolutionary Optimizer ready!")
    print(f"Config: {config.project_name}")


if __name__ == "__main__":
    main()
```

<a id="docs-tutorials-medical_llm-md"></a>

#### `docs/tutorials/medical_llm.md`
*5350 bytes Â· ~1,337 tokens*

```markdown
# Medical LLM Evolution Tutorial

This tutorial walks you through evolving a medical question-answering model using Genesis.

## Overview

We'll create a specialized medical QA model by:
1. Using a large LLM as teacher (e.g., Llama-2-7B)
2. Evolving LoRA adapters for medical domain expertise
3. Applying knowledge distillation from teacher to student
4. Pruning the final model for efficiency

## Prerequisites

- 2 GPUs with at least 24GB VRAM each
- PubMedQA dataset (automatically downloaded)
- Llama-2-7B model access (HuggingFace token)

## Step 1: Configuration

Create a configuration file `config.yaml`:

```yaml
project_name: "medical_qa_evolution"
output_dir: "./outputs/medical"
checkpoint_dir: "./checkpoints/medical"
seed: 42

teacher_model: "meta-llama/Llama-2-7b-hf"
use_lora: true

teacher_device: "cuda:0"
student_device: "cuda:1"

genetic:
  population_size: 20
  generations: 50
  mutation_rate: 0.1
  crossover_rate: 0.7
  elite_size: 2

distillation:
  temperature: 4.0
  alpha: 0.5
  learning_rate: 2e-5
  max_steps: 1000

lora:
  r: 16
  lora_alpha: 32
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

dataset_name: "pubmed_qa"
max_samples: 5000
eval_samples: 500
```

## Step 2: Load Data

```python
from transformers import AutoTokenizer
from genesis.data.datasets import PubMedQADataset
from torch.utils.data import DataLoader

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer.pad_token = tokenizer.eos_token

# Create datasets
train_dataset = PubMedQADataset(
    tokenizer=tokenizer,
    split="train",
    max_length=512,
    max_samples=5000,
)

eval_dataset = PubMedQADataset(
    tokenizer=tokenizer,
    split="train",
    max_length=512,
    max_samples=500,
)

# Create dataloaders
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
eval_loader = DataLoader(eval_dataset, batch_size=8)
```

## Step 3: Initialize Models

```python
from genesis.models import TeacherModel, StudentModel
import torch

# Teacher model (frozen, provides soft targets)
teacher = TeacherModel(
    model_name_or_path="meta-llama/Llama-2-7b-hf",
    device="cuda:0",
    dtype=torch.float16,
)
teacher.load()

# Student model (evolved with LoRA)
student = StudentModel(
    model_name_or_path="meta-llama/Llama-2-7b-hf",
    device="cuda:1",
    dtype=torch.float16,
    use_lora=True,
)
student.load()
```

## Step 4: Create Fitness Evaluator

```python
from genesis.core.fitness import QAFitness

fitness_evaluator = QAFitness(
    dataloader=eval_loader,
    tokenizer=tokenizer,
    device="cuda:1",
    max_samples=500,
    max_new_tokens=50,
)
```

## Step 5: Run Evolution

```python
from genesis import EvolutionaryOptimizer, GenesisConfig

# Load config
config = GenesisConfig.from_yaml("config.yaml")

# Create optimizer
optimizer = EvolutionaryOptimizer(
    config=config,
    teacher_model=teacher,
    student_model=student,
    train_dataloader=train_loader,
    eval_dataloader=eval_loader,
)

# Set fitness evaluator
optimizer.set_fitness_evaluator(fitness_evaluator)

# Run evolution with callback
def on_generation(info):
    print(f"Generation {info['generation']}")
    print(f"  Best fitness: {info['best'].fitness:.4f}")
    print(f"  Avg fitness: {info['avg_fitness']:.4f}")

results = optimizer.run(callback=on_generation)
```

## Step 6: Post-Processing

### Pruning

```python
# Prune 30% of weights
prune_stats = optimizer.prune_model(target_sparsity=0.3)
print(f"Actual sparsity: {prune_stats['actual_sparsity']:.2%}")
```

### Final Distillation

```python
# Additional distillation to recover from pruning
distill_results = optimizer.distill(num_steps=500)
print(f"Final loss: {distill_results['best_eval_loss']:.4f}")
```

## Step 7: Evaluation

```python
# Load the best model
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("./outputs/medical/best_model")
model.to("cuda:0")

# Test with a medical question
question = "What are the common symptoms of diabetes?"
context = "Diabetes mellitus is a metabolic disease..."

input_text = f"Question: {question}\nContext: {context}\nAnswer:"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda:0")

outputs = model.generate(
    inputs.input_ids,
    max_new_tokens=100,
    temperature=0.7,
)

answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(answer)
```

## Expected Results

After 50 generations of evolution:
- Best fitness: ~0.75 (combined exact match + F1)
- Model size reduction: 30% (with pruning)
- Inference speed improvement: ~40%

## Tips for Better Results

1. **Larger Population**: Increase `population_size` for more diversity
2. **Longer Evolution**: More generations allow finer optimization
3. **Adaptive Mutation**: Enable for automatic rate adjustment
4. **Feature Distillation**: Add hidden state matching for better transfer
5. **Multiple Runs**: Average results from multiple seeds

## Troubleshooting

### Out of Memory

- Reduce `batch_size`
- Enable gradient checkpointing
- Use 8-bit quantization for teacher

### Slow Convergence

- Increase `mutation_rate`
- Use larger `elite_size`
- Check if fitness function is well-calibrated

### Fitness Plateau

- Increase `mutation_scale` temporarily
- Add more diverse data samples
- Try different selection strategies
```

<a id="docs-tutorials-tts_evolution-md"></a>

#### `docs/tutorials/tts_evolution.md`
*6883 bytes Â· ~1,720 tokens*

```markdown
# TTS Voice Evolution Tutorial

This tutorial demonstrates how to evolve TTS style tokens for voice customization.

## Overview

We'll evolve style tokens to:
1. Match a target speaker's voice characteristics
2. Optimize for naturalness and expressiveness
3. Create customized voice profiles

## Prerequisites

- GPU with at least 8GB VRAM
- Reference audio samples (WAV format)
- Pre-trained TTS model (optional)

## Step 1: Prepare Reference Audio

```python
from pathlib import Path
from genesis.data.preprocessing import AudioPreprocessor

# Configure audio processing
preprocessor = AudioPreprocessor(
    sample_rate=22050,
    n_fft=1024,
    hop_length=256,
    n_mels=80,
)

# Load reference audio files
reference_dir = Path("./data/reference_audio")
reference_mels = []

for audio_file in reference_dir.glob("*.wav"):
    features = preprocessor.process_audio_file(str(audio_file))
    reference_mels.append(features["mel_spectrogram"])

print(f"Loaded {len(reference_mels)} reference samples")
```

## Step 2: Configure TTS Evolution

```python
from genesis.tts import TTSChild, TTSConfig
from genesis.tts.style_evolution import StyleEvolution
from genesis.tts.mcd_fitness import MCDFitness

# TTS configuration
tts_config = TTSConfig(
    model_type="tacotron2",
    style_dim=128,
    speaker_dim=256,
    num_speakers=1,
    sample_rate=22050,
    n_mel_channels=80,
)

# Style evolution configuration
style_evolution = StyleEvolution(
    style_dim=128,
    num_tokens=10,
    population_size=30,
    elite_size=3,
    mutation_rate=0.15,
    mutation_scale=0.02,
    crossover_rate=0.7,
)
```

## Step 3: Create Fitness Evaluator

```python
# MCD-based fitness evaluation
fitness_evaluator = MCDFitness(
    reference_mels=reference_mels,
    target_mcd=5.0,  # Lower MCD = better match
    weight_naturalness=0.4,
    weight_similarity=0.6,
    device="cuda",
)
```

## Step 4: Initialize TTS Model

```python
# Create TTS child
tts_child = TTSChild(config=tts_config)

# Optional: Load pre-trained weights
# tts_child.load_model("./pretrained_tts.pt", device="cuda")

# Initialize style evolution population
style_evolution.initialize_population(
    base_tokens=None,  # Random initialization
    perturbation_scale=0.1,
)
```

## Step 5: Run Evolution

```python
import torch
from tqdm import tqdm

num_generations = 100
test_text = "The quick brown fox jumps over the lazy dog."

best_overall_fitness = 0.0
best_style_tokens = None

for generation in tqdm(range(num_generations)):
    # Evaluate each individual
    population = style_evolution.get_population()
    fitnesses = []

    for idx, style_tokens in enumerate(population):
        # Set style tokens
        tts_child.style_tokens = style_tokens

        # Synthesize
        with torch.no_grad():
            output = tts_child.synthesize(
                text=test_text,
                device="cuda",
            )
        synthesized_mel = output["mel_spectrogram"].squeeze(0)

        # Evaluate fitness
        result = fitness_evaluator.evaluate(synthesized_mel)
        fitnesses.append(result["fitness"])

    # Update fitnesses
    style_evolution.set_all_fitnesses(fitnesses)

    # Get best
    best_tokens, best_fitness = style_evolution.get_best()

    if best_fitness > best_overall_fitness:
        best_overall_fitness = best_fitness
        best_style_tokens = best_tokens.clone()
        print(f"\nNew best at gen {generation}: {best_fitness:.4f}")

    # Log progress
    if generation % 10 == 0:
        avg = style_evolution.average_fitness
        print(f"Gen {generation}: Best={best_fitness:.4f}, Avg={avg:.4f}")

    # Evolve
    style_evolution.evolve()

print(f"\nEvolution complete! Best fitness: {best_overall_fitness:.4f}")
```

## Step 6: Evaluate Results

```python
# Set best style tokens
tts_child.style_tokens = best_style_tokens

# Synthesize multiple test sentences
test_sentences = [
    "Hello, how are you today?",
    "The weather is beautiful.",
    "I'm excited to demonstrate this voice.",
]

results = []
for text in test_sentences:
    output = tts_child.synthesize(text=text, device="cuda")
    mel = output["mel_spectrogram"].squeeze(0)

    # Evaluate against references
    fitness_result = fitness_evaluator.evaluate(mel)
    results.append({
        "text": text,
        "mcd": fitness_result["mcd"],
        "naturalness": fitness_result["naturalness_fitness"],
        "similarity": fitness_result["similarity_fitness"],
    })

# Print results
for r in results:
    print(f"Text: {r['text'][:30]}...")
    print(f"  MCD: {r['mcd']:.2f}")
    print(f"  Naturalness: {r['naturalness']:.4f}")
    print(f"  Similarity: {r['similarity']:.4f}")
```

## Step 7: Save Results

```python
import json

# Save best style tokens
torch.save({
    "style_tokens": best_style_tokens,
    "fitness": best_overall_fitness,
    "config": tts_config.__dict__,
}, "./outputs/best_voice_style.pt")

# Save evolution state
style_evolution.save_state("./outputs/evolution_state.pt")

# Save evaluation results
with open("./outputs/evaluation_results.json", "w") as f:
    json.dump(results, f, indent=2)
```

## Multi-Aspect Evolution

For more sophisticated voice control, evolve multiple style aspects:

```python
from genesis.tts.style_evolution import MultiStyleEvolution

multi_evolution = MultiStyleEvolution(
    style_configs={
        "prosody": {
            "dim": 64,
            "num_tokens": 5,
            "mutation_rate": 0.2,
        },
        "emotion": {
            "dim": 32,
            "num_tokens": 8,
            "mutation_rate": 0.15,
        },
        "timbre": {
            "dim": 128,
            "num_tokens": 4,
            "mutation_rate": 0.1,
        },
    },
    population_size=20,
)

multi_evolution.initialize_all()

# During evolution, get combined styles
for gen in range(50):
    for idx in range(20):
        combined_style = multi_evolution.get_combined_style({
            "prosody": idx,
            "emotion": idx,
            "timbre": idx,
        })
        # Use combined_style for synthesis...

    multi_evolution.evolve_all()
```

## Tips for Better Voice Quality

1. **More Reference Samples**: Use diverse samples of target speaker
2. **F0 Matching**: Add F0 RMSE to fitness function
3. **Duration Modeling**: Match speaking rate of target
4. **Vocoder Quality**: Use high-quality neural vocoder (HiFi-GAN)
5. **Longer Evolution**: TTS often needs 100+ generations

## Troubleshooting

### Unnatural Voice

- Increase `weight_naturalness`
- Add more diverse reference samples
- Check mel spectrogram normalization

### Poor Speaker Similarity

- Increase `weight_similarity`
- Use more reference samples from target speaker
- Ensure reference audio is clean and well-recorded

### Slow Progress

- Increase `mutation_rate` early, decrease later
- Try larger `population_size`
- Use multiple random restarts
```

<a id="experiments-llm_medical-run_evolution-py"></a>

#### `experiments/llm_medical/run_evolution.py`
*6053 bytes Â· ~1,513 tokens*

```python
#!/usr/bin/env python3
"""
Medical LLM Evolution Experiment

Evolves a medical question-answering model using knowledge distillation
from a larger teacher model and evolutionary optimization.
"""

import argparse
import logging
from pathlib import Path
import yaml
import torch
from transformers import AutoTokenizer

from genesis import EvolutionaryOptimizer, GenesisConfig
from genesis.data.datasets import PubMedQADataset, create_dataloader
from genesis.models.teacher import TeacherModel
from genesis.models.student import StudentModel
from genesis.core.fitness import QAFitness
from genesis.utils.logging import setup_logging


def load_config(config_path: str) -> GenesisConfig:
    """Load configuration from YAML file."""
    with open(config_path, "r") as f:
        config_dict = yaml.safe_load(f)
    return GenesisConfig.from_dict(config_dict)


def main():
    parser = argparse.ArgumentParser(description="Medical LLM Evolution Experiment")
    parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Path to configuration file",
    )
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="Path to checkpoint to resume from",
    )
    parser.add_argument(
        "--generations",
        type=int,
        default=None,
        help="Override number of generations",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Run with minimal settings for testing",
    )
    args = parser.parse_args()

    # Load configuration
    config_path = Path(__file__).parent / args.config
    config = load_config(str(config_path))

    # Dry run settings
    if args.dry_run:
        config.genetic.population_size = 4
        config.genetic.generations = 2
        config.max_samples = 100
        config.eval_samples = 20
        config.distillation.max_steps = 10

    # Setup logging
    logger = setup_logging(
        log_level="INFO",
        log_dir=config.log_dir,
        name="medical_llm",
    )
    logger.info(f"Starting Medical LLM Evolution Experiment")
    logger.info(f"Config: {config.project_name}")

    # Set random seed
    torch.manual_seed(config.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config.seed)

    # Load tokenizer
    logger.info(f"Loading tokenizer from {config.teacher_model}")
    tokenizer = AutoTokenizer.from_pretrained(config.teacher_model)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Create datasets
    logger.info("Loading PubMedQA dataset...")
    train_dataset = PubMedQADataset(
        tokenizer=tokenizer,
        split="train",
        max_length=512,
        max_samples=config.max_samples,
    )

    eval_dataset = PubMedQADataset(
        tokenizer=tokenizer,
        split="train",  # PubMedQA doesn't have separate val split
        max_length=512,
        max_samples=config.eval_samples,
    )

    # Create dataloaders
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=config.distillation.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
    )

    eval_dataloader = torch.utils.data.DataLoader(
        eval_dataset,
        batch_size=config.distillation.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
    )

    # Initialize models
    logger.info("Initializing teacher model...")
    teacher = TeacherModel(
        model_name_or_path=config.teacher_model,
        device=config.teacher_device,
        dtype=torch.float16 if config.mixed_precision == "fp16" else torch.bfloat16,
    )
    teacher.load()

    logger.info("Initializing student model...")
    student = StudentModel(
        model_name_or_path=config.student_model or config.teacher_model,
        device=config.student_device,
        dtype=torch.float16 if config.mixed_precision == "fp16" else torch.bfloat16,
        use_lora=config.use_lora,
        lora_config=config.lora,
    )
    student.load()

    # Create fitness evaluator
    fitness_evaluator = QAFitness(
        dataloader=eval_dataloader,
        tokenizer=tokenizer,
        device=config.student_device,
        max_samples=config.eval_samples,
    )

    # Create optimizer
    logger.info("Creating evolutionary optimizer...")
    optimizer = EvolutionaryOptimizer(
        config=config,
        teacher_model=teacher,
        student_model=student,
        train_dataloader=train_dataloader,
        eval_dataloader=eval_dataloader,
    )

    # Set custom fitness evaluator
    optimizer.set_fitness_evaluator(fitness_evaluator)

    # Resume from checkpoint if specified
    if args.resume:
        logger.info(f"Resuming from checkpoint: {args.resume}")
        optimizer.load_checkpoint(args.resume)

    # Run evolution
    logger.info("Starting evolution...")
    results = optimizer.run(
        num_generations=args.generations,
        callback=lambda r: logger.info(
            f"Gen {r['generation']}: Best={r['best'].fitness:.4f}, Avg={r['avg_fitness']:.4f}"
        ),
    )

    # Log results
    logger.info("=" * 50)
    logger.info("Evolution Complete!")
    logger.info(f"Best Fitness: {results['best_fitness']:.4f}")
    logger.info(f"Generations: {results['generations']}")
    logger.info(f"Converged: {results['converged']}")
    logger.info("=" * 50)

    # Optional: Prune the best model
    if config.pruning.target_sparsity > 0:
        logger.info("Pruning best model...")
        prune_stats = optimizer.prune_model()
        logger.info(f"Pruning complete: {prune_stats['actual_sparsity']:.2%} sparsity")

    # Optional: Final distillation refinement
    logger.info("Running final distillation refinement...")
    distill_results = optimizer.distill(num_steps=config.distillation.max_steps)
    logger.info(f"Final distillation loss: {distill_results.get('best_eval_loss', 'N/A')}")

    logger.info(f"Results saved to {config.output_dir}")


if __name__ == "__main__":
    main()
```

<a id="experiments-tts_voice-run_evolution-py"></a>

#### `experiments/tts_voice/run_evolution.py`
*8129 bytes Â· ~2,032 tokens*

```python
#!/usr/bin/env python3
"""
TTS Voice Evolution Experiment

Evolves TTS style tokens and speaker embeddings to optimize
for voice quality, naturalness, and target speaker similarity.
"""

import argparse
import logging
from pathlib import Path
import yaml
import torch
import numpy as np

from genesis.tts.tts_child import TTSChild, TTSConfig
from genesis.tts.style_evolution import StyleEvolution
from genesis.tts.mcd_fitness import MCDFitness, compute_mcd
from genesis.data.datasets import TTSDataset
from genesis.utils.logging import setup_logging, ProgressLogger
from genesis.utils.checkpointing import CheckpointManager


def load_config(config_path: str) -> dict:
    """Load configuration from YAML file."""
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def main():
    parser = argparse.ArgumentParser(description="TTS Voice Evolution Experiment")
    parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Path to configuration file",
    )
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="Path to checkpoint to resume from",
    )
    parser.add_argument(
        "--generations",
        type=int,
        default=None,
        help="Override number of generations",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Run with minimal settings for testing",
    )
    args = parser.parse_args()

    # Load configuration
    config_path = Path(__file__).parent / args.config
    config = load_config(str(config_path))

    # Dry run settings
    if args.dry_run:
        config["genetic"]["population_size"] = 5
        config["genetic"]["generations"] = 3
        config["data"]["max_samples"] = 10
        config["data"]["eval_samples"] = 5

    # Setup logging
    logger = setup_logging(
        log_level="INFO",
        log_dir=config["log_dir"],
        name="tts_evolution",
    )
    logger.info("Starting TTS Voice Evolution Experiment")

    # Set random seed
    torch.manual_seed(config["seed"])
    np.random.seed(config["seed"])

    # Create directories
    Path(config["output_dir"]).mkdir(parents=True, exist_ok=True)
    Path(config["checkpoint_dir"]).mkdir(parents=True, exist_ok=True)

    # Create TTS config
    tts_config = TTSConfig(
        model_type=config["tts"]["model_type"],
        style_dim=config["tts"]["style_dim"],
        speaker_dim=config["tts"]["speaker_dim"],
        num_speakers=config["tts"]["num_speakers"],
        sample_rate=config["tts"]["sample_rate"],
        n_mel_channels=config["tts"]["n_mel_channels"],
        hop_length=config["tts"]["hop_length"],
        win_length=config["tts"]["win_length"],
        n_fft=config["tts"]["n_fft"],
    )

    # Load reference audio for fitness evaluation
    logger.info("Loading reference audio...")
    try:
        tts_dataset = TTSDataset(
            data_dir=config["data"]["data_dir"],
            sample_rate=config["tts"]["sample_rate"],
            max_samples=config["data"]["eval_samples"],
        )
        reference_mels = [sample["mel_spectrogram"] for sample in tts_dataset]
        logger.info(f"Loaded {len(reference_mels)} reference mel spectrograms")
    except Exception as e:
        logger.warning(f"Could not load reference audio: {e}")
        logger.warning("Using synthetic reference mels for demonstration")
        # Create synthetic reference mels for demonstration
        reference_mels = [
            torch.randn(config["tts"]["n_mel_channels"], 100 + i * 10)
            for i in range(10)
        ]

    # Create fitness evaluator
    fitness_evaluator = MCDFitness(
        reference_mels=reference_mels,
        target_mcd=config["fitness"]["target_mcd"],
        weight_naturalness=config["fitness"]["weight_naturalness"],
        weight_similarity=config["fitness"]["weight_similarity"],
        device=config["device"],
    )

    # Initialize style evolution
    logger.info("Initializing style evolution...")
    style_evolution = StyleEvolution(
        style_dim=config["tts"]["style_dim"],
        num_tokens=config["style_evolution"]["num_style_tokens"],
        population_size=config["genetic"]["population_size"],
        elite_size=config["genetic"]["elite_size"],
        mutation_rate=config["genetic"]["mutation_rate"],
        mutation_scale=config["genetic"]["mutation_scale"],
        crossover_rate=config["genetic"]["crossover_rate"],
    )

    # Initialize population
    style_evolution.initialize_population(
        perturbation_scale=config["style_evolution"]["perturbation_scale"],
    )

    # Create checkpoint manager
    checkpoint_manager = CheckpointManager(
        checkpoint_dir=config["checkpoint_dir"],
        max_checkpoints=5,
        metric_name="fitness",
        mode="max",
    )

    # Resume from checkpoint if specified
    if args.resume:
        logger.info(f"Resuming from checkpoint: {args.resume}")
        style_evolution.load_state(args.resume)

    # Progress logging
    num_generations = args.generations or config["genetic"]["generations"]
    progress_logger = ProgressLogger(num_generations)

    # Create TTS child for synthesis
    tts_child = TTSChild(config=tts_config)

    # Evolution loop
    logger.info(f"Starting evolution for {num_generations} generations")
    best_overall_fitness = 0.0
    best_style_tokens = None

    for generation in range(num_generations):
        # Evaluate population
        population = style_evolution.get_population()
        fitnesses = []

        for idx, style_tokens in enumerate(population):
            # Set style tokens in TTS child
            tts_child.style_tokens = style_tokens

            # Synthesize mel spectrogram (using a test text)
            test_text = "This is a test sentence for voice evolution."

            try:
                output = tts_child.synthesize(
                    text=test_text,
                    device=config["device"],
                )
                synthesized_mel = output["mel_spectrogram"]
            except Exception:
                # If synthesis fails, create dummy output
                synthesized_mel = torch.randn(
                    1,
                    config["tts"]["n_mel_channels"],
                    100,
                )

            # Evaluate fitness
            fitness_result = fitness_evaluator.evaluate(synthesized_mel.squeeze(0))
            fitnesses.append(fitness_result["fitness"])

        # Update fitnesses in evolution
        style_evolution.set_all_fitnesses(fitnesses)

        # Get best individual
        best_tokens, best_fitness = style_evolution.get_best()

        if best_fitness > best_overall_fitness:
            best_overall_fitness = best_fitness
            best_style_tokens = best_tokens.clone()

        # Log progress
        avg_fitness = style_evolution.average_fitness
        progress_logger.log_generation(
            generation=generation,
            best_fitness=best_fitness,
            avg_fitness=avg_fitness,
        )

        # Checkpoint
        if generation % config["save_every_n_generations"] == 0:
            style_evolution.save_state(
                Path(config["checkpoint_dir"]) / f"evolution_gen{generation}.pt"
            )

        # Evolve to next generation
        style_evolution.evolve()

    # Save final results
    logger.info("=" * 50)
    logger.info("Evolution Complete!")
    logger.info(f"Best Fitness: {best_overall_fitness:.4f}")
    logger.info("=" * 50)

    # Save best style tokens
    output_path = Path(config["output_dir"]) / "best_style_tokens.pt"
    torch.save(
        {
            "style_tokens": best_style_tokens,
            "fitness": best_overall_fitness,
            "config": config,
        },
        output_path,
    )
    logger.info(f"Best style tokens saved to {output_path}")

    # Save final evolution state
    style_evolution.save_state(
        Path(config["checkpoint_dir"]) / "final_evolution_state.pt"
    )

    logger.info(f"Results saved to {config['output_dir']}")


if __name__ == "__main__":
    main()
```

<a id="genesis-data-__init__-py"></a>

#### `genesis/data/__init__.py`
*802 bytes Â· ~200 tokens*

```python
"""Data handling for Genesis."""

__all__ = [
    "DatasetLoader",
    "PubMedQADataset",
    "TTSDataset",
    "TextPreprocessor",
    "AudioPreprocessor",
]


def __getattr__(name):
    if name in ("DatasetLoader", "PubMedQADataset", "TTSDataset"):
        from genesis.data.datasets import DatasetLoader, PubMedQADataset, TTSDataset
        return {"DatasetLoader": DatasetLoader, "PubMedQADataset": PubMedQADataset,
                "TTSDataset": TTSDataset}[name]
    if name in ("TextPreprocessor", "AudioPreprocessor"):
        from genesis.data.preprocessing import TextPreprocessor, AudioPreprocessor
        return {"TextPreprocessor": TextPreprocessor,
                "AudioPreprocessor": AudioPreprocessor}[name]
    raise AttributeError(f"module 'genesis.data' has no attribute {name!r}")
```

<a id="genesis-data-datasets-py"></a>

#### `genesis/data/datasets.py`
*11572 bytes Â· ~2,893 tokens*

```python
"""Dataset loaders for Genesis experiments."""

from typing import Any, Callable, Optional
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
import logging

logger = logging.getLogger(__name__)


class DatasetLoader:
    """
    Unified dataset loader for various data sources.
    """

    def __init__(
        self,
        dataset_name: str,
        tokenizer: Any = None,
        max_length: int = 512,
        split: str = "train",
        max_samples: Optional[int] = None,
        cache_dir: Optional[str] = None,
    ):
        """
        Initialize dataset loader.

        Args:
            dataset_name: Name of dataset (HuggingFace or local)
            tokenizer: Tokenizer for text processing
            max_length: Maximum sequence length
            split: Dataset split
            max_samples: Maximum number of samples
            cache_dir: Cache directory for downloads
        """
        self.dataset_name = dataset_name
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.split = split
        self.max_samples = max_samples
        self.cache_dir = cache_dir
        self._dataset = None

    def load(self) -> Dataset:
        """Load the dataset."""
        logger.info(f"Loading dataset: {self.dataset_name}")

        # Try to load from HuggingFace
        try:
            self._dataset = load_dataset(
                self.dataset_name,
                split=self.split,
                cache_dir=self.cache_dir,
            )

            if self.max_samples:
                self._dataset = self._dataset.select(range(min(self.max_samples, len(self._dataset))))

            logger.info(f"Loaded {len(self._dataset)} samples")
            return self._dataset

        except Exception as e:
            logger.error(f"Failed to load dataset: {e}")
            raise

    def get_dataloader(
        self,
        batch_size: int = 8,
        shuffle: bool = True,
        num_workers: int = 4,
        collate_fn: Optional[Callable] = None,
    ) -> DataLoader:
        """
        Create a DataLoader from the dataset.

        Args:
            batch_size: Batch size
            shuffle: Whether to shuffle
            num_workers: Number of data loading workers
            collate_fn: Custom collation function

        Returns:
            DataLoader instance
        """
        if self._dataset is None:
            self.load()

        return DataLoader(
            self._dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=collate_fn or self._default_collate,
            pin_memory=True,
        )

    def _default_collate(self, batch: list[dict]) -> dict[str, torch.Tensor]:
        """Default collation function."""
        if self.tokenizer is None:
            raise ValueError("Tokenizer required for default collation")

        # Handle different dataset formats
        if "text" in batch[0]:
            texts = [item["text"] for item in batch]
        elif "question" in batch[0] and "context" in batch[0]:
            texts = [f"{item['question']} {item['context']}" for item in batch]
        else:
            # Fallback: concatenate all string fields
            texts = [" ".join(str(v) for v in item.values() if isinstance(v, str)) for item in batch]

        # Tokenize
        encodings = self.tokenizer(
            texts,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        return {
            "input_ids": encodings["input_ids"],
            "attention_mask": encodings["attention_mask"],
            "labels": encodings["input_ids"].clone(),
        }


class PubMedQADataset(Dataset):
    """
    Dataset for PubMedQA medical question answering.
    """

    def __init__(
        self,
        tokenizer: Any,
        split: str = "train",
        max_length: int = 512,
        max_samples: Optional[int] = None,
    ):
        """
        Initialize PubMedQA dataset.

        Args:
            tokenizer: Tokenizer for text processing
            split: Dataset split
            max_length: Maximum sequence length
            max_samples: Maximum number of samples
        """
        self.tokenizer = tokenizer
        self.max_length = max_length

        # Load dataset
        logger.info("Loading PubMedQA dataset...")
        dataset = load_dataset("pubmed_qa", "pqa_labeled", split=split)

        if max_samples:
            dataset = dataset.select(range(min(max_samples, len(dataset))))

        self.data = dataset
        logger.info(f"Loaded {len(self.data)} PubMedQA samples")

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:
        item = self.data[idx]

        # Format: Question + Context -> Answer
        question = item["question"]
        context = " ".join(item["context"]["contexts"])
        answer = item["final_decision"]  # yes/no/maybe

        # Create input text
        input_text = f"Question: {question}\nContext: {context}\nAnswer:"
        target_text = answer

        # Tokenize input
        input_encoding = self.tokenizer(
            input_text,
            max_length=self.max_length - 10,  # Leave room for answer
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        # Tokenize target
        target_encoding = self.tokenizer(
            target_text,
            max_length=10,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        # For causal LM, labels are the same as input shifted
        full_input = torch.cat([
            input_encoding["input_ids"].squeeze(),
            target_encoding["input_ids"].squeeze()[:5],  # First 5 tokens of answer
        ])

        return {
            "input_ids": input_encoding["input_ids"].squeeze(),
            "attention_mask": input_encoding["attention_mask"].squeeze(),
            "labels": input_encoding["input_ids"].squeeze(),
            "target_text": target_text,
        }


class TTSDataset(Dataset):
    """
    Dataset for TTS experiments with text-audio pairs.
    """

    def __init__(
        self,
        data_dir: str,
        sample_rate: int = 22050,
        max_samples: Optional[int] = None,
        mel_config: Optional[dict] = None,
    ):
        """
        Initialize TTS dataset.

        Args:
            data_dir: Directory containing audio and text files
            sample_rate: Audio sample rate
            max_samples: Maximum number of samples
            mel_config: Mel spectrogram configuration
        """
        self.data_dir = data_dir
        self.sample_rate = sample_rate
        self.mel_config = mel_config or {
            "n_fft": 1024,
            "hop_length": 256,
            "win_length": 1024,
            "n_mels": 80,
        }

        self._samples: list[dict] = []
        self._load_metadata()

        if max_samples:
            self._samples = self._samples[:max_samples]

    def _load_metadata(self) -> None:
        """Load dataset metadata from data directory."""
        import os

        # Look for common metadata files
        metadata_files = ["metadata.csv", "transcript.txt", "filelist.txt"]

        for meta_file in metadata_files:
            meta_path = os.path.join(self.data_dir, meta_file)
            if os.path.exists(meta_path):
                self._parse_metadata(meta_path)
                return

        # Fallback: scan directory for audio files
        for filename in os.listdir(self.data_dir):
            if filename.endswith((".wav", ".mp3", ".flac")):
                audio_path = os.path.join(self.data_dir, filename)
                text_path = audio_path.rsplit(".", 1)[0] + ".txt"

                text = ""
                if os.path.exists(text_path):
                    with open(text_path, "r") as f:
                        text = f.read().strip()

                self._samples.append({
                    "audio_path": audio_path,
                    "text": text,
                })

    def _parse_metadata(self, path: str) -> None:
        """Parse metadata file."""
        import os

        with open(path, "r") as f:
            for line in f:
                parts = line.strip().split("|")
                if len(parts) >= 2:
                    audio_file = parts[0]
                    text = parts[1]

                    audio_path = os.path.join(self.data_dir, audio_file)
                    if not audio_path.endswith((".wav", ".mp3", ".flac")):
                        audio_path += ".wav"

                    self._samples.append({
                        "audio_path": audio_path,
                        "text": text,
                    })

    def __len__(self) -> int:
        return len(self._samples)

    def __getitem__(self, idx: int) -> dict[str, Any]:
        sample = self._samples[idx]

        # Load audio and compute mel spectrogram
        mel = self._load_and_process_audio(sample["audio_path"])

        return {
            "mel_spectrogram": mel,
            "text": sample["text"],
            "audio_path": sample["audio_path"],
        }

    def _load_and_process_audio(self, audio_path: str) -> torch.Tensor:
        """Load audio and compute mel spectrogram."""
        try:
            import librosa

            # Load audio
            audio, sr = librosa.load(audio_path, sr=self.sample_rate)

            # Compute mel spectrogram
            mel = librosa.feature.melspectrogram(
                y=audio,
                sr=sr,
                n_fft=self.mel_config["n_fft"],
                hop_length=self.mel_config["hop_length"],
                win_length=self.mel_config["win_length"],
                n_mels=self.mel_config["n_mels"],
            )

            # Convert to log scale
            mel = librosa.power_to_db(mel, ref=np.max)

            return torch.from_numpy(mel).float()

        except ImportError:
            logger.warning("librosa not installed. Returning dummy mel spectrogram.")
            return torch.randn(self.mel_config["n_mels"], 100)
        except Exception as e:
            logger.error(f"Error loading audio {audio_path}: {e}")
            return torch.randn(self.mel_config["n_mels"], 100)


def create_dataloader(
    dataset_name: str,
    tokenizer: Any,
    batch_size: int = 8,
    split: str = "train",
    max_samples: Optional[int] = None,
    **kwargs,
) -> DataLoader:
    """
    Factory function to create dataloaders.

    Args:
        dataset_name: Name of dataset
        tokenizer: Tokenizer for text processing
        batch_size: Batch size
        split: Dataset split
        max_samples: Maximum samples
        **kwargs: Additional arguments

    Returns:
        DataLoader instance
    """
    if dataset_name == "pubmed_qa":
        dataset = PubMedQADataset(
            tokenizer=tokenizer,
            split=split,
            max_samples=max_samples,
        )
    else:
        loader = DatasetLoader(
            dataset_name=dataset_name,
            tokenizer=tokenizer,
            split=split,
            max_samples=max_samples,
        )
        dataset = loader.load()

    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(split == "train"),
        num_workers=kwargs.get("num_workers", 4),
        pin_memory=True,
    )
```

<a id="genesis-data-preprocessing-py"></a>

#### `genesis/data/preprocessing.py`
*10289 bytes Â· ~2,572 tokens*

```python
"""Data preprocessing utilities for Genesis."""

from typing import Any, Optional
import torch
import numpy as np
import re
import logging

logger = logging.getLogger(__name__)


class TextPreprocessor:
    """
    Text preprocessing for NLP tasks.
    """

    def __init__(
        self,
        tokenizer: Any = None,
        max_length: int = 512,
        lowercase: bool = False,
        remove_special_chars: bool = False,
        normalize_whitespace: bool = True,
    ):
        """
        Initialize text preprocessor.

        Args:
            tokenizer: Tokenizer for text processing
            max_length: Maximum sequence length
            lowercase: Convert to lowercase
            remove_special_chars: Remove special characters
            normalize_whitespace: Normalize whitespace
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.lowercase = lowercase
        self.remove_special_chars = remove_special_chars
        self.normalize_whitespace = normalize_whitespace

    def preprocess(self, text: str) -> str:
        """
        Apply preprocessing steps to text.

        Args:
            text: Input text

        Returns:
            Preprocessed text
        """
        if self.lowercase:
            text = text.lower()

        if self.remove_special_chars:
            text = re.sub(r"[^a-zA-Z0-9\s.,!?;:'\"-]", "", text)

        if self.normalize_whitespace:
            text = " ".join(text.split())

        return text

    def tokenize(
        self,
        text: str,
        return_tensors: str = "pt",
    ) -> dict[str, torch.Tensor]:
        """
        Tokenize preprocessed text.

        Args:
            text: Input text
            return_tensors: Return format

        Returns:
            Tokenized output
        """
        if self.tokenizer is None:
            raise ValueError("Tokenizer not provided")

        text = self.preprocess(text)

        return self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors=return_tensors,
        )

    def batch_tokenize(
        self,
        texts: list[str],
        return_tensors: str = "pt",
    ) -> dict[str, torch.Tensor]:
        """
        Tokenize a batch of texts.

        Args:
            texts: List of input texts
            return_tensors: Return format

        Returns:
            Batch tokenized output
        """
        if self.tokenizer is None:
            raise ValueError("Tokenizer not provided")

        texts = [self.preprocess(t) for t in texts]

        return self.tokenizer(
            texts,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors=return_tensors,
        )

    def create_qa_input(
        self,
        question: str,
        context: str,
        answer: Optional[str] = None,
    ) -> dict[str, Any]:
        """
        Create input for QA tasks.

        Args:
            question: Question text
            context: Context text
            answer: Optional answer text

        Returns:
            Formatted input dictionary
        """
        question = self.preprocess(question)
        context = self.preprocess(context)

        # Format input
        input_text = f"Question: {question}\nContext: {context}\nAnswer:"

        result = self.tokenize(input_text)

        if answer:
            result["target_text"] = self.preprocess(answer)

        return result


class AudioPreprocessor:
    """
    Audio preprocessing for TTS tasks.
    """

    def __init__(
        self,
        sample_rate: int = 22050,
        n_fft: int = 1024,
        hop_length: int = 256,
        win_length: int = 1024,
        n_mels: int = 80,
        fmin: float = 0.0,
        fmax: Optional[float] = None,
        normalize: bool = True,
    ):
        """
        Initialize audio preprocessor.

        Args:
            sample_rate: Target sample rate
            n_fft: FFT size
            hop_length: Hop length
            win_length: Window length
            n_mels: Number of mel bands
            fmin: Minimum frequency
            fmax: Maximum frequency
            normalize: Normalize audio
        """
        self.sample_rate = sample_rate
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.win_length = win_length
        self.n_mels = n_mels
        self.fmin = fmin
        self.fmax = fmax or sample_rate / 2
        self.normalize = normalize

    def load_audio(self, audio_path: str) -> np.ndarray:
        """
        Load audio from file.

        Args:
            audio_path: Path to audio file

        Returns:
            Audio waveform as numpy array
        """
        try:
            import librosa

            audio, sr = librosa.load(audio_path, sr=self.sample_rate)

            if self.normalize:
                audio = audio / (np.abs(audio).max() + 1e-8)

            return audio

        except ImportError:
            logger.error("librosa not installed")
            raise
        except Exception as e:
            logger.error(f"Error loading audio: {e}")
            raise

    def compute_mel_spectrogram(
        self,
        audio: np.ndarray,
        return_tensor: bool = True,
    ) -> torch.Tensor:
        """
        Compute mel spectrogram from audio.

        Args:
            audio: Audio waveform
            return_tensor: Return as PyTorch tensor

        Returns:
            Mel spectrogram
        """
        try:
            import librosa

            mel = librosa.feature.melspectrogram(
                y=audio,
                sr=self.sample_rate,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                win_length=self.win_length,
                n_mels=self.n_mels,
                fmin=self.fmin,
                fmax=self.fmax,
            )

            # Convert to log scale
            mel = librosa.power_to_db(mel, ref=np.max)

            if return_tensor:
                return torch.from_numpy(mel).float()
            return mel

        except ImportError:
            logger.error("librosa not installed")
            raise

    def compute_f0(
        self,
        audio: np.ndarray,
        method: str = "pyin",
    ) -> np.ndarray:
        """
        Compute fundamental frequency (F0) from audio.

        Args:
            audio: Audio waveform
            method: F0 estimation method ('pyin', 'yin', 'swipe')

        Returns:
            F0 contour
        """
        try:
            import librosa

            if method == "pyin":
                f0, voiced_flag, voiced_probs = librosa.pyin(
                    audio,
                    fmin=librosa.note_to_hz("C2"),
                    fmax=librosa.note_to_hz("C7"),
                    sr=self.sample_rate,
                    hop_length=self.hop_length,
                )
            else:
                # Fallback to basic method
                f0 = librosa.yin(
                    audio,
                    fmin=80,
                    fmax=600,
                    sr=self.sample_rate,
                    hop_length=self.hop_length,
                )

            return np.nan_to_num(f0)

        except ImportError:
            logger.error("librosa not installed")
            raise

    def process_audio_file(self, audio_path: str) -> dict[str, torch.Tensor]:
        """
        Process audio file and extract features.

        Args:
            audio_path: Path to audio file

        Returns:
            Dictionary with extracted features
        """
        audio = self.load_audio(audio_path)
        mel = self.compute_mel_spectrogram(audio)
        f0 = self.compute_f0(audio)

        return {
            "audio": torch.from_numpy(audio).float(),
            "mel_spectrogram": mel,
            "f0": torch.from_numpy(f0).float(),
            "duration": len(audio) / self.sample_rate,
        }

    def normalize_mel(self, mel: torch.Tensor) -> torch.Tensor:
        """
        Normalize mel spectrogram.

        Args:
            mel: Mel spectrogram

        Returns:
            Normalized mel spectrogram
        """
        mel_min = mel.min()
        mel_max = mel.max()
        return (mel - mel_min) / (mel_max - mel_min + 1e-8)

    def denormalize_mel(
        self,
        mel: torch.Tensor,
        mel_min: float = -100,
        mel_max: float = 0,
    ) -> torch.Tensor:
        """
        Denormalize mel spectrogram.

        Args:
            mel: Normalized mel spectrogram
            mel_min: Original minimum value
            mel_max: Original maximum value

        Returns:
            Denormalized mel spectrogram
        """
        return mel * (mel_max - mel_min) + mel_min


def pad_sequence(
    sequences: list[torch.Tensor],
    padding_value: float = 0.0,
    max_length: Optional[int] = None,
) -> torch.Tensor:
    """
    Pad sequences to same length.

    Args:
        sequences: List of tensors
        padding_value: Value to use for padding
        max_length: Maximum length (uses max sequence length if None)

    Returns:
        Padded tensor
    """
    if max_length is None:
        max_length = max(s.size(-1) for s in sequences)

    padded = []
    for seq in sequences:
        if seq.size(-1) < max_length:
            pad_size = max_length - seq.size(-1)
            if seq.dim() == 1:
                pad = torch.full((pad_size,), padding_value, dtype=seq.dtype)
                seq = torch.cat([seq, pad])
            else:
                pad = torch.full((*seq.shape[:-1], pad_size), padding_value, dtype=seq.dtype)
                seq = torch.cat([seq, pad], dim=-1)
        elif seq.size(-1) > max_length:
            seq = seq[..., :max_length]
        padded.append(seq)

    return torch.stack(padded)


def create_attention_mask(
    input_ids: torch.Tensor,
    pad_token_id: int = 0,
) -> torch.Tensor:
    """
    Create attention mask from input IDs.

    Args:
        input_ids: Input token IDs
        pad_token_id: Padding token ID

    Returns:
        Attention mask (1 for real tokens, 0 for padding)
    """
    return (input_ids != pad_token_id).long()
```

<a id="genesis-distillation-__init__-py"></a>

#### `genesis/distillation/__init__.py`
*709 bytes Â· ~177 tokens*

```python
"""Knowledge distillation module for Genesis."""

from genesis.distillation.kd_loss import (
    KDLoss,
    kl_divergence_loss,
    soft_target_loss,
    feature_distillation_loss,
)

__all__ = [
    "KDLoss",
    "kl_divergence_loss",
    "soft_target_loss",
    "feature_distillation_loss",
    "DistillationTrainer",
    "TrainingConfig",
]


def __getattr__(name):
    if name in ("DistillationTrainer", "TrainingConfig"):
        from genesis.distillation.trainer import DistillationTrainer, TrainingConfig
        if name == "DistillationTrainer":
            return DistillationTrainer
        return TrainingConfig
    raise AttributeError(f"module 'genesis.distillation' has no attribute {name!r}")
```

<a id="genesis-distillation-kd_loss-py"></a>

#### `genesis/distillation/kd_loss.py`
*14458 bytes Â· ~3,611 tokens*

```python
"""Knowledge distillation loss functions."""

from typing import Optional
import torch
import torch.nn as nn
import torch.nn.functional as F


def kl_divergence_loss(
    student_logits: torch.Tensor,
    teacher_logits: torch.Tensor,
    temperature: float = 4.0,
    reduction: str = "batchmean",
) -> torch.Tensor:
    """
    Compute KL divergence loss between student and teacher logits.

    Args:
        student_logits: Student model logits [batch, seq_len, vocab]
        teacher_logits: Teacher model logits [batch, seq_len, vocab]
        temperature: Temperature for softmax scaling
        reduction: Reduction method ('batchmean', 'sum', 'mean', 'none')

    Returns:
        KL divergence loss
    """
    # Scale logits by temperature
    student_soft = F.log_softmax(student_logits / temperature, dim=-1)
    teacher_soft = F.softmax(teacher_logits / temperature, dim=-1)

    # KL divergence
    kl_loss = F.kl_div(student_soft, teacher_soft, reduction=reduction)

    # Scale by temperature squared (as per original KD paper)
    return kl_loss * (temperature**2)


def topk_kl_divergence_loss(
    student_logits: torch.Tensor,
    teacher_probs: torch.Tensor,
    temperature: float = 4.0,
) -> torch.Tensor:
    """
    Noise-free Top-K KL divergence loss.

    When a teacher only returns probabilities for its Top-K tokens (e.g. Ollama
    top_logprobs), the remaining ~151 000 vocab positions have probability 0.
    Standard KL divergence against a full softmax treats those zeros as a
    uniform signal, forcing the student to learn "flat noise" for the vast
    majority of the vocabulary and degrading its language model.

    This function computes KL divergence **exclusively over the positions where
    the teacher has non-zero probability**, eliminating the noise entirely.

    Args:
        student_logits: [batch, seq_len, vocab_size] â€” raw student logits.
        teacher_probs:  [batch, seq_len, vocab_size] â€” sparse teacher
                        probability tensor (zeros everywhere except Top-K).
        temperature:    Softmax temperature (same as used when obtaining
                        teacher_probs).

    Returns:
        Scalar loss = TÂ² Ã— mean D_KL(teacher_topk â€– student_topk).
    """
    # Mask: positions where the teacher has actual signal (prob > 0)
    mask = (teacher_probs > 0).float()

    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)
    # Avoid log(0) with a small epsilon; only matters on masked-out positions
    teacher_log_probs = torch.log(teacher_probs + 1e-8)

    # D_KL(P â€– Q) = Î£ P * (log P âˆ’ log Q), summed over vocab
    kl = teacher_probs * (teacher_log_probs - student_log_probs)

    # Zero out positions where the teacher has no signal, then normalise
    # by the number of valid (teacher-assigned) tokens rather than vocab size
    masked_kl = (kl * mask).sum(dim=-1)           # [batch, seq_len]
    n_valid = mask.sum(dim=-1).clamp(min=1.0)      # [batch, seq_len]
    loss = (masked_kl / n_valid).mean()

    return loss * (temperature ** 2)


def soft_target_loss(
    student_logits: torch.Tensor,
    teacher_logits: torch.Tensor,
    temperature: float = 4.0,
    attention_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    Compute soft target cross-entropy loss.

    Args:
        student_logits: Student model logits
        teacher_logits: Teacher model logits
        temperature: Temperature for softmax
        attention_mask: Optional mask for valid positions

    Returns:
        Soft target loss
    """
    # Get soft targets from teacher
    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)

    # Log softmax of student
    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)

    # Cross entropy with soft targets
    loss = -(teacher_probs * student_log_probs).sum(dim=-1)

    # Apply mask if provided
    if attention_mask is not None:
        loss = loss * attention_mask
        loss = loss.sum() / attention_mask.sum()
    else:
        loss = loss.mean()

    return loss * (temperature**2)


def feature_distillation_loss(
    student_hidden: torch.Tensor,
    teacher_hidden: torch.Tensor,
    projection: Optional[nn.Module] = None,
    loss_type: str = "mse",
) -> torch.Tensor:
    """
    Compute feature distillation loss between hidden states.

    Args:
        student_hidden: Student hidden states [batch, seq_len, hidden_dim]
        teacher_hidden: Teacher hidden states [batch, seq_len, hidden_dim]
        projection: Optional projection layer for dimension mismatch
        loss_type: Loss type ('mse', 'cosine', 'l1')

    Returns:
        Feature distillation loss
    """
    # Project student features if needed
    if projection is not None:
        student_hidden = projection(student_hidden)

    # Normalize hidden states
    student_norm = F.normalize(student_hidden, p=2, dim=-1)
    teacher_norm = F.normalize(teacher_hidden, p=2, dim=-1)

    if loss_type == "mse":
        loss = F.mse_loss(student_norm, teacher_norm)
    elif loss_type == "cosine":
        loss = 1 - F.cosine_similarity(student_norm, teacher_norm, dim=-1).mean()
    elif loss_type == "l1":
        loss = F.l1_loss(student_norm, teacher_norm)
    else:
        raise ValueError(f"Unknown loss type: {loss_type}")

    return loss


def attention_distillation_loss(
    student_attention: torch.Tensor,
    teacher_attention: torch.Tensor,
    attention_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    Compute attention map distillation loss.

    Args:
        student_attention: Student attention weights [batch, heads, seq, seq]
        teacher_attention: Teacher attention weights [batch, heads, seq, seq]
        attention_mask: Optional attention mask

    Returns:
        Attention distillation loss
    """
    # KL divergence between attention distributions
    student_attn = F.log_softmax(student_attention, dim=-1)
    teacher_attn = F.softmax(teacher_attention, dim=-1)

    kl_loss = F.kl_div(student_attn, teacher_attn, reduction="none")

    if attention_mask is not None:
        # Expand mask for attention heads
        mask = attention_mask.unsqueeze(1).unsqueeze(2)
        kl_loss = kl_loss * mask
        return kl_loss.sum() / mask.sum()

    return kl_loss.mean()


class KDLoss(nn.Module):
    """
    Comprehensive knowledge distillation loss module.

    Combines multiple distillation objectives with configurable weights.
    """

    def __init__(
        self,
        temperature: float = 4.0,
        alpha: float = 0.5,
        use_topk_kl: bool = True,
        use_feature_distillation: bool = False,
        feature_weight: float = 0.1,
        feature_layers: Optional[list[int]] = None,
        use_attention_distillation: bool = False,
        attention_weight: float = 0.1,
        student_hidden_dim: Optional[int] = None,
        teacher_hidden_dim: Optional[int] = None,
    ):
        """
        Initialize KD loss module.

        Args:
            temperature: Temperature for soft targets
            alpha: Weight for distillation loss vs hard label loss
            use_feature_distillation: Whether to use feature distillation
            feature_weight: Weight for feature distillation loss
            feature_layers: Layer indices for feature distillation
            use_topk_kl: If True, use noise-free Top-K KL when teacher_probs
                is a sparse tensor (zeros outside Top-K).  Falls back to full
                KL when teacher provides dense logits.
            use_attention_distillation: Whether to use attention distillation
            attention_weight: Weight for attention distillation loss
            student_hidden_dim: Student hidden dimension (for projection)
            teacher_hidden_dim: Teacher hidden dimension (for projection)
        """
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.use_topk_kl = use_topk_kl
        self.use_feature_distillation = use_feature_distillation
        self.feature_weight = feature_weight
        self.feature_layers = feature_layers or [-1]
        self.use_attention_distillation = use_attention_distillation
        self.attention_weight = attention_weight

        # Create projection layer if dimensions differ
        self.projection = None
        if (
            use_feature_distillation
            and student_hidden_dim is not None
            and teacher_hidden_dim is not None
            and student_hidden_dim != teacher_hidden_dim
        ):
            self.projection = nn.Linear(student_hidden_dim, teacher_hidden_dim)

    def forward(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        hard_labels: Optional[torch.Tensor] = None,
        student_hidden_states: Optional[tuple[torch.Tensor, ...]] = None,
        teacher_hidden_states: Optional[tuple[torch.Tensor, ...]] = None,
        student_attentions: Optional[tuple[torch.Tensor, ...]] = None,
        teacher_attentions: Optional[tuple[torch.Tensor, ...]] = None,
        attention_mask: Optional[torch.Tensor] = None,
    ) -> dict[str, torch.Tensor]:
        """
        Compute knowledge distillation loss.

        Args:
            student_logits: Student model logits
            teacher_logits: Teacher model logits
            hard_labels: Optional hard labels
            student_hidden_states: Optional student hidden states
            teacher_hidden_states: Optional teacher hidden states
            student_attentions: Optional student attention weights
            teacher_attentions: Optional teacher attention weights
            attention_mask: Optional attention mask

        Returns:
            Dictionary containing total loss and component losses
        """
        losses = {}
        total_loss = torch.tensor(0.0, device=student_logits.device)

        # KL divergence loss (main distillation objective).
        # Use noise-free Top-K KL when the teacher tensor is sparse (most
        # entries are exactly 0), otherwise use the standard full-vocab KL.
        teacher_is_sparse = (teacher_logits == 0).float().mean() > 0.5
        if self.use_topk_kl and teacher_is_sparse:
            # teacher_logits is already a probability tensor when sparse
            kd_loss = topk_kl_divergence_loss(
                student_logits,
                teacher_logits,
                temperature=self.temperature,
            )
        else:
            kd_loss = kl_divergence_loss(
                student_logits,
                teacher_logits,
                temperature=self.temperature,
            )
        losses["kd_loss"] = kd_loss
        total_loss = total_loss + self.alpha * kd_loss

        # Hard label loss
        if hard_labels is not None:
            # Shift for causal LM
            shift_logits = student_logits[..., :-1, :].contiguous()
            shift_labels = hard_labels[..., 1:].contiguous()

            hard_loss = F.cross_entropy(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1),
                ignore_index=-100,
            )
            losses["hard_loss"] = hard_loss
            total_loss = total_loss + (1 - self.alpha) * hard_loss

        # Feature distillation loss
        if (
            self.use_feature_distillation
            and student_hidden_states is not None
            and teacher_hidden_states is not None
        ):
            feature_loss = torch.tensor(0.0, device=student_logits.device)

            for layer_idx in self.feature_layers:
                student_hidden = student_hidden_states[layer_idx]
                teacher_hidden = teacher_hidden_states[layer_idx]

                layer_loss = feature_distillation_loss(
                    student_hidden,
                    teacher_hidden,
                    projection=self.projection,
                )
                feature_loss = feature_loss + layer_loss

            feature_loss = feature_loss / len(self.feature_layers)
            losses["feature_loss"] = feature_loss
            total_loss = total_loss + self.feature_weight * feature_loss

        # Attention distillation loss
        if (
            self.use_attention_distillation
            and student_attentions is not None
            and teacher_attentions is not None
        ):
            attn_loss = torch.tensor(0.0, device=student_logits.device)

            for student_attn, teacher_attn in zip(student_attentions, teacher_attentions):
                layer_loss = attention_distillation_loss(
                    student_attn,
                    teacher_attn,
                    attention_mask,
                )
                attn_loss = attn_loss + layer_loss

            attn_loss = attn_loss / len(student_attentions)
            losses["attention_loss"] = attn_loss
            total_loss = total_loss + self.attention_weight * attn_loss

        losses["total_loss"] = total_loss
        return losses


class ProgressiveKDLoss(KDLoss):
    """
    Progressive knowledge distillation with curriculum.

    Starts with high temperature and alpha, gradually reducing.
    """

    def __init__(
        self,
        initial_temperature: float = 10.0,
        final_temperature: float = 1.0,
        initial_alpha: float = 0.9,
        final_alpha: float = 0.1,
        total_steps: int = 10000,
        **kwargs,
    ):
        super().__init__(temperature=initial_temperature, alpha=initial_alpha, **kwargs)
        self.initial_temperature = initial_temperature
        self.final_temperature = final_temperature
        self.initial_alpha = initial_alpha
        self.final_alpha = final_alpha
        self.total_steps = total_steps
        self._current_step = 0

    def step(self) -> None:
        """Update temperature and alpha based on current step."""
        progress = min(1.0, self._current_step / self.total_steps)

        # Linear interpolation
        self.temperature = (
            self.initial_temperature
            + (self.final_temperature - self.initial_temperature) * progress
        )
        self.alpha = self.initial_alpha + (self.final_alpha - self.initial_alpha) * progress

        self._current_step += 1

    def reset(self) -> None:
        """Reset to initial state."""
        self._current_step = 0
        self.temperature = self.initial_temperature
        self.alpha = self.initial_alpha
```

<a id="genesis-distillation-trainer-py"></a>

#### `genesis/distillation/trainer.py`
*16069 bytes Â· ~4,016 tokens*

```python
"""Distillation training loop with dual-GPU support."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any, Callable, Optional
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR
from tqdm import tqdm
import logging

from genesis.distillation.kd_loss import KDLoss
from genesis.config.hardware import synchronize, empty_cache

if TYPE_CHECKING:
    from genesis.models.teacher import TeacherModel
    from genesis.models.student import StudentModel

logger = logging.getLogger(__name__)


@dataclass
class TrainingConfig:
    """Configuration for distillation training."""

    learning_rate: float = 2e-5
    weight_decay: float = 0.01
    max_steps: int = 1000
    warmup_steps: int = 100
    gradient_accumulation_steps: int = 4
    max_grad_norm: float = 1.0
    logging_steps: int = 10
    eval_steps: int = 100
    save_steps: int = 500
    mixed_precision: str = "fp16"  # fp16, bf16, fp32

    # Distillation settings
    temperature: float = 4.0
    alpha: float = 0.5

    # Checkpointing
    output_dir: str = "./outputs"
    save_total_limit: int = 3


class DistillationTrainer:
    """
    Trainer for knowledge distillation with dual-GPU support.

    Manages teacher model on GPU 0 and student model on GPU 1,
    handling cross-device data transfer efficiently.
    """

    def __init__(
        self,
        teacher: TeacherModel,
        student: StudentModel,
        train_dataloader: DataLoader,
        eval_dataloader: Optional[DataLoader] = None,
        config: Optional[TrainingConfig] = None,
        kd_loss: Optional[KDLoss] = None,
    ):
        """
        Initialize the distillation trainer.

        Args:
            teacher: Teacher model wrapper
            student: Student model wrapper
            train_dataloader: Training data loader
            eval_dataloader: Optional evaluation data loader
            config: Training configuration
            kd_loss: Optional custom KD loss module
        """
        self.teacher = teacher
        self.student = student
        self.train_dataloader = train_dataloader
        self.eval_dataloader = eval_dataloader
        self.config = config or TrainingConfig()

        # Initialize KD loss
        self.kd_loss = kd_loss or KDLoss(
            temperature=self.config.temperature,
            alpha=self.config.alpha,
        )

        # Initialize optimizer and scheduler
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()

        # Mixed precision
        self.scaler = None
        if self.config.mixed_precision in ["fp16", "bf16"]:
            self.scaler = torch.amp.GradScaler("cuda")

        # Training state
        self.global_step = 0
        self.epoch = 0
        self.best_eval_loss = float("inf")
        self.training_logs: list[dict[str, Any]] = []

    def _create_optimizer(self) -> AdamW:
        """Create optimizer for student model."""
        # Get trainable parameters
        params = [p for p in self.student.model.parameters() if p.requires_grad]

        return AdamW(
            params,
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
        )

    def _create_scheduler(self):
        """Create learning rate scheduler with warmup."""
        warmup_scheduler = LinearLR(
            self.optimizer,
            start_factor=0.1,
            end_factor=1.0,
            total_iters=self.config.warmup_steps,
        )

        main_scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=self.config.max_steps - self.config.warmup_steps,
            eta_min=1e-7,
        )

        return SequentialLR(
            self.optimizer,
            schedulers=[warmup_scheduler, main_scheduler],
            milestones=[self.config.warmup_steps],
        )

    def train(
        self,
        num_steps: Optional[int] = None,
        callback: Optional[Callable[[dict], None]] = None,
    ) -> dict[str, Any]:
        """
        Run distillation training.

        Args:
            num_steps: Optional override for number of training steps
            callback: Optional callback function called after each step

        Returns:
            Training results dictionary
        """
        num_steps = num_steps or self.config.max_steps
        self.student.model.train()

        # Ensure teacher is in eval mode
        self.teacher.model.eval()

        progress_bar = tqdm(total=num_steps, desc="Training")
        accumulated_loss = 0.0
        accumulated_steps = 0
        log_window_loss = 0.0   # Sum of per-optimizer-step average losses since last log
        log_window_steps = 0    # Number of optimizer steps since last log

        data_iter = iter(self.train_dataloader)

        while self.global_step < num_steps:
            # Get batch
            try:
                batch = next(data_iter)
            except StopIteration:
                data_iter = iter(self.train_dataloader)
                batch = next(data_iter)
                self.epoch += 1

            # Training step
            step_results = self._training_step(batch)
            accumulated_loss += step_results["loss"]
            accumulated_steps += 1

            # Gradient accumulation
            if accumulated_steps >= self.config.gradient_accumulation_steps:
                self._optimization_step()
                self.global_step += 1

                # Record the average loss for this optimizer step, then reset
                log_window_loss += accumulated_loss / self.config.gradient_accumulation_steps
                log_window_steps += 1
                accumulated_loss = 0.0
                accumulated_steps = 0

                # Logging
                if self.global_step % self.config.logging_steps == 0:
                    avg_loss = log_window_loss / log_window_steps
                    log_entry = {
                        "step": self.global_step,
                        "loss": avg_loss,
                        "lr": self.scheduler.get_last_lr()[0],
                        **step_results,
                    }
                    self.training_logs.append(log_entry)
                    logger.info(f"Step {self.global_step}: loss={avg_loss:.4f}")
                    log_window_loss = 0.0
                    log_window_steps = 0

                # Evaluation
                if self.eval_dataloader and self.global_step % self.config.eval_steps == 0:
                    eval_results = self.evaluate()
                    if eval_results["loss"] < self.best_eval_loss:
                        self.best_eval_loss = eval_results["loss"]
                        self._save_checkpoint("best")

                # Callback
                if callback:
                    callback({**step_results, "step": self.global_step, "lr": self.scheduler.get_last_lr()[0]})

                progress_bar.update(1)

        progress_bar.close()

        return {
            "global_step": self.global_step,
            "best_eval_loss": self.best_eval_loss,
            "training_logs": self.training_logs,
        }

    def _training_step(self, batch: dict[str, torch.Tensor]) -> dict[str, Any]:
        """
        Perform a single training step.

        Args:
            batch: Input batch

        Returns:
            Step results including loss components
        """
        input_ids = batch["input_ids"]
        attention_mask = batch.get("attention_mask")
        labels = batch.get("labels", input_ids)

        # Get teacher outputs (no gradients needed)
        with torch.no_grad():
            teacher_outputs = self.teacher.forward(
                input_ids,
                attention_mask,
                output_hidden_states=self.kd_loss.use_feature_distillation,
            )
            teacher_logits = teacher_outputs["logits"]
            has_logprobs = teacher_outputs.get("has_logprobs", teacher_logits is not None)

        # Forward through student
        amp_dtype = torch.float16 if self.config.mixed_precision == "fp16" else torch.bfloat16

        with torch.amp.autocast(device_type="cuda", dtype=amp_dtype, enabled=self.scaler is not None):
            student_outputs = self.student.forward(
                input_ids,
                attention_mask,
                output_hidden_states=self.kd_loss.use_feature_distillation,
            )
            student_logits = student_outputs["logits"]

            if has_logprobs and teacher_logits is not None:
                # Full KD loss: soft targets + hard labels
                teacher_logits_student_device = teacher_logits.to(self.student.device)
                loss_dict = self.kd_loss(
                    student_logits=student_logits,
                    teacher_logits=teacher_logits_student_device,
                    hard_labels=labels.to(self.student.device),
                    student_hidden_states=student_outputs.get("hidden_states"),
                    teacher_hidden_states=self._transfer_hidden_states(
                        teacher_outputs.get("hidden_states")
                    ),
                    attention_mask=attention_mask.to(self.student.device) if attention_mask is not None else None,
                )
            else:
                # No logprobs from teacher â€” use hard-label CE only to avoid
                # corrupting the student by training toward a uniform distribution.
                shift_logits = student_logits[..., :-1, :].contiguous()
                shift_labels = labels.to(self.student.device)[..., 1:].contiguous()
                import torch.nn.functional as F
                hard_loss = F.cross_entropy(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1),
                    ignore_index=-100,
                )
                loss_dict = {
                    "total_loss": hard_loss,
                    "hard_loss": hard_loss,
                    "kd_loss": torch.tensor(0.0, device=self.student.device),
                }

            loss = loss_dict["total_loss"] / self.config.gradient_accumulation_steps

        # Backward pass
        if self.scaler is not None:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()

        return {
            "loss": loss.item() * self.config.gradient_accumulation_steps,
            "kd_loss": loss_dict.get("kd_loss", torch.tensor(0)).item(),
            "hard_loss": loss_dict.get("hard_loss", torch.tensor(0)).item(),
        }

    def _transfer_hidden_states(
        self,
        hidden_states: Optional[tuple[torch.Tensor, ...]],
    ) -> Optional[tuple[torch.Tensor, ...]]:
        """Transfer hidden states to student device."""
        if hidden_states is None:
            return None
        return tuple(h.to(self.student.device) for h in hidden_states)

    def _optimization_step(self) -> None:
        """Perform optimization step with gradient clipping."""
        if self.scaler is not None:
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(
                self.student.model.parameters(),
                self.config.max_grad_norm,
            )
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            torch.nn.utils.clip_grad_norm_(
                self.student.model.parameters(),
                self.config.max_grad_norm,
            )
            self.optimizer.step()

        self.scheduler.step()
        self.optimizer.zero_grad()

    @torch.no_grad()
    def evaluate(self) -> dict[str, float]:
        """
        Evaluate the student model.

        Returns:
            Evaluation metrics
        """
        if self.eval_dataloader is None:
            return {}

        self.student.model.eval()
        total_loss = 0.0
        total_kd_loss = 0.0
        total_hard_loss = 0.0
        num_batches = 0

        for batch in tqdm(self.eval_dataloader, desc="Evaluating"):
            input_ids = batch["input_ids"]
            attention_mask = batch.get("attention_mask")
            labels = batch.get("labels", input_ids)

            # Teacher forward
            teacher_outputs = self.teacher.forward(input_ids, attention_mask)
            teacher_logits = teacher_outputs["logits"]
            has_logprobs = teacher_outputs.get("has_logprobs", teacher_logits is not None)

            # Student forward
            student_outputs = self.student.forward(input_ids, attention_mask)
            student_logits = student_outputs["logits"]

            # Compute loss (hard-label only when teacher has no logprobs)
            if has_logprobs and teacher_logits is not None:
                loss_dict = self.kd_loss(
                    student_logits=student_logits,
                    teacher_logits=teacher_logits.to(self.student.device),
                    hard_labels=labels.to(self.student.device),
                )
            else:
                import torch.nn.functional as F
                shift_logits = student_logits[..., :-1, :].contiguous()
                shift_labels = labels.to(self.student.device)[..., 1:].contiguous()
                hard_loss = F.cross_entropy(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1),
                    ignore_index=-100,
                )
                loss_dict = {
                    "total_loss": hard_loss,
                    "hard_loss": hard_loss,
                    "kd_loss": torch.tensor(0.0),
                }

            total_loss += loss_dict["total_loss"].item()
            total_kd_loss += loss_dict.get("kd_loss", torch.tensor(0)).item()
            total_hard_loss += loss_dict.get("hard_loss", torch.tensor(0)).item()
            num_batches += 1

        self.student.model.train()

        results = {
            "loss": total_loss / num_batches,
            "kd_loss": total_kd_loss / num_batches,
            "hard_loss": total_hard_loss / num_batches,
        }

        logger.info(f"Evaluation: {results}")
        return results

    def _save_checkpoint(self, name: str) -> None:
        """Save training checkpoint."""
        import os

        checkpoint_dir = os.path.join(self.config.output_dir, f"checkpoint-{name}")
        os.makedirs(checkpoint_dir, exist_ok=True)

        # Save student model
        self.student.save(checkpoint_dir)

        # Save training state
        torch.save(
            {
                "global_step": self.global_step,
                "epoch": self.epoch,
                "optimizer_state": self.optimizer.state_dict(),
                "scheduler_state": self.scheduler.state_dict(),
                "best_eval_loss": self.best_eval_loss,
            },
            os.path.join(checkpoint_dir, "training_state.pt"),
        )

        logger.info(f"Checkpoint saved to {checkpoint_dir}")

    def load_checkpoint(self, checkpoint_dir: str) -> None:
        """Load training checkpoint."""
        import os

        # Load training state
        state_path = os.path.join(checkpoint_dir, "training_state.pt")
        if os.path.exists(state_path):
            state = torch.load(state_path, weights_only=False)
            self.global_step = state["global_step"]
            self.epoch = state["epoch"]
            self.optimizer.load_state_dict(state["optimizer_state"])
            self.scheduler.load_state_dict(state["scheduler_state"])
            self.best_eval_loss = state["best_eval_loss"]

        logger.info(f"Checkpoint loaded from {checkpoint_dir}")

    def get_training_state(self) -> dict[str, Any]:
        """Get current training state."""
        return {
            "global_step": self.global_step,
            "epoch": self.epoch,
            "best_eval_loss": self.best_eval_loss,
            "current_lr": self.scheduler.get_last_lr()[0],
        }
```

<a id="genesis-pruning-__init__-py"></a>

#### `genesis/pruning/__init__.py`
*333 bytes Â· ~83 tokens*

```python
"""Model pruning utilities for Genesis."""

from genesis.pruning.saliency import SaliencyCalculator, compute_weight_importance
from genesis.pruning.pruner import Pruner, PruningConfig, PruningStrategy

__all__ = [
    "SaliencyCalculator",
    "compute_weight_importance",
    "Pruner",
    "PruningConfig",
    "PruningStrategy",
]
```

<a id="genesis-pruning-pruner-py"></a>

#### `genesis/pruning/pruner.py`
*18112 bytes Â· ~4,504 tokens*

```python
"""Pruning strategies for model compression."""

from dataclasses import dataclass, field
from enum import Enum
from typing import Optional
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import logging

from genesis.pruning.saliency import SaliencyCalculator, compute_weight_importance

logger = logging.getLogger(__name__)


class PruningStrategy(Enum):
    """Pruning strategy types."""

    MAGNITUDE = "magnitude"
    GRADIENT = "gradient"
    TAYLOR = "taylor"
    RANDOM = "random"
    STRUCTURED = "structured"


@dataclass
class PruningConfig:
    """Configuration for model pruning."""

    target_sparsity: float = 0.3
    pruning_method: str = "magnitude"
    structured: bool = False
    granularity: str = "element"  # element, row, column, block
    block_size: int = 4
    iterative_steps: int = 1

    # Gradual pruning
    initial_sparsity: float = 0.0
    final_sparsity: float = 0.3
    pruning_schedule: str = "cubic"  # linear, cubic, exponential

    # Layer-wise settings
    skip_layers: list[str] = field(default_factory=list)
    layer_sparsity_overrides: dict[str, float] = field(default_factory=dict)


class Pruner:
    """
    Model pruner supporting various pruning strategies.

    Handles both unstructured and structured pruning with
    configurable sparsity schedules.
    """

    def __init__(
        self,
        model: nn.Module,
        config: Optional[PruningConfig] = None,
        dataloader: Optional[DataLoader] = None,
        device: str = "cuda",
    ):
        """
        Initialize pruner.

        Args:
            model: Model to prune
            config: Pruning configuration
            dataloader: DataLoader for importance calculation
            device: Device to run on
        """
        self.model = model
        self.config = config or PruningConfig()
        self.dataloader = dataloader
        self.device = device

        self._masks: dict[str, torch.Tensor] = {}
        self._original_weights: dict[str, torch.Tensor] = {}
        self._current_sparsity = 0.0
        self._pruning_step = 0

        self._saliency_calculator = SaliencyCalculator(
            model=model,
            method=self.config.pruning_method,
            dataloader=dataloader,
            device=device,
        )

    def compute_target_sparsity(self) -> float:
        """Compute target sparsity for current step based on schedule."""
        if self.config.iterative_steps <= 1:
            return self.config.final_sparsity

        progress = self._pruning_step / (self.config.iterative_steps - 1)
        progress = min(1.0, max(0.0, progress))

        initial = self.config.initial_sparsity
        final = self.config.final_sparsity

        if self.config.pruning_schedule == "linear":
            return initial + (final - initial) * progress
        elif self.config.pruning_schedule == "cubic":
            return initial + (final - initial) * (progress ** 3)
        elif self.config.pruning_schedule == "exponential":
            return initial + (final - initial) * (1 - (1 - progress) ** 3)
        else:
            return final

    def prune(self, sparsity: Optional[float] = None) -> dict[str, float]:
        """
        Prune the model to target sparsity.

        Args:
            sparsity: Target sparsity (overrides config if provided)

        Returns:
            Dictionary with pruning statistics
        """
        target_sparsity = sparsity or self.compute_target_sparsity()

        if self.config.structured:
            stats = self._structured_prune(target_sparsity)
        else:
            stats = self._unstructured_prune(target_sparsity)

        self._current_sparsity = target_sparsity
        self._pruning_step += 1

        logger.info(f"Pruning complete. Sparsity: {stats['actual_sparsity']:.2%}")
        return stats

    def _unstructured_prune(self, target_sparsity: float) -> dict[str, float]:
        """Apply unstructured (element-wise) pruning."""
        # Compute importance scores
        importance = self._saliency_calculator.compute()

        # Get masks
        self._masks = self._saliency_calculator.get_pruning_mask(
            sparsity=target_sparsity,
            per_layer=True,
        )

        # Apply masks
        total_params = 0
        pruned_params = 0

        with torch.no_grad():
            for name, param in self.model.named_parameters():
                if name in self._masks and name not in self.config.skip_layers:
                    # Apply layer-specific sparsity if specified
                    if name in self.config.layer_sparsity_overrides:
                        layer_mask = self._saliency_calculator.get_pruning_mask(
                            sparsity=self.config.layer_sparsity_overrides[name],
                            per_layer=True,
                        )[name]
                    else:
                        layer_mask = self._masks[name]

                    # Store original weights
                    if name not in self._original_weights:
                        self._original_weights[name] = param.data.clone()

                    # Apply mask
                    param.data *= layer_mask.to(param.device)

                    total_params += param.numel()
                    pruned_params += (layer_mask == 0).sum().item()

        actual_sparsity = pruned_params / total_params if total_params > 0 else 0.0

        return {
            "target_sparsity": target_sparsity,
            "actual_sparsity": actual_sparsity,
            "total_params": total_params,
            "pruned_params": pruned_params,
        }

    def _structured_prune(self, target_sparsity: float) -> dict[str, float]:
        """Apply structured pruning (row/column/block)."""
        importance = self._saliency_calculator.compute()

        total_structures = 0
        pruned_structures = 0

        with torch.no_grad():
            for name, param in self.model.named_parameters():
                if name not in importance or name in self.config.skip_layers:
                    continue

                if param.dim() < 2:
                    continue  # Skip 1D parameters

                scores = importance[name]

                if self.config.granularity == "row":
                    # Compute row importance (sum along columns)
                    row_importance = scores.abs().sum(dim=-1)
                    num_rows = row_importance.numel()
                    num_to_prune = int(num_rows * target_sparsity)

                    # Get indices of rows to prune
                    _, indices = torch.topk(row_importance, num_to_prune, largest=False)

                    # Create mask
                    mask = torch.ones_like(param)
                    for idx in indices:
                        mask[idx] = 0

                    total_structures += num_rows
                    pruned_structures += num_to_prune

                elif self.config.granularity == "column":
                    # Compute column importance
                    col_importance = scores.abs().sum(dim=0)
                    num_cols = col_importance.numel()
                    num_to_prune = int(num_cols * target_sparsity)

                    _, indices = torch.topk(col_importance, num_to_prune, largest=False)

                    mask = torch.ones_like(param)
                    for idx in indices:
                        mask[:, idx] = 0

                    total_structures += num_cols
                    pruned_structures += num_to_prune

                elif self.config.granularity == "block":
                    # Block-wise pruning
                    mask = self._block_prune(param, scores, target_sparsity)
                    block_size = self.config.block_size
                    num_blocks = (param.numel() // (block_size ** 2))
                    total_structures += num_blocks
                    pruned_structures += int(num_blocks * target_sparsity)

                else:
                    continue

                # Store original and apply mask
                if name not in self._original_weights:
                    self._original_weights[name] = param.data.clone()

                self._masks[name] = mask
                param.data *= mask.to(param.device)

        actual_sparsity = pruned_structures / total_structures if total_structures > 0 else 0.0

        return {
            "target_sparsity": target_sparsity,
            "actual_sparsity": actual_sparsity,
            "total_structures": total_structures,
            "pruned_structures": pruned_structures,
            "granularity": self.config.granularity,
        }

    def _block_prune(
        self,
        param: torch.Tensor,
        scores: torch.Tensor,
        target_sparsity: float,
    ) -> torch.Tensor:
        """Apply block-wise pruning."""
        block_size = self.config.block_size
        mask = torch.ones_like(param)

        if param.dim() != 2:
            return mask

        rows, cols = param.shape
        block_rows = rows // block_size
        block_cols = cols // block_size

        # Compute block importance
        block_importance = []
        block_indices = []

        for i in range(block_rows):
            for j in range(block_cols):
                block = scores[
                    i * block_size: (i + 1) * block_size,
                    j * block_size: (j + 1) * block_size,
                ]
                block_importance.append(block.abs().sum().item())
                block_indices.append((i, j))

        # Sort by importance and prune
        sorted_indices = sorted(
            range(len(block_importance)),
            key=lambda x: block_importance[x],
        )

        num_to_prune = int(len(sorted_indices) * target_sparsity)

        for idx in sorted_indices[:num_to_prune]:
            i, j = block_indices[idx]
            mask[
                i * block_size: (i + 1) * block_size,
                j * block_size: (j + 1) * block_size,
            ] = 0

        return mask

    def restore_weights(self) -> None:
        """Restore original unpruned weights."""
        with torch.no_grad():
            for name, original in self._original_weights.items():
                for param_name, param in self.model.named_parameters():
                    if param_name == name:
                        param.data.copy_(original)
                        break

        self._masks.clear()
        self._current_sparsity = 0.0
        logger.info("Weights restored to original values")

    def apply_masks(self) -> None:
        """Re-apply stored masks (useful after optimizer step)."""
        with torch.no_grad():
            for name, mask in self._masks.items():
                for param_name, param in self.model.named_parameters():
                    if param_name == name:
                        param.data *= mask.to(param.device)
                        break

    def get_sparsity_stats(self) -> dict[str, float]:
        """Get current sparsity statistics."""
        total_params = 0
        zero_params = 0
        layer_stats = {}

        for name, param in self.model.named_parameters():
            layer_total = param.numel()
            layer_zeros = (param == 0).sum().item()

            total_params += layer_total
            zero_params += layer_zeros

            layer_stats[name] = {
                "total": layer_total,
                "zeros": layer_zeros,
                "sparsity": layer_zeros / layer_total if layer_total > 0 else 0,
            }

        return {
            "global_sparsity": zero_params / total_params if total_params > 0 else 0,
            "total_params": total_params,
            "zero_params": zero_params,
            "layer_stats": layer_stats,
        }

    def make_pruning_permanent(self) -> None:
        """Make pruning permanent by removing mask references."""
        self._original_weights.clear()
        logger.info("Pruning made permanent")

    def apply_nvidia_2_4_sparsity(self) -> dict[str, float]:
        """Apply hardware-accelerated 2:4 structured sparsity.

        NVIDIA Ampere+ GPUs (A100, RTX 3090+) support 2:4 *semi-structured*
        sparse matrices natively via Tensor Cores, delivering 2Ã— speedup and
        50 % VRAM reduction with no software emulation overhead.

        The pattern "2:4" means exactly 2 non-zero values in every group of 4
        consecutive elements along the inner dimension of each weight matrix.

        **Scope**: applied to ``nn.Linear`` base-model layers only.
        LoRA A/B adapter layers are excluded (they are too small and the
        sparsity pattern would destroy the low-rank structure).

        **Lifecycle**: call this *after* LoRA merge + full training is complete.
        The conversion sets ``requires_grad = False`` on compressed layers â€”
        they cannot be fine-tuned further in their compressed form.

        Returns:
            Dict with ``actual_sparsity`` (fraction of zeroed weights),
            ``compressed_layers`` count, and ``method="nvidia_2:4"``.

        Raises:
            ImportError: if PyTorch < 2.1 (SparseSemiStructuredTensor missing).
            RuntimeError: if CUDA is not available.
        """
        try:
            from torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor
            SparseSemiStructuredTensor._FORCE_INT32_PTX = True
        except ImportError:
            raise ImportError(
                "NVIDIA 2:4 sparsity requires PyTorch â‰¥ 2.1.  "
                "Upgrade with: pip install torch --upgrade"
            )

        if not torch.cuda.is_available():
            raise RuntimeError("NVIDIA 2:4 sparsity requires a CUDA-capable GPU.")

        logger.info("Applying NVIDIA 2:4 semi-structured sparsity to base-model linear layersâ€¦")

        total_params = 0
        compressed_params = 0
        compressed_layers = 0

        with torch.no_grad():
            for name, module in self.model.named_modules():
                # Target only base-model Linear layers; skip LoRA A/B
                if not isinstance(module, nn.Linear):
                    continue
                if "lora_" in name:
                    continue
                if any(skip in name for skip in self.config.skip_layers):
                    continue

                w = module.weight

                # Hardware constraint: inner dimension must be divisible by 16,
                # outer dimension by 8, and the tensor must be 2-D and on CUDA.
                if w.dim() != 2:
                    continue
                if w.shape[0] % 8 != 0 or w.shape[1] % 16 != 0:
                    logger.debug("Skipping %s â€” shape %s not divisible by 8Ã—16", name, tuple(w.shape))
                    continue
                if w.device.type != "cuda":
                    logger.debug("Skipping %s â€” not on CUDA", name)
                    continue

                total_params += w.numel()

                # â”€â”€ Build 2:4 mask by magnitude â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                # Reshape into groups of 4, zero the 2 smallest-magnitude ones.
                reshaped = w.abs().reshape(-1, 4)
                _, smallest_idx = torch.topk(reshaped, k=2, dim=1, largest=False)
                mask = torch.ones_like(reshaped)
                mask.scatter_(1, smallest_idx, 0.0)
                mask = mask.reshape_as(w)

                # Apply mask to create the 2:4-sparse dense tensor
                w_masked = w * mask

                # â”€â”€ Physical compression to SparseSemiStructuredTensor â”€â”€â”€â”€â”€â”€â”€â”€
                try:
                    module.weight = nn.Parameter(
                        to_sparse_semi_structured(w_masked), requires_grad=False
                    )
                    compressed_params += w.numel() // 2   # 50 % physical storage
                    compressed_layers += 1
                except Exception as exc:
                    # Fall back to dense masked weight if compression fails
                    logger.warning("2:4 compression failed for %s (%s), keeping dense", name, exc)
                    w.data.copy_(w_masked)

        torch.cuda.empty_cache()

        actual_sparsity = compressed_params / max(1, total_params)
        logger.info(
            "2:4 sparsity applied: %d layers compressed, %.1f%% effective sparsity",
            compressed_layers, actual_sparsity * 100,
        )
        return {
            "actual_sparsity": actual_sparsity,
            "compressed_layers": compressed_layers,
            "total_params": total_params,
            "method": "nvidia_2:4",
        }

    @property
    def current_sparsity(self) -> float:
        """Get current sparsity level."""
        return self._current_sparsity

    @property
    def masks(self) -> dict[str, torch.Tensor]:
        """Get current pruning masks."""
        return self._masks


def iterative_prune(
    model: nn.Module,
    config: PruningConfig,
    dataloader: Optional[DataLoader] = None,
    fine_tune_fn: Optional[callable] = None,
    device: str = "cuda",
) -> dict[str, float]:
    """
    Perform iterative pruning with optional fine-tuning between steps.

    Args:
        model: Model to prune
        config: Pruning configuration
        dataloader: DataLoader for importance calculation
        fine_tune_fn: Optional fine-tuning function called after each step
        device: Device to run on

    Returns:
        Final pruning statistics
    """
    pruner = Pruner(model, config, dataloader, device)

    for step in range(config.iterative_steps):
        logger.info(f"Pruning step {step + 1}/{config.iterative_steps}")

        stats = pruner.prune()

        if fine_tune_fn is not None and step < config.iterative_steps - 1:
            logger.info("Fine-tuning after pruning step...")
            fine_tune_fn(model)

            # Re-apply masks after fine-tuning
            pruner.apply_masks()

    pruner.make_pruning_permanent()
    return pruner.get_sparsity_stats()
```

<a id="genesis-pruning-saliency-py"></a>

#### `genesis/pruning/saliency.py`
*9709 bytes Â· ~2,427 tokens*

```python
"""Weight importance calculation for pruning."""

from typing import Callable, Optional
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import logging

logger = logging.getLogger(__name__)


def compute_weight_importance(
    model: nn.Module,
    method: str = "magnitude",
    dataloader: Optional[DataLoader] = None,
    device: str = "cuda",
    num_samples: int = 100,
) -> dict[str, torch.Tensor]:
    """
    Compute importance scores for all model weights.

    Args:
        model: Model to compute importance for
        method: Importance method ('magnitude', 'gradient', 'taylor', 'fisher')
        dataloader: DataLoader for gradient-based methods
        device: Device to run on
        num_samples: Number of samples for gradient-based methods

    Returns:
        Dictionary mapping parameter names to importance scores
    """
    if method == "magnitude":
        return _magnitude_importance(model)
    elif method == "gradient":
        if dataloader is None:
            raise ValueError("DataLoader required for gradient importance")
        return _gradient_importance(model, dataloader, device, num_samples)
    elif method == "taylor":
        if dataloader is None:
            raise ValueError("DataLoader required for Taylor importance")
        return _taylor_importance(model, dataloader, device, num_samples)
    elif method == "fisher":
        if dataloader is None:
            raise ValueError("DataLoader required for Fisher importance")
        return _fisher_importance(model, dataloader, device, num_samples)
    else:
        raise ValueError(f"Unknown importance method: {method}")


def _magnitude_importance(model: nn.Module) -> dict[str, torch.Tensor]:
    """Compute importance based on weight magnitude."""
    importance = {}
    for name, param in model.named_parameters():
        if param.requires_grad:
            importance[name] = torch.abs(param.data)
    return importance


def _gradient_importance(
    model: nn.Module,
    dataloader: DataLoader,
    device: str,
    num_samples: int,
) -> dict[str, torch.Tensor]:
    """Compute importance based on gradient magnitude."""
    model.to(device)
    model.train()

    # Accumulate gradients
    gradient_sums = {name: torch.zeros_like(param) for name, param in model.named_parameters() if param.requires_grad}
    samples_processed = 0

    for batch in dataloader:
        if samples_processed >= num_samples:
            break

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch.get("attention_mask", torch.ones_like(input_ids)).to(device)
        labels = batch.get("labels", input_ids).to(device)

        model.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()

        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                gradient_sums[name] += torch.abs(param.grad)

        samples_processed += input_ids.size(0)

    # Average and compute importance
    importance = {}
    for name, grad_sum in gradient_sums.items():
        importance[name] = grad_sum / max(1, samples_processed)

    return importance


def _taylor_importance(
    model: nn.Module,
    dataloader: DataLoader,
    device: str,
    num_samples: int,
) -> dict[str, torch.Tensor]:
    """
    Compute importance using first-order Taylor expansion.

    Importance = |weight * gradient|
    """
    model.to(device)
    model.train()

    taylor_sums = {name: torch.zeros_like(param) for name, param in model.named_parameters() if param.requires_grad}
    samples_processed = 0

    for batch in dataloader:
        if samples_processed >= num_samples:
            break

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch.get("attention_mask", torch.ones_like(input_ids)).to(device)
        labels = batch.get("labels", input_ids).to(device)

        model.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()

        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                taylor_sums[name] += torch.abs(param.data * param.grad)

        samples_processed += input_ids.size(0)

    # Average
    importance = {}
    for name, taylor_sum in taylor_sums.items():
        importance[name] = taylor_sum / max(1, samples_processed)

    return importance


def _fisher_importance(
    model: nn.Module,
    dataloader: DataLoader,
    device: str,
    num_samples: int,
) -> dict[str, torch.Tensor]:
    """
    Compute importance using Fisher information.

    Approximates the diagonal of the Fisher information matrix.
    """
    model.to(device)
    model.train()

    fisher_sums = {name: torch.zeros_like(param) for name, param in model.named_parameters() if param.requires_grad}
    samples_processed = 0

    for batch in dataloader:
        if samples_processed >= num_samples:
            break

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch.get("attention_mask", torch.ones_like(input_ids)).to(device)
        labels = batch.get("labels", input_ids).to(device)

        model.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()

        for name, param in model.named_parameters():
            if param.requires_grad and param.grad is not None:
                fisher_sums[name] += param.grad ** 2

        samples_processed += input_ids.size(0)

    # Average (Fisher is expectation of squared gradients)
    importance = {}
    for name, fisher_sum in fisher_sums.items():
        importance[name] = fisher_sum / max(1, samples_processed)

    return importance


class SaliencyCalculator:
    """
    Calculator for weight saliency/importance scores.

    Supports multiple importance methods and caching.
    """

    def __init__(
        self,
        model: nn.Module,
        method: str = "magnitude",
        dataloader: Optional[DataLoader] = None,
        device: str = "cuda",
    ):
        """
        Initialize saliency calculator.

        Args:
            model: Model to calculate saliency for
            method: Importance calculation method
            dataloader: DataLoader for gradient-based methods
            device: Device to run on
        """
        self.model = model
        self.method = method
        self.dataloader = dataloader
        self.device = device
        self._importance_cache: Optional[dict[str, torch.Tensor]] = None

    def compute(self, num_samples: int = 100, force_recompute: bool = False) -> dict[str, torch.Tensor]:
        """
        Compute importance scores.

        Args:
            num_samples: Number of samples for gradient-based methods
            force_recompute: Force recomputation even if cached

        Returns:
            Dictionary of importance scores
        """
        if self._importance_cache is not None and not force_recompute:
            return self._importance_cache

        self._importance_cache = compute_weight_importance(
            self.model,
            method=self.method,
            dataloader=self.dataloader,
            device=self.device,
            num_samples=num_samples,
        )

        return self._importance_cache

    def get_top_k_mask(
        self,
        k: float,
        per_layer: bool = True,
    ) -> dict[str, torch.Tensor]:
        """
        Get mask keeping top k% most important weights.

        Args:
            k: Percentage of weights to keep (0-1)
            per_layer: Whether to apply threshold per layer

        Returns:
            Dictionary of binary masks
        """
        importance = self.compute()
        masks = {}

        if per_layer:
            for name, scores in importance.items():
                threshold = torch.quantile(scores.flatten(), 1 - k)
                masks[name] = (scores >= threshold).float()
        else:
            # Global threshold
            all_scores = torch.cat([s.flatten() for s in importance.values()])
            threshold = torch.quantile(all_scores, 1 - k)

            for name, scores in importance.items():
                masks[name] = (scores >= threshold).float()

        return masks

    def get_pruning_mask(
        self,
        sparsity: float,
        per_layer: bool = True,
    ) -> dict[str, torch.Tensor]:
        """
        Get mask for achieving target sparsity.

        Args:
            sparsity: Target sparsity (0-1, percentage of weights to prune)
            per_layer: Whether to apply threshold per layer

        Returns:
            Dictionary of binary masks (1 = keep, 0 = prune)
        """
        return self.get_top_k_mask(k=1 - sparsity, per_layer=per_layer)

    def get_importance_ranking(self) -> list[tuple[str, int, float]]:
        """
        Get global ranking of all weights by importance.

        Returns:
            List of (param_name, flat_index, importance_score)
        """
        importance = self.compute()
        rankings = []

        for name, scores in importance.items():
            flat_scores = scores.flatten()
            for idx, score in enumerate(flat_scores):
                rankings.append((name, idx, score.item()))

        # Sort by importance (ascending for pruning)
        rankings.sort(key=lambda x: x[2])
        return rankings

    def clear_cache(self) -> None:
        """Clear cached importance scores."""
        self._importance_cache = None
```

<a id="genesis-tts-__init__-py"></a>

#### `genesis/tts/__init__.py`
*347 bytes Â· ~86 tokens*

```python
"""TTS evolution system for Genesis."""

from genesis.tts.tts_child import TTSChild, TTSConfig
from genesis.tts.style_evolution import StyleEvolution, StyleToken
from genesis.tts.mcd_fitness import MCDFitness, compute_mcd

__all__ = [
    "TTSChild",
    "TTSConfig",
    "StyleEvolution",
    "StyleToken",
    "MCDFitness",
    "compute_mcd",
]
```

<a id="genesis-tts-mcd_fitness-py"></a>

#### `genesis/tts/mcd_fitness.py`
*9152 bytes Â· ~2,287 tokens*

```python
"""MCD-based fitness evaluation for TTS models."""

from typing import Optional
import torch
import numpy as np
import logging

logger = logging.getLogger(__name__)


def compute_mcd(
    reference_mel: torch.Tensor,
    synthesized_mel: torch.Tensor,
    reduction: str = "mean",
) -> torch.Tensor:
    """
    Compute Mel Cepstral Distortion (MCD) between two mel spectrograms.

    MCD is a common metric for evaluating TTS quality, measuring the
    difference between reference and synthesized speech in the mel-cepstral domain.

    Args:
        reference_mel: Reference mel spectrogram [batch, mel_dim, time]
        synthesized_mel: Synthesized mel spectrogram [batch, mel_dim, time]
        reduction: Reduction method ('mean', 'sum', 'none')

    Returns:
        MCD value (lower is better)
    """
    # Ensure same length (truncate to shorter)
    min_len = min(reference_mel.size(-1), synthesized_mel.size(-1))
    reference_mel = reference_mel[..., :min_len]
    synthesized_mel = synthesized_mel[..., :min_len]

    # Convert to numpy for DCT (if needed)
    if reference_mel.is_cuda:
        ref_np = reference_mel.cpu().numpy()
        syn_np = synthesized_mel.cpu().numpy()
    else:
        ref_np = reference_mel.numpy()
        syn_np = synthesized_mel.numpy()

    # Compute mel cepstral coefficients using DCT
    # MCC = DCT(log mel spectrogram)
    ref_mcc = _mel_to_mcc(ref_np)
    syn_mcc = _mel_to_mcc(syn_np)

    # MCD formula: (10 * sqrt(2) / ln(10)) * sqrt(sum((ref_mcc - syn_mcc)^2))
    # Typically computed over MCC coefficients 1-13 (excluding c0)
    diff = ref_mcc[..., 1:14, :] - syn_mcc[..., 1:14, :]
    frame_mcd = np.sqrt(np.sum(diff ** 2, axis=-2))

    # Scale factor: 10 * sqrt(2) / ln(10) â‰ˆ 6.1415
    scale = 10 * np.sqrt(2) / np.log(10)
    mcd = scale * frame_mcd

    if reduction == "mean":
        return torch.tensor(np.mean(mcd))
    elif reduction == "sum":
        return torch.tensor(np.sum(mcd))
    else:
        return torch.tensor(mcd)


def _mel_to_mcc(mel_spectrogram: np.ndarray, n_mcc: int = 25) -> np.ndarray:
    """
    Convert mel spectrogram to mel cepstral coefficients using DCT.

    Args:
        mel_spectrogram: Mel spectrogram [batch, mel_dim, time]
        n_mcc: Number of MCC coefficients to compute

    Returns:
        MCC coefficients [batch, n_mcc, time]
    """
    from scipy.fftpack import dct

    # Apply log (add small epsilon to avoid log(0))
    log_mel = np.log(mel_spectrogram + 1e-8)

    # Apply DCT along mel dimension
    # DCT-II with orthonormal normalization
    mcc = dct(log_mel, type=2, axis=-2, norm="ortho")

    # Keep first n_mcc coefficients
    return mcc[..., :n_mcc, :]


class MCDFitness:
    """
    Fitness evaluator based on Mel Cepstral Distortion.

    Lower MCD indicates better match to reference, so fitness
    is computed as inverse of MCD.
    """

    def __init__(
        self,
        reference_mels: Optional[list[torch.Tensor]] = None,
        target_mcd: float = 5.0,
        weight_naturalness: float = 0.5,
        weight_similarity: float = 0.5,
        device: str = "cuda",
    ):
        """
        Initialize MCD fitness evaluator.

        Args:
            reference_mels: Optional list of reference mel spectrograms
            target_mcd: Target MCD value for fitness scaling
            weight_naturalness: Weight for naturalness score
            weight_similarity: Weight for similarity score
            device: Device to run on
        """
        self.reference_mels = reference_mels or []
        self.target_mcd = target_mcd
        self.weight_naturalness = weight_naturalness
        self.weight_similarity = weight_similarity
        self.device = device

    def add_reference(self, mel: torch.Tensor) -> None:
        """Add a reference mel spectrogram."""
        self.reference_mels.append(mel)

    def evaluate(
        self,
        synthesized_mel: torch.Tensor,
        reference_idx: Optional[int] = None,
    ) -> dict[str, float]:
        """
        Evaluate fitness of synthesized mel spectrogram.

        Args:
            synthesized_mel: Synthesized mel spectrogram
            reference_idx: Optional specific reference index

        Returns:
            Dictionary with fitness scores
        """
        if not self.reference_mels:
            raise ValueError("No reference mel spectrograms provided")

        # Compute MCD against reference(s)
        if reference_idx is not None:
            references = [self.reference_mels[reference_idx]]
        else:
            references = self.reference_mels

        mcds = []
        for ref in references:
            mcd = compute_mcd(ref, synthesized_mel, reduction="mean")
            mcds.append(mcd.item())

        avg_mcd = np.mean(mcds)
        min_mcd = np.min(mcds)

        # Convert MCD to fitness (lower MCD = higher fitness)
        # Using sigmoid-like transformation
        similarity_fitness = 1.0 / (1.0 + avg_mcd / self.target_mcd)

        # Naturalness score (based on mel statistics)
        naturalness_fitness = self._compute_naturalness(synthesized_mel)

        # Combined fitness
        total_fitness = (
            self.weight_similarity * similarity_fitness
            + self.weight_naturalness * naturalness_fitness
        )

        return {
            "fitness": total_fitness,
            "mcd": avg_mcd,
            "min_mcd": min_mcd,
            "similarity_fitness": similarity_fitness,
            "naturalness_fitness": naturalness_fitness,
        }

    def _compute_naturalness(self, mel: torch.Tensor) -> float:
        """
        Compute naturalness score based on mel statistics.

        Checks if mel spectrogram has reasonable statistics.
        """
        # Convert to numpy
        if mel.is_cuda:
            mel_np = mel.cpu().numpy()
        else:
            mel_np = mel.numpy()

        # Check for reasonable dynamic range
        dynamic_range = mel_np.max() - mel_np.min()
        range_score = min(1.0, dynamic_range / 80.0)  # Expect ~80dB range

        # Check for smoothness (penalize excessive noise)
        diff = np.diff(mel_np, axis=-1)
        smoothness = 1.0 / (1.0 + np.std(diff))

        # Check for reasonable energy distribution
        energy = np.mean(mel_np ** 2)
        energy_score = 1.0 / (1.0 + np.abs(energy - 0.5))

        # Combine scores
        naturalness = 0.4 * range_score + 0.3 * smoothness + 0.3 * energy_score

        return float(naturalness)

    def batch_evaluate(
        self,
        synthesized_mels: list[torch.Tensor],
    ) -> list[dict[str, float]]:
        """
        Evaluate multiple synthesized mel spectrograms.

        Args:
            synthesized_mels: List of synthesized mel spectrograms

        Returns:
            List of fitness dictionaries
        """
        results = []
        for mel in synthesized_mels:
            result = self.evaluate(mel)
            results.append(result)
        return results

    def rank_population(
        self,
        synthesized_mels: list[torch.Tensor],
    ) -> list[tuple[int, float]]:
        """
        Rank population by fitness.

        Args:
            synthesized_mels: List of synthesized mel spectrograms

        Returns:
            List of (index, fitness) tuples sorted by fitness descending
        """
        results = self.batch_evaluate(synthesized_mels)
        indexed_fitness = [(i, r["fitness"]) for i, r in enumerate(results)]
        indexed_fitness.sort(key=lambda x: x[1], reverse=True)
        return indexed_fitness


def compute_f0_rmse(
    reference_f0: torch.Tensor,
    synthesized_f0: torch.Tensor,
) -> torch.Tensor:
    """
    Compute RMSE of F0 (fundamental frequency) contours.

    Args:
        reference_f0: Reference F0 contour
        synthesized_f0: Synthesized F0 contour

    Returns:
        F0 RMSE (lower is better)
    """
    # Align lengths
    min_len = min(reference_f0.size(-1), synthesized_f0.size(-1))
    ref = reference_f0[..., :min_len]
    syn = synthesized_f0[..., :min_len]

    # Only compare voiced regions (F0 > 0)
    voiced_mask = (ref > 0) & (syn > 0)

    if voiced_mask.sum() == 0:
        return torch.tensor(float("inf"))

    # Convert to log scale for perceptually meaningful comparison
    ref_log = torch.log(ref[voiced_mask] + 1e-8)
    syn_log = torch.log(syn[voiced_mask] + 1e-8)

    rmse = torch.sqrt(torch.mean((ref_log - syn_log) ** 2))

    # Convert back to cents (100 cents = 1 semitone)
    rmse_cents = rmse * 1200 / np.log(2)

    return rmse_cents


def compute_vuv_error(
    reference_f0: torch.Tensor,
    synthesized_f0: torch.Tensor,
) -> float:
    """
    Compute voiced/unvoiced error rate.

    Args:
        reference_f0: Reference F0 contour
        synthesized_f0: Synthesized F0 contour

    Returns:
        VUV error rate (0-1, lower is better)
    """
    min_len = min(reference_f0.size(-1), synthesized_f0.size(-1))
    ref = reference_f0[..., :min_len]
    syn = synthesized_f0[..., :min_len]

    ref_voiced = ref > 0
    syn_voiced = syn > 0

    errors = (ref_voiced != syn_voiced).float().mean()

    return errors.item()
```

<a id="genesis-tts-style_evolution-py"></a>

#### `genesis/tts/style_evolution.py`
*10891 bytes Â· ~2,722 tokens*

```python
"""Style token evolution for TTS models."""

from dataclasses import dataclass, field
from typing import Optional
import torch
import torch.nn as nn
import numpy as np
from copy import deepcopy
import logging

logger = logging.getLogger(__name__)


@dataclass
class StyleToken:
    """Represents a learnable style token."""

    embedding: torch.Tensor
    name: str = ""
    fitness: float = 0.0
    metadata: dict = field(default_factory=dict)

    def clone(self) -> "StyleToken":
        """Create a deep copy."""
        return StyleToken(
            embedding=self.embedding.clone(),
            name=self.name,
            fitness=self.fitness,
            metadata=deepcopy(self.metadata),
        )


class StyleEvolution:
    """
    Evolutionary optimization of TTS style tokens.

    Evolves style embeddings to optimize for specific voice
    characteristics like expressiveness, naturalness, etc.
    """

    def __init__(
        self,
        style_dim: int = 128,
        num_tokens: int = 10,
        population_size: int = 20,
        elite_size: int = 2,
        mutation_rate: float = 0.1,
        mutation_scale: float = 0.05,
        crossover_rate: float = 0.7,
    ):
        """
        Initialize style evolution.

        Args:
            style_dim: Dimension of style embeddings
            num_tokens: Number of style tokens per individual
            population_size: Population size
            elite_size: Number of elites to preserve
            mutation_rate: Probability of mutation
            mutation_scale: Scale of mutation noise
            crossover_rate: Probability of crossover
        """
        self.style_dim = style_dim
        self.num_tokens = num_tokens
        self.population_size = population_size
        self.elite_size = elite_size
        self.mutation_rate = mutation_rate
        self.mutation_scale = mutation_scale
        self.crossover_rate = crossover_rate

        self._population: list[torch.Tensor] = []
        self._fitnesses: list[float] = []
        self._generation = 0

    def initialize_population(
        self,
        base_tokens: Optional[torch.Tensor] = None,
        perturbation_scale: float = 0.1,
    ) -> None:
        """
        Initialize the population of style tokens.

        Args:
            base_tokens: Optional base tokens to initialize from
            perturbation_scale: Scale of initial perturbations
        """
        self._population = []
        self._fitnesses = []

        if base_tokens is not None:
            # First individual is the base tokens
            self._population.append(base_tokens.clone())

            # Rest are perturbed versions
            for _ in range(self.population_size - 1):
                perturbed = base_tokens + torch.randn_like(base_tokens) * perturbation_scale
                self._population.append(perturbed)
        else:
            # Random initialization
            for _ in range(self.population_size):
                tokens = torch.randn(self.num_tokens, self.style_dim)
                # Normalize to unit sphere
                tokens = tokens / (tokens.norm(dim=-1, keepdim=True) + 1e-8)
                self._population.append(tokens)

        self._fitnesses = [0.0] * self.population_size
        logger.info(f"Initialized population with {self.population_size} individuals")

    def set_fitness(self, index: int, fitness: float) -> None:
        """Set fitness for individual at index."""
        if 0 <= index < len(self._fitnesses):
            self._fitnesses[index] = fitness

    def set_all_fitnesses(self, fitnesses: list[float]) -> None:
        """Set all fitness values."""
        assert len(fitnesses) == len(self._population)
        self._fitnesses = list(fitnesses)

    def evolve(self) -> None:
        """Perform one generation of evolution."""
        # Sort by fitness
        sorted_indices = np.argsort(self._fitnesses)[::-1]
        sorted_pop = [self._population[i] for i in sorted_indices]
        sorted_fit = [self._fitnesses[i] for i in sorted_indices]

        new_population = []

        # Elitism: keep top individuals
        for i in range(self.elite_size):
            new_population.append(sorted_pop[i].clone())

        # Generate rest through selection, crossover, mutation
        while len(new_population) < self.population_size:
            # Tournament selection
            parent1_idx = self._tournament_select(sorted_fit)
            parent2_idx = self._tournament_select(sorted_fit)

            parent1 = sorted_pop[parent1_idx]
            parent2 = sorted_pop[parent2_idx]

            # Crossover
            if np.random.random() < self.crossover_rate:
                child = self._crossover(parent1, parent2)
            else:
                child = parent1.clone()

            # Mutation
            if np.random.random() < self.mutation_rate:
                child = self._mutate(child)

            new_population.append(child)

        self._population = new_population
        self._fitnesses = [0.0] * self.population_size
        self._generation += 1

        logger.info(f"Generation {self._generation}: Best fitness = {sorted_fit[0]:.4f}")

    def _tournament_select(
        self,
        fitnesses: list[float],
        tournament_size: int = 3,
    ) -> int:
        """Tournament selection."""
        indices = np.random.choice(len(fitnesses), size=tournament_size, replace=False)
        tournament_fitnesses = [fitnesses[i] for i in indices]
        winner_idx = indices[np.argmax(tournament_fitnesses)]
        return winner_idx

    def _crossover(
        self,
        parent1: torch.Tensor,
        parent2: torch.Tensor,
    ) -> torch.Tensor:
        """Perform crossover between two parents."""
        # SLERP-style interpolation
        ratio = np.random.uniform(0.3, 0.7)

        # Normalize parents
        p1_norm = parent1 / (parent1.norm(dim=-1, keepdim=True) + 1e-8)
        p2_norm = parent2 / (parent2.norm(dim=-1, keepdim=True) + 1e-8)

        # Compute angle between vectors
        dot = (p1_norm * p2_norm).sum(dim=-1, keepdim=True).clamp(-1, 1)
        theta = torch.acos(dot)

        # SLERP
        sin_theta = torch.sin(theta) + 1e-8
        child = (
            torch.sin((1 - ratio) * theta) / sin_theta * parent1
            + torch.sin(ratio * theta) / sin_theta * parent2
        )

        return child

    def _mutate(self, tokens: torch.Tensor) -> torch.Tensor:
        """Apply mutation to style tokens."""
        # Create mutation mask
        mask = torch.rand_like(tokens) < 0.2  # Mutate 20% of elements
        noise = torch.randn_like(tokens) * self.mutation_scale

        mutated = tokens + noise * mask.float()

        # Optionally renormalize
        # mutated = mutated / (mutated.norm(dim=-1, keepdim=True) + 1e-8)

        return mutated

    def get_best(self) -> tuple[torch.Tensor, float]:
        """Get best individual and its fitness."""
        best_idx = np.argmax(self._fitnesses)
        return self._population[best_idx], self._fitnesses[best_idx]

    def get_population(self) -> list[torch.Tensor]:
        """Get entire population."""
        return self._population

    def get_individual(self, index: int) -> torch.Tensor:
        """Get individual at index."""
        return self._population[index]

    @property
    def generation(self) -> int:
        """Current generation number."""
        return self._generation

    @property
    def best_fitness(self) -> float:
        """Best fitness in current population."""
        return max(self._fitnesses) if self._fitnesses else 0.0

    @property
    def average_fitness(self) -> float:
        """Average fitness of population."""
        return sum(self._fitnesses) / len(self._fitnesses) if self._fitnesses else 0.0

    def save_state(self, path: str) -> None:
        """Save evolution state to file."""
        state = {
            "generation": self._generation,
            "population": self._population,
            "fitnesses": self._fitnesses,
            "config": {
                "style_dim": self.style_dim,
                "num_tokens": self.num_tokens,
                "population_size": self.population_size,
                "elite_size": self.elite_size,
                "mutation_rate": self.mutation_rate,
                "mutation_scale": self.mutation_scale,
                "crossover_rate": self.crossover_rate,
            },
        }
        torch.save(state, path)

    def load_state(self, path: str) -> None:
        """Load evolution state from file."""
        state = torch.load(path, weights_only=False)
        self._generation = state["generation"]
        self._population = state["population"]
        self._fitnesses = state["fitnesses"]

        config = state["config"]
        self.style_dim = config["style_dim"]
        self.num_tokens = config["num_tokens"]
        self.population_size = config["population_size"]
        self.elite_size = config["elite_size"]
        self.mutation_rate = config["mutation_rate"]
        self.mutation_scale = config["mutation_scale"]
        self.crossover_rate = config["crossover_rate"]


class MultiStyleEvolution:
    """
    Evolve multiple style aspects simultaneously.

    Handles prosody, emotion, speaker characteristics, etc.
    """

    def __init__(
        self,
        style_configs: dict[str, dict],
        population_size: int = 20,
    ):
        """
        Initialize multi-style evolution.

        Args:
            style_configs: Dictionary mapping style name to config
            population_size: Population size
        """
        self.evolutions: dict[str, StyleEvolution] = {}

        for name, config in style_configs.items():
            self.evolutions[name] = StyleEvolution(
                style_dim=config.get("dim", 128),
                num_tokens=config.get("num_tokens", 10),
                population_size=population_size,
                mutation_rate=config.get("mutation_rate", 0.1),
                mutation_scale=config.get("mutation_scale", 0.05),
            )

    def initialize_all(self, base_styles: Optional[dict[str, torch.Tensor]] = None) -> None:
        """Initialize all style evolutions."""
        for name, evolution in self.evolutions.items():
            base = base_styles.get(name) if base_styles else None
            evolution.initialize_population(base)

    def evolve_all(self) -> None:
        """Evolve all style populations."""
        for evolution in self.evolutions.values():
            evolution.evolve()

    def get_combined_style(self, indices: dict[str, int]) -> dict[str, torch.Tensor]:
        """Get combined style from multiple evolutions."""
        result = {}
        for name, idx in indices.items():
            if name in self.evolutions:
                result[name] = self.evolutions[name].get_individual(idx)
        return result
```

<a id="genesis-tts-tts_child-py"></a>

#### `genesis/tts/tts_child.py`
*10969 bytes Â· ~2,742 tokens*

```python
"""TTS model child class for evolutionary optimization."""

from dataclasses import dataclass, field
from typing import Any, Optional
import torch
import torch.nn as nn
import numpy as np
from copy import deepcopy
import uuid
import logging

logger = logging.getLogger(__name__)


@dataclass
class TTSConfig:
    """Configuration for TTS child models."""

    model_type: str = "tacotron2"  # tacotron2, fastspeech2, vits
    style_dim: int = 128
    speaker_dim: int = 256
    num_speakers: int = 1
    sample_rate: int = 22050
    n_mel_channels: int = 80
    hop_length: int = 256
    win_length: int = 1024
    n_fft: int = 1024

    # Evolution parameters
    mutation_rate: float = 0.1
    mutation_scale: float = 0.01


class TTSChild:
    """
    TTS model child for evolutionary optimization.

    Represents an individual TTS model configuration that can be
    evolved through genetic operations.
    """

    def __init__(
        self,
        config: Optional[TTSConfig] = None,
        model: Optional[nn.Module] = None,
        style_tokens: Optional[torch.Tensor] = None,
        speaker_embeddings: Optional[torch.Tensor] = None,
    ):
        """
        Initialize TTS child.

        Args:
            config: TTS configuration
            model: Optional pre-initialized model
            style_tokens: Optional style token embeddings
            speaker_embeddings: Optional speaker embeddings
        """
        self.id = str(uuid.uuid4())[:8]
        self.config = config or TTSConfig()
        self._model = model
        self.fitness = 0.0
        self.generation = 0
        self.parent_ids: list[str] = []
        self.metadata: dict[str, Any] = {}

        # Evolved parameters
        self._style_tokens = style_tokens
        self._speaker_embeddings = speaker_embeddings

        # Initialize if not provided
        if self._style_tokens is None:
            self._style_tokens = torch.randn(10, self.config.style_dim)

        if self._speaker_embeddings is None:
            self._speaker_embeddings = torch.randn(
                self.config.num_speakers,
                self.config.speaker_dim,
            )

    @property
    def style_tokens(self) -> torch.Tensor:
        """Get style token embeddings."""
        return self._style_tokens

    @style_tokens.setter
    def style_tokens(self, value: torch.Tensor) -> None:
        """Set style token embeddings."""
        self._style_tokens = value

    @property
    def speaker_embeddings(self) -> torch.Tensor:
        """Get speaker embeddings."""
        return self._speaker_embeddings

    @speaker_embeddings.setter
    def speaker_embeddings(self, value: torch.Tensor) -> None:
        """Set speaker embeddings."""
        self._speaker_embeddings = value

    @property
    def model(self) -> Optional[nn.Module]:
        """Get the TTS model."""
        return self._model

    def load_model(self, model_path: str, device: str = "cuda") -> None:
        """
        Load TTS model from checkpoint.

        Args:
            model_path: Path to model checkpoint
            device: Device to load on
        """
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)

        if "model_state_dict" in checkpoint:
            state_dict = checkpoint["model_state_dict"]
        else:
            state_dict = checkpoint

        # Model architecture depends on config
        if self.config.model_type == "tacotron2":
            self._model = self._create_tacotron2()
        elif self.config.model_type == "fastspeech2":
            self._model = self._create_fastspeech2()
        elif self.config.model_type == "vits":
            self._model = self._create_vits()

        if self._model is not None:
            self._model.load_state_dict(state_dict, strict=False)
            self._model.to(device)

    def _create_tacotron2(self) -> nn.Module:
        """Create Tacotron2-like architecture placeholder."""
        # Placeholder - actual implementation would use real Tacotron2
        return nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, self.config.n_mel_channels),
        )

    def _create_fastspeech2(self) -> nn.Module:
        """Create FastSpeech2-like architecture placeholder."""
        return nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, self.config.n_mel_channels),
        )

    def _create_vits(self) -> nn.Module:
        """Create VITS-like architecture placeholder."""
        return nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, self.config.n_mel_channels),
        )

    def synthesize(
        self,
        text: str,
        speaker_id: int = 0,
        style_idx: Optional[int] = None,
        device: str = "cuda",
    ) -> dict[str, torch.Tensor]:
        """
        Synthesize speech from text.

        Args:
            text: Input text
            speaker_id: Speaker ID for multi-speaker models
            style_idx: Optional style token index
            device: Device to run on

        Returns:
            Dictionary with mel spectrogram and optional audio
        """
        if self._model is None:
            raise RuntimeError("Model not loaded. Call load_model() first.")

        self._model.eval()
        self._model.to(device)

        # Get embeddings
        speaker_emb = self._speaker_embeddings[speaker_id].to(device)
        style_emb = (
            self._style_tokens[style_idx].to(device)
            if style_idx is not None
            else self._style_tokens.mean(dim=0).to(device)
        )

        # Placeholder synthesis (actual implementation would be more complex)
        with torch.no_grad():
            # This is a placeholder - real implementation would:
            # 1. Convert text to phonemes/tokens
            # 2. Run through encoder
            # 3. Condition on speaker and style
            # 4. Generate mel spectrogram
            # 5. Optionally run vocoder

            # For now, return dummy output
            seq_len = len(text) * 10  # Rough estimate
            mel = torch.randn(1, self.config.n_mel_channels, seq_len, device=device)

        return {
            "mel_spectrogram": mel,
            "speaker_embedding": speaker_emb,
            "style_embedding": style_emb,
        }

    def get_evolved_params(self) -> dict[str, torch.Tensor]:
        """Get all evolved parameters."""
        return {
            "style_tokens": self._style_tokens,
            "speaker_embeddings": self._speaker_embeddings,
        }

    def set_evolved_params(self, params: dict[str, torch.Tensor]) -> None:
        """Set evolved parameters."""
        if "style_tokens" in params:
            self._style_tokens = params["style_tokens"]
        if "speaker_embeddings" in params:
            self._speaker_embeddings = params["speaker_embeddings"]

    def mutate(
        self,
        mutation_rate: Optional[float] = None,
        mutation_scale: Optional[float] = None,
    ) -> "TTSChild":
        """
        Create mutated copy of this child.

        Args:
            mutation_rate: Override default mutation rate
            mutation_scale: Override default mutation scale

        Returns:
            Mutated TTSChild
        """
        rate = mutation_rate or self.config.mutation_rate
        scale = mutation_scale or self.config.mutation_scale

        child = TTSChild(
            config=self.config,
            model=self._model,
            style_tokens=self._style_tokens.clone(),
            speaker_embeddings=self._speaker_embeddings.clone(),
        )

        # Mutate style tokens
        if np.random.random() < rate:
            mask = torch.rand_like(child._style_tokens) < 0.1
            noise = torch.randn_like(child._style_tokens) * scale
            child._style_tokens = child._style_tokens + noise * mask.float()

        # Mutate speaker embeddings
        if np.random.random() < rate:
            mask = torch.rand_like(child._speaker_embeddings) < 0.1
            noise = torch.randn_like(child._speaker_embeddings) * scale
            child._speaker_embeddings = child._speaker_embeddings + noise * mask.float()

        child.generation = self.generation + 1
        child.parent_ids = [self.id]

        return child

    def crossover(self, other: "TTSChild") -> "TTSChild":
        """
        Create child through crossover with another TTSChild.

        Args:
            other: Other parent

        Returns:
            Child TTSChild
        """
        child = TTSChild(
            config=self.config,
            model=self._model,
        )

        # Interpolate style tokens
        ratio = np.random.uniform(0.3, 0.7)
        child._style_tokens = (
            ratio * self._style_tokens + (1 - ratio) * other._style_tokens
        )

        # Interpolate speaker embeddings
        child._speaker_embeddings = (
            ratio * self._speaker_embeddings + (1 - ratio) * other._speaker_embeddings
        )

        child.generation = max(self.generation, other.generation) + 1
        child.parent_ids = [self.id, other.id]

        return child

    def clone(self) -> "TTSChild":
        """Create a deep copy of this child."""
        child = TTSChild(
            config=self.config,
            model=self._model,
            style_tokens=self._style_tokens.clone(),
            speaker_embeddings=self._speaker_embeddings.clone(),
        )
        child.fitness = self.fitness
        child.generation = self.generation
        child.parent_ids = [self.id]
        child.metadata = deepcopy(self.metadata)
        return child

    def save(self, path: str) -> None:
        """Save TTS child to file."""
        state = {
            "id": self.id,
            "config": self.config.__dict__,
            "fitness": self.fitness,
            "generation": self.generation,
            "parent_ids": self.parent_ids,
            "metadata": self.metadata,
            "style_tokens": self._style_tokens,
            "speaker_embeddings": self._speaker_embeddings,
        }
        torch.save(state, path)

    @classmethod
    def load(cls, path: str) -> "TTSChild":
        """Load TTS child from file."""
        state = torch.load(path, weights_only=False)
        config = TTSConfig(**state["config"])

        child = cls(
            config=config,
            style_tokens=state["style_tokens"],
            speaker_embeddings=state["speaker_embeddings"],
        )
        child.id = state["id"]
        child.fitness = state["fitness"]
        child.generation = state["generation"]
        child.parent_ids = state["parent_ids"]
        child.metadata = state["metadata"]

        return child

    def __repr__(self) -> str:
        return (
            f"TTSChild(id={self.id}, fitness={self.fitness:.4f}, "
            f"gen={self.generation}, type={self.config.model_type})"
        )
```


### ğŸ”§ Utilities

<a id="genesis-utils-__init__-py"></a>

#### `genesis/utils/__init__.py`
*521 bytes Â· ~130 tokens*

```python
"""Utilities for Genesis."""

from genesis.utils.logging import setup_logging, get_logger
from genesis.utils.checkpointing import CheckpointManager, save_checkpoint, load_checkpoint
from genesis.utils.metrics import (
    MetricsTracker,
    compute_perplexity,
    compute_accuracy,
    compute_bleu,
)

__all__ = [
    "setup_logging",
    "get_logger",
    "CheckpointManager",
    "save_checkpoint",
    "load_checkpoint",
    "MetricsTracker",
    "compute_perplexity",
    "compute_accuracy",
    "compute_bleu",
]
```

<a id="genesis-utils-checkpointing-py"></a>

#### `genesis/utils/checkpointing.py`
*10786 bytes Â· ~2,696 tokens*

```python
"""Model checkpointing utilities for Genesis."""

from pathlib import Path
from typing import Any, Optional
import torch
import torch.nn as nn
import json
import shutil
import logging
from datetime import datetime

logger = logging.getLogger(__name__)


def save_checkpoint(
    path: str,
    model: Optional[nn.Module] = None,
    optimizer: Optional[torch.optim.Optimizer] = None,
    scheduler: Optional[Any] = None,
    epoch: int = 0,
    step: int = 0,
    best_metric: float = 0.0,
    config: Optional[dict] = None,
    **kwargs,
) -> None:
    """
    Save a training checkpoint.

    Args:
        path: Save path
        model: Model to save
        optimizer: Optimizer state
        scheduler: Scheduler state
        epoch: Current epoch
        step: Current step
        best_metric: Best metric value
        config: Configuration dictionary
        **kwargs: Additional items to save
    """
    checkpoint = {
        "epoch": epoch,
        "step": step,
        "best_metric": best_metric,
        "timestamp": datetime.now().isoformat(),
    }

    if model is not None:
        checkpoint["model_state_dict"] = model.state_dict()

    if optimizer is not None:
        checkpoint["optimizer_state_dict"] = optimizer.state_dict()

    if scheduler is not None:
        checkpoint["scheduler_state_dict"] = scheduler.state_dict()

    if config is not None:
        checkpoint["config"] = config

    checkpoint.update(kwargs)

    # Create directory if needed
    Path(path).parent.mkdir(parents=True, exist_ok=True)

    torch.save(checkpoint, path)
    logger.info(f"Checkpoint saved to {path}")


def load_checkpoint(
    path: str,
    model: Optional[nn.Module] = None,
    optimizer: Optional[torch.optim.Optimizer] = None,
    scheduler: Optional[Any] = None,
    device: str = "cpu",
    strict: bool = True,
) -> dict:
    """
    Load a training checkpoint.

    Args:
        path: Checkpoint path
        model: Model to load state into
        optimizer: Optimizer to load state into
        scheduler: Scheduler to load state into
        device: Device to load onto
        strict: Strict loading for model state dict

    Returns:
        Checkpoint dictionary
    """
    checkpoint = torch.load(path, map_location=device, weights_only=False)

    if model is not None and "model_state_dict" in checkpoint:
        model.load_state_dict(checkpoint["model_state_dict"], strict=strict)
        logger.info("Model state loaded")

    if optimizer is not None and "optimizer_state_dict" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        logger.info("Optimizer state loaded")

    if scheduler is not None and "scheduler_state_dict" in checkpoint:
        scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        logger.info("Scheduler state loaded")

    logger.info(f"Checkpoint loaded from {path}")
    return checkpoint


class CheckpointManager:
    """
    Manages model checkpoints with automatic cleanup.
    """

    def __init__(
        self,
        checkpoint_dir: str,
        max_checkpoints: int = 5,
        checkpoint_prefix: str = "checkpoint",
        metric_name: str = "loss",
        mode: str = "min",
    ):
        """
        Initialize checkpoint manager.

        Args:
            checkpoint_dir: Directory to save checkpoints
            max_checkpoints: Maximum number of checkpoints to keep
            checkpoint_prefix: Prefix for checkpoint filenames
            metric_name: Name of metric to track
            mode: 'min' or 'max' for metric comparison
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.max_checkpoints = max_checkpoints
        self.checkpoint_prefix = checkpoint_prefix
        self.metric_name = metric_name
        self.mode = mode

        self.best_metric = float("inf") if mode == "min" else float("-inf")
        self._checkpoints: list[dict] = []

        # Load existing checkpoints info
        self._load_checkpoint_info()

    def _load_checkpoint_info(self) -> None:
        """Load information about existing checkpoints."""
        info_path = self.checkpoint_dir / "checkpoints.json"
        if info_path.exists():
            with open(info_path, "r") as f:
                data = json.load(f)
                self._checkpoints = data.get("checkpoints", [])
                self.best_metric = data.get("best_metric", self.best_metric)

    def _save_checkpoint_info(self) -> None:
        """Save checkpoint information to JSON."""
        info_path = self.checkpoint_dir / "checkpoints.json"
        with open(info_path, "w") as f:
            json.dump(
                {
                    "checkpoints": self._checkpoints,
                    "best_metric": self.best_metric,
                },
                f,
                indent=2,
            )

    def save(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        epoch: int = 0,
        step: int = 0,
        metric: Optional[float] = None,
        **kwargs,
    ) -> Optional[str]:
        """
        Save a checkpoint.

        Args:
            model: Model to save
            optimizer: Optional optimizer
            scheduler: Optional scheduler
            epoch: Current epoch
            step: Current step
            metric: Current metric value
            **kwargs: Additional items to save

        Returns:
            Path to saved checkpoint, or None if not saved
        """
        # Determine checkpoint name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_name = f"{self.checkpoint_prefix}_step{step}_{timestamp}.pt"
        checkpoint_path = self.checkpoint_dir / checkpoint_name

        # Check if this is the best checkpoint
        is_best = False
        if metric is not None:
            if self.mode == "min" and metric < self.best_metric:
                self.best_metric = metric
                is_best = True
            elif self.mode == "max" and metric > self.best_metric:
                self.best_metric = metric
                is_best = True

        # Save checkpoint
        save_checkpoint(
            path=str(checkpoint_path),
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=epoch,
            step=step,
            best_metric=self.best_metric,
            **kwargs,
        )

        # Update checkpoint list
        self._checkpoints.append(
            {
                "path": str(checkpoint_path),
                "step": step,
                "epoch": epoch,
                "metric": metric,
                "timestamp": timestamp,
                "is_best": is_best,
            }
        )

        # Save best checkpoint separately
        if is_best:
            best_path = self.checkpoint_dir / f"{self.checkpoint_prefix}_best.pt"
            shutil.copy(checkpoint_path, best_path)
            logger.info(f"New best checkpoint! {self.metric_name}={metric:.4f}")

        # Cleanup old checkpoints
        self._cleanup()

        # Save checkpoint info
        self._save_checkpoint_info()

        return str(checkpoint_path)

    def _cleanup(self) -> None:
        """Remove old checkpoints exceeding max_checkpoints."""
        # Keep best checkpoint and most recent ones
        if len(self._checkpoints) <= self.max_checkpoints:
            return

        # Sort by step (most recent first)
        sorted_checkpoints = sorted(
            self._checkpoints,
            key=lambda x: x["step"],
            reverse=True,
        )

        # Keep best and most recent
        to_keep = set()
        for cp in sorted_checkpoints:
            if cp["is_best"]:
                to_keep.add(cp["path"])

        for cp in sorted_checkpoints[: self.max_checkpoints - 1]:
            to_keep.add(cp["path"])

        # Remove old checkpoints
        new_checkpoints = []
        for cp in self._checkpoints:
            if cp["path"] in to_keep:
                new_checkpoints.append(cp)
            else:
                # Delete file
                path = Path(cp["path"])
                if path.exists():
                    path.unlink()
                    logger.debug(f"Removed old checkpoint: {cp['path']}")

        self._checkpoints = new_checkpoints

    def load_best(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        device: str = "cpu",
    ) -> dict:
        """
        Load the best checkpoint.

        Args:
            model: Model to load state into
            optimizer: Optional optimizer
            scheduler: Optional scheduler
            device: Device to load onto

        Returns:
            Checkpoint dictionary
        """
        best_path = self.checkpoint_dir / f"{self.checkpoint_prefix}_best.pt"
        if not best_path.exists():
            raise FileNotFoundError("No best checkpoint found")

        return load_checkpoint(
            path=str(best_path),
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            device=device,
        )

    def load_latest(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        device: str = "cpu",
    ) -> dict:
        """
        Load the most recent checkpoint.

        Args:
            model: Model to load state into
            optimizer: Optional optimizer
            scheduler: Optional scheduler
            device: Device to load onto

        Returns:
            Checkpoint dictionary
        """
        if not self._checkpoints:
            raise FileNotFoundError("No checkpoints found")

        latest = max(self._checkpoints, key=lambda x: x["step"])

        return load_checkpoint(
            path=latest["path"],
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            device=device,
        )

    def get_checkpoint_list(self) -> list[dict]:
        """Get list of all checkpoints."""
        return self._checkpoints.copy()

    @property
    def latest_checkpoint(self) -> Optional[str]:
        """Get path to latest checkpoint."""
        if not self._checkpoints:
            return None
        return max(self._checkpoints, key=lambda x: x["step"])["path"]

    @property
    def best_checkpoint(self) -> Optional[str]:
        """Get path to best checkpoint."""
        best_path = self.checkpoint_dir / f"{self.checkpoint_prefix}_best.pt"
        return str(best_path) if best_path.exists() else None
```

<a id="genesis-utils-dashboard-py"></a>

#### `genesis/utils/dashboard.py`
*9620 bytes Â· ~2,398 tokens*

```python
"""Live training dashboard with Rich terminal display and matplotlib plot generation."""

from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Optional

from rich.console import Console
from rich.live import Live
from rich.table import Table
from rich.panel import Panel
from rich.layout import Layout
from rich.text import Text


class TrainingDashboard:
    """
    Live terminal dashboard for training metrics + persistent plot/report generation.

    Uses Rich Live for a live-updating terminal panel and stores metric history
    in-memory for later plot generation.
    """

    def __init__(self, title: str = "Genesis Training Dashboard"):
        self.title = title
        self.console = Console()

        # History stores
        self._distill_history: list[dict] = []
        self._evo_history: list[dict] = []

        # Live display state (latest values for the live table)
        self._last_distill: dict = {}
        self._last_evo: dict = {}

        # Rich Live context
        self._live: Optional[Live] = None

    # ------------------------------------------------------------------
    # Live display lifecycle
    # ------------------------------------------------------------------

    def start(self) -> None:
        """Start the Rich Live display."""
        self._live = Live(
            self._build_renderable(),
            console=self.console,
            refresh_per_second=4,
            transient=False,
        )
        self._live.__enter__()

    def stop(self) -> None:
        """Stop the Rich Live display."""
        if self._live is not None:
            self._live.__exit__(None, None, None)
            self._live = None

    def _build_renderable(self):
        """Build the Rich renderable panel from current state."""
        table = Table(title=self.title, expand=True, show_lines=True)

        # Distillation section
        if self._last_distill or self._distill_history:
            table.add_column("Metric", style="cyan", no_wrap=True)
            table.add_column("Value", style="green")

            d = self._last_distill
            table.add_row("Phase", "Distillation")
            table.add_row("Step", str(d.get("step", "â€”")))
            table.add_row("Loss", f"{d.get('loss', float('nan')):.4f}" if "loss" in d else "â€”")
            table.add_row("KD Loss", f"{d.get('kd_loss', float('nan')):.4f}" if "kd_loss" in d else "â€”")
            table.add_row("Hard Loss", f"{d.get('hard_loss', float('nan')):.4f}" if "hard_loss" in d else "â€”")
            table.add_row("LR", f"{d.get('lr', float('nan')):.2e}" if "lr" in d else "â€”")

        # Evolutionary section
        if self._last_evo or self._evo_history:
            e = self._last_evo
            if not (self._last_distill or self._distill_history):
                table.add_column("Metric", style="cyan", no_wrap=True)
                table.add_column("Value", style="green")
            table.add_row("â€”", "â€”")
            table.add_row("Phase", "Evolution")
            table.add_row("Generation", str(e.get("gen", "â€”")))
            table.add_row("Best Fitness", f"{e.get('best_fitness', float('nan')):.4f}" if "best_fitness" in e else "â€”")
            table.add_row("Avg Fitness", f"{e.get('avg_fitness', float('nan')):.4f}" if "avg_fitness" in e else "â€”")
            table.add_row("Diversity", f"{e.get('diversity', float('nan')):.4f}" if "diversity" in e else "â€”")
            table.add_row("Mutation Rate", f"{e.get('mutation_rate', float('nan')):.4f}" if "mutation_rate" in e else "â€”")

        if not table.columns:
            return Panel(Text("Waiting for metricsâ€¦", style="yellow"), title=self.title)

        return Panel(table, title=self.title)

    def _refresh(self) -> None:
        """Refresh the live display if active."""
        if self._live is not None:
            self._live.update(self._build_renderable())

    # ------------------------------------------------------------------
    # Update methods
    # ------------------------------------------------------------------

    def update_distillation(
        self,
        payload: dict,
    ) -> None:
        """
        Update dashboard with a distillation step's metrics.

        Accepts either a raw dict (from trainer callback) or keyword args.
        The trainer callback dict contains: loss, kd_loss, hard_loss, step, lr.
        """
        step = payload.get("step", len(self._distill_history))
        loss = payload.get("loss", float("nan"))
        kd_loss = payload.get("kd_loss", float("nan"))
        hard_loss = payload.get("hard_loss", float("nan"))
        lr = payload.get("lr", float("nan"))

        entry = {
            "step": step,
            "loss": loss,
            "kd_loss": kd_loss,
            "hard_loss": hard_loss,
            "lr": lr,
        }
        self._distill_history.append(entry)
        self._last_distill = entry
        self._refresh()

    def update_evolution(
        self,
        gen: int,
        best_fitness: float,
        avg_fitness: float,
        diversity: float,
        mutation_rate: float,
    ) -> None:
        """Update dashboard with an evolutionary generation's metrics."""
        entry = {
            "gen": gen,
            "best_fitness": best_fitness,
            "avg_fitness": avg_fitness,
            "diversity": diversity,
            "mutation_rate": mutation_rate,
        }
        self._evo_history.append(entry)
        self._last_evo = entry
        self._refresh()

    # ------------------------------------------------------------------
    # Persistence
    # ------------------------------------------------------------------

    def save_plots(self, output_dir: str) -> None:
        """
        Generate and save 4 matplotlib plots as PNGs.

        Plots saved:
        - loss_curve.png  : total loss + kd_loss + hard_loss vs step
        - lr_curve.png    : learning rate schedule vs step
        - fitness_curve.png: best/avg fitness vs generation
        - diversity_curve.png: population diversity vs generation
        """
        import matplotlib
        matplotlib.use("Agg")  # non-interactive backend
        import matplotlib.pyplot as plt

        plots_dir = Path(output_dir) / "plots"
        plots_dir.mkdir(parents=True, exist_ok=True)

        # --- Loss curve ---
        if self._distill_history:
            steps = [e["step"] for e in self._distill_history]
            total = [e["loss"] for e in self._distill_history]
            kd = [e["kd_loss"] for e in self._distill_history]
            hard = [e["hard_loss"] for e in self._distill_history]

            fig, ax = plt.subplots(figsize=(10, 5))
            ax.plot(steps, total, label="Total Loss", linewidth=2)
            ax.plot(steps, kd, label="KD Loss", linewidth=1.5, linestyle="--")
            ax.plot(steps, hard, label="Hard Loss", linewidth=1.5, linestyle=":")
            ax.set_xlabel("Step")
            ax.set_ylabel("Loss")
            ax.set_title("Distillation Loss Curves")
            ax.legend()
            ax.grid(alpha=0.3)
            fig.tight_layout()
            fig.savefig(plots_dir / "loss_curve.png", dpi=120)
            plt.close(fig)

            # --- LR curve ---
            lrs = [e["lr"] for e in self._distill_history]
            fig, ax = plt.subplots(figsize=(10, 4))
            ax.plot(steps, lrs, color="orange", linewidth=2)
            ax.set_xlabel("Step")
            ax.set_ylabel("Learning Rate")
            ax.set_title("Learning Rate Schedule")
            ax.grid(alpha=0.3)
            fig.tight_layout()
            fig.savefig(plots_dir / "lr_curve.png", dpi=120)
            plt.close(fig)

        # --- Fitness curve ---
        if self._evo_history:
            gens = [e["gen"] for e in self._evo_history]
            best = [e["best_fitness"] for e in self._evo_history]
            avg = [e["avg_fitness"] for e in self._evo_history]

            fig, ax = plt.subplots(figsize=(10, 5))
            ax.plot(gens, best, label="Best Fitness", linewidth=2, color="green")
            ax.plot(gens, avg, label="Avg Fitness", linewidth=1.5, linestyle="--", color="blue")
            ax.set_xlabel("Generation")
            ax.set_ylabel("Fitness")
            ax.set_title("Evolutionary Fitness Curves")
            ax.legend()
            ax.grid(alpha=0.3)
            fig.tight_layout()
            fig.savefig(plots_dir / "fitness_curve.png", dpi=120)
            plt.close(fig)

            # --- Diversity curve ---
            diversity = [e["diversity"] for e in self._evo_history]
            fig, ax = plt.subplots(figsize=(10, 4))
            ax.plot(gens, diversity, color="purple", linewidth=2)
            ax.set_xlabel("Generation")
            ax.set_ylabel("Diversity")
            ax.set_title("Population Diversity")
            ax.grid(alpha=0.3)
            fig.tight_layout()
            fig.savefig(plots_dir / "diversity_curve.png", dpi=120)
            plt.close(fig)

        self.console.print(f"[green]Plots saved to {plots_dir}[/green]")

    def save_report(self, output_dir: str) -> None:
        """Save a JSON report with all metric history."""
        output_path = Path(output_dir) / "training_report.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)

        report = {
            "distillation": self._distill_history,
            "evolution": self._evo_history,
        }

        with open(output_path, "w") as f:
            json.dump(report, f, indent=2)

        self.console.print(f"[green]Report saved to {output_path}[/green]")
```

<a id="genesis-utils-logging-py"></a>

#### `genesis/utils/logging.py`
*7660 bytes Â· ~1,915 tokens*

```python
"""Logging configuration for Genesis."""

import logging
import sys
from pathlib import Path
from typing import Optional
from datetime import datetime


def setup_logging(
    log_level: str = "INFO",
    log_file: Optional[str] = None,
    log_dir: Optional[str] = None,
    name: str = "genesis",
    format_string: Optional[str] = None,
) -> logging.Logger:
    """
    Set up logging configuration.

    Args:
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR)
        log_file: Optional log file path
        log_dir: Optional log directory (creates timestamped file)
        name: Logger name
        format_string: Optional custom format string

    Returns:
        Configured logger
    """
    # Create logger
    logger = logging.getLogger(name)
    logger.setLevel(getattr(logging, log_level.upper()))

    # Clear existing handlers
    logger.handlers.clear()

    # Default format
    if format_string is None:
        format_string = "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"

    formatter = logging.Formatter(format_string, datefmt="%Y-%m-%d %H:%M:%S")

    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(getattr(logging, log_level.upper()))
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # File handler
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)  # Log everything to file
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    elif log_dir:
        Path(log_dir).mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_path = Path(log_dir) / f"{name}_{timestamp}.log"
        file_handler = logging.FileHandler(log_path)
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger


def get_logger(name: str = "genesis") -> logging.Logger:
    """
    Get or create a logger with the given name.

    Args:
        name: Logger name

    Returns:
        Logger instance
    """
    logger = logging.getLogger(name)

    # If no handlers, set up basic logging
    if not logger.handlers:
        setup_logging(name=name)

    return logger


class LoggerAdapter(logging.LoggerAdapter):
    """
    Logger adapter with additional context.
    """

    def __init__(self, logger: logging.Logger, prefix: str = ""):
        super().__init__(logger, {})
        self.prefix = prefix

    def process(self, msg, kwargs):
        if self.prefix:
            return f"[{self.prefix}] {msg}", kwargs
        return msg, kwargs


class TrainingLogger:
    """
    Specialized logger for training progress.
    """

    def __init__(
        self,
        name: str = "genesis.training",
        log_dir: Optional[str] = None,
        use_tensorboard: bool = True,
        use_wandb: bool = False,
        wandb_project: Optional[str] = None,
    ):
        """
        Initialize training logger.

        Args:
            name: Logger name
            log_dir: Log directory
            use_tensorboard: Enable TensorBoard logging
            use_wandb: Enable Weights & Biases logging
            wandb_project: W&B project name
        """
        self.logger = get_logger(name)
        self.log_dir = log_dir

        self.tb_writer = None
        self.wandb_run = None

        if use_tensorboard and log_dir:
            try:
                from torch.utils.tensorboard import SummaryWriter

                tb_dir = Path(log_dir) / "tensorboard"
                tb_dir.mkdir(parents=True, exist_ok=True)
                self.tb_writer = SummaryWriter(log_dir=str(tb_dir))
                self.logger.info(f"TensorBoard logging to {tb_dir}")
            except ImportError:
                self.logger.warning("TensorBoard not available")

        if use_wandb:
            try:
                import wandb

                self.wandb_run = wandb.init(project=wandb_project or "genesis")
                self.logger.info("Weights & Biases logging enabled")
            except ImportError:
                self.logger.warning("wandb not available")

    def log_metrics(
        self,
        metrics: dict[str, float],
        step: int,
        prefix: str = "",
    ) -> None:
        """
        Log training metrics.

        Args:
            metrics: Dictionary of metric names and values
            step: Training step
            prefix: Optional metric name prefix
        """
        # Log to console
        metrics_str = " | ".join(f"{k}={v:.4f}" for k, v in metrics.items())
        self.logger.info(f"Step {step}: {metrics_str}")

        # Log to TensorBoard
        if self.tb_writer:
            for name, value in metrics.items():
                tag = f"{prefix}/{name}" if prefix else name
                self.tb_writer.add_scalar(tag, value, step)

        # Log to W&B
        if self.wandb_run:
            import wandb

            prefixed_metrics = {
                f"{prefix}/{k}" if prefix else k: v for k, v in metrics.items()
            }
            wandb.log(prefixed_metrics, step=step)

    def log_histogram(
        self,
        name: str,
        values: "torch.Tensor",
        step: int,
    ) -> None:
        """Log histogram to TensorBoard."""
        if self.tb_writer:
            self.tb_writer.add_histogram(name, values, step)

    def log_text(self, name: str, text: str, step: int) -> None:
        """Log text to TensorBoard."""
        if self.tb_writer:
            self.tb_writer.add_text(name, text, step)

    def close(self) -> None:
        """Close logging resources."""
        if self.tb_writer:
            self.tb_writer.close()
        if self.wandb_run:
            import wandb

            wandb.finish()


class ProgressLogger:
    """
    Simple progress logger for evolutionary algorithms.
    """

    def __init__(self, total_generations: int, log_interval: int = 1):
        """
        Initialize progress logger.

        Args:
            total_generations: Total number of generations
            log_interval: Log every N generations
        """
        self.total_generations = total_generations
        self.log_interval = log_interval
        self.logger = get_logger("genesis.evolution")
        self.history: list[dict] = []

    def log_generation(
        self,
        generation: int,
        best_fitness: float,
        avg_fitness: float,
        diversity: Optional[float] = None,
        **kwargs,
    ) -> None:
        """
        Log generation statistics.

        Args:
            generation: Current generation
            best_fitness: Best fitness in population
            avg_fitness: Average fitness
            diversity: Population diversity
            **kwargs: Additional metrics
        """
        entry = {
            "generation": generation,
            "best_fitness": best_fitness,
            "avg_fitness": avg_fitness,
            "diversity": diversity,
            **kwargs,
        }
        self.history.append(entry)

        if generation % self.log_interval == 0:
            progress = generation / self.total_generations * 100
            msg = (
                f"Gen {generation}/{self.total_generations} ({progress:.1f}%) | "
                f"Best: {best_fitness:.4f} | Avg: {avg_fitness:.4f}"
            )
            if diversity is not None:
                msg += f" | Div: {diversity:.4f}"
            self.logger.info(msg)

    def get_history(self) -> list[dict]:
        """Get logged history."""
        return self.history
```

<a id="genesis-utils-metrics-py"></a>

#### `genesis/utils/metrics.py`
*11002 bytes Â· ~2,750 tokens*

```python
"""Evaluation metrics for Genesis experiments."""

from typing import Any, Optional
import torch
import numpy as np
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)


def compute_perplexity(
    loss: torch.Tensor,
    base: float = np.e,
) -> float:
    """
    Compute perplexity from loss.

    Args:
        loss: Cross-entropy loss
        base: Base for exponential (e for natural, 2 for bits)

    Returns:
        Perplexity value
    """
    if isinstance(loss, torch.Tensor):
        loss = loss.item()
    return base ** loss


def compute_accuracy(
    predictions: torch.Tensor,
    labels: torch.Tensor,
    ignore_index: int = -100,
) -> float:
    """
    Compute accuracy between predictions and labels.

    Args:
        predictions: Predicted class indices
        labels: Ground truth labels
        ignore_index: Index to ignore in calculation

    Returns:
        Accuracy (0-1)
    """
    mask = labels != ignore_index
    if mask.sum() == 0:
        return 0.0

    correct = (predictions[mask] == labels[mask]).sum()
    total = mask.sum()

    return (correct / total).item()


def compute_bleu(
    predictions: list[str],
    references: list[str],
    max_n: int = 4,
    smooth: bool = True,
) -> dict[str, float]:
    """
    Compute BLEU score.

    Args:
        predictions: List of predicted strings
        references: List of reference strings
        max_n: Maximum n-gram size
        smooth: Whether to apply smoothing

    Returns:
        Dictionary with BLEU scores
    """
    from collections import Counter

    def get_ngrams(tokens: list[str], n: int) -> Counter:
        return Counter(tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1))

    total_matches = defaultdict(int)
    total_counts = defaultdict(int)
    total_ref_length = 0
    total_pred_length = 0

    for pred, ref in zip(predictions, references):
        pred_tokens = pred.lower().split()
        ref_tokens = ref.lower().split()

        total_ref_length += len(ref_tokens)
        total_pred_length += len(pred_tokens)

        for n in range(1, max_n + 1):
            pred_ngrams = get_ngrams(pred_tokens, n)
            ref_ngrams = get_ngrams(ref_tokens, n)

            for ngram, count in pred_ngrams.items():
                total_matches[n] += min(count, ref_ngrams.get(ngram, 0))
                total_counts[n] += count

    # Compute precision for each n
    precisions = []
    for n in range(1, max_n + 1):
        if total_counts[n] == 0:
            precision = 0.0
        else:
            precision = total_matches[n] / total_counts[n]
            if smooth and precision == 0:
                precision = 1 / (total_counts[n] + 1)
        precisions.append(precision)

    # Compute brevity penalty
    if total_pred_length == 0:
        bp = 0.0
    elif total_pred_length >= total_ref_length:
        bp = 1.0
    else:
        bp = np.exp(1 - total_ref_length / total_pred_length)

    # Compute BLEU scores
    results = {}
    for n in range(1, max_n + 1):
        if any(p == 0 for p in precisions[:n]):
            bleu_n = 0.0
        else:
            log_precision = sum(np.log(p) for p in precisions[:n]) / n
            bleu_n = bp * np.exp(log_precision)
        results[f"bleu_{n}"] = bleu_n

    results["brevity_penalty"] = bp

    return results


def compute_rouge(
    predictions: list[str],
    references: list[str],
) -> dict[str, float]:
    """
    Compute ROUGE scores.

    Args:
        predictions: List of predicted strings
        references: List of reference strings

    Returns:
        Dictionary with ROUGE scores
    """
    from collections import Counter

    def get_lcs_length(a: list[str], b: list[str]) -> int:
        """Compute longest common subsequence length."""
        m, n = len(a), len(b)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if a[i - 1] == b[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1] + 1
                else:
                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
        return dp[m][n]

    rouge_1_f1 = []
    rouge_2_f1 = []
    rouge_l_f1 = []

    for pred, ref in zip(predictions, references):
        pred_tokens = pred.lower().split()
        ref_tokens = ref.lower().split()

        # ROUGE-1 (unigrams)
        pred_unigrams = Counter(pred_tokens)
        ref_unigrams = Counter(ref_tokens)
        overlap = sum((pred_unigrams & ref_unigrams).values())

        if len(pred_tokens) > 0 and len(ref_tokens) > 0:
            precision = overlap / len(pred_tokens)
            recall = overlap / len(ref_tokens)
            f1 = 2 * precision * recall / (precision + recall + 1e-8)
        else:
            f1 = 0.0
        rouge_1_f1.append(f1)

        # ROUGE-2 (bigrams)
        pred_bigrams = Counter(
            tuple(pred_tokens[i : i + 2]) for i in range(len(pred_tokens) - 1)
        )
        ref_bigrams = Counter(
            tuple(ref_tokens[i : i + 2]) for i in range(len(ref_tokens) - 1)
        )
        overlap = sum((pred_bigrams & ref_bigrams).values())

        if len(pred_bigrams) > 0 and len(ref_bigrams) > 0:
            precision = overlap / len(pred_bigrams)
            recall = overlap / len(ref_bigrams)
            f1 = 2 * precision * recall / (precision + recall + 1e-8)
        else:
            f1 = 0.0
        rouge_2_f1.append(f1)

        # ROUGE-L (LCS)
        lcs_len = get_lcs_length(pred_tokens, ref_tokens)
        if len(pred_tokens) > 0 and len(ref_tokens) > 0:
            precision = lcs_len / len(pred_tokens)
            recall = lcs_len / len(ref_tokens)
            f1 = 2 * precision * recall / (precision + recall + 1e-8)
        else:
            f1 = 0.0
        rouge_l_f1.append(f1)

    return {
        "rouge_1": np.mean(rouge_1_f1),
        "rouge_2": np.mean(rouge_2_f1),
        "rouge_l": np.mean(rouge_l_f1),
    }


class MetricsTracker:
    """
    Track and aggregate metrics during training/evaluation.
    """

    def __init__(self, window_size: int = 100):
        """
        Initialize metrics tracker.

        Args:
            window_size: Window size for moving averages
        """
        self.window_size = window_size
        self._metrics: dict[str, list[float]] = defaultdict(list)
        self._step = 0

    def update(self, metrics: dict[str, float]) -> None:
        """
        Update with new metric values.

        Args:
            metrics: Dictionary of metric names and values
        """
        for name, value in metrics.items():
            if isinstance(value, torch.Tensor):
                value = value.item()
            self._metrics[name].append(value)

        self._step += 1

    def get_average(self, name: str, window: Optional[int] = None) -> float:
        """
        Get average of a metric.

        Args:
            name: Metric name
            window: Optional window size (uses all values if None)

        Returns:
            Average value
        """
        if name not in self._metrics:
            return 0.0

        values = self._metrics[name]
        if window:
            values = values[-window:]

        return np.mean(values) if values else 0.0

    def get_latest(self, name: str) -> float:
        """Get most recent value of a metric."""
        if name not in self._metrics or not self._metrics[name]:
            return 0.0
        return self._metrics[name][-1]

    def get_all(self, name: str) -> list[float]:
        """Get all values of a metric."""
        return self._metrics.get(name, []).copy()

    def get_summary(self) -> dict[str, dict[str, float]]:
        """
        Get summary statistics for all metrics.

        Returns:
            Dictionary with stats for each metric
        """
        summary = {}
        for name, values in self._metrics.items():
            if values:
                summary[name] = {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "min": np.min(values),
                    "max": np.max(values),
                    "latest": values[-1],
                    "count": len(values),
                }
        return summary

    def reset(self) -> None:
        """Reset all metrics."""
        self._metrics.clear()
        self._step = 0

    @property
    def step(self) -> int:
        """Current step count."""
        return self._step


class FitnessTracker:
    """
    Track fitness statistics for evolutionary algorithms.
    """

    def __init__(self):
        """Initialize fitness tracker."""
        self.best_fitness_history: list[float] = []
        self.avg_fitness_history: list[float] = []
        self.diversity_history: list[float] = []
        self.generation_history: list[int] = []

    def update(
        self,
        generation: int,
        best_fitness: float,
        avg_fitness: float,
        diversity: Optional[float] = None,
    ) -> None:
        """
        Update with generation statistics.

        Args:
            generation: Generation number
            best_fitness: Best fitness in population
            avg_fitness: Average fitness
            diversity: Population diversity
        """
        self.generation_history.append(generation)
        self.best_fitness_history.append(best_fitness)
        self.avg_fitness_history.append(avg_fitness)
        if diversity is not None:
            self.diversity_history.append(diversity)

    def get_improvement(self, window: int = 10) -> float:
        """
        Get fitness improvement over window.

        Args:
            window: Number of generations to consider

        Returns:
            Improvement (positive = improving)
        """
        if len(self.best_fitness_history) < window:
            return 0.0

        recent = self.best_fitness_history[-window:]
        return recent[-1] - recent[0]

    def is_converged(
        self,
        threshold: float = 1e-4,
        window: int = 10,
    ) -> bool:
        """
        Check if evolution has converged.

        Args:
            threshold: Improvement threshold
            window: Window size to check

        Returns:
            True if converged
        """
        if len(self.best_fitness_history) < window:
            return False

        improvement = abs(self.get_improvement(window))
        return improvement < threshold

    def get_summary(self) -> dict[str, Any]:
        """Get summary of fitness tracking."""
        return {
            "generations": len(self.generation_history),
            "best_fitness": max(self.best_fitness_history) if self.best_fitness_history else 0,
            "final_best": self.best_fitness_history[-1] if self.best_fitness_history else 0,
            "final_avg": self.avg_fitness_history[-1] if self.avg_fitness_history else 0,
            "improvement": self.get_improvement(),
            "converged": self.is_converged(),
        }
```


### ğŸ”© Config Internals

<a id="experiments-llm_medical-config-yaml"></a>

#### `experiments/llm_medical/config.yaml`
*1685 bytes Â· ~421 tokens*

```yaml
# Genesis Medical LLM Experiment Configuration

project_name: "medical_llm_evolution"
output_dir: "./outputs/medical_llm"
checkpoint_dir: "./checkpoints/medical_llm"
log_dir: "./logs/medical_llm"
seed: 42

# Model settings
teacher_model: "meta-llama/Llama-2-7b-hf"
student_model: null  # Derived from teacher
use_lora: true

# Hardware settings
teacher_device: "cuda:0"
student_device: "cuda:1"
mixed_precision: "fp16"

# Genetic algorithm settings
genetic:
  population_size: 20
  generations: 50
  mutation_rate: 0.1
  crossover_rate: 0.7
  elite_size: 2
  tournament_size: 3
  slerp_ratio: 0.5
  mutation_scale: 0.01
  mutation_prob_per_weight: 0.1
  adaptive_mutation: true
  mutation_decay: 0.95
  min_mutation_rate: 0.01

# Knowledge distillation settings
distillation:
  temperature: 4.0
  alpha: 0.5
  learning_rate: 2.0e-5
  warmup_steps: 100
  max_steps: 1000
  batch_size: 8
  gradient_accumulation_steps: 4
  use_feature_distillation: false
  feature_layers: [-1, -2, -3]
  feature_weight: 0.1

# Pruning settings
pruning:
  target_sparsity: 0.3
  pruning_method: "magnitude"
  structured: false
  granularity: "element"
  iterative_steps: 3
  initial_sparsity: 0.0
  final_sparsity: 0.3
  pruning_schedule: "cubic"

# LoRA settings
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Experiment settings
experiment_type: "llm"
dataset_name: "pubmed_qa"
max_samples: 5000
eval_samples: 500

# Logging
use_tensorboard: true
use_wandb: false
wandb_project: "genesis-medical"
log_every_n_steps: 10
eval_every_n_generations: 5
save_every_n_generations: 10
```

<a id="experiments-tts_voice-config-yaml"></a>

#### `experiments/tts_voice/config.yaml`
*1307 bytes Â· ~326 tokens*

```yaml
# Genesis TTS Voice Evolution Experiment Configuration

project_name: "tts_voice_evolution"
output_dir: "./outputs/tts_voice"
checkpoint_dir: "./checkpoints/tts_voice"
log_dir: "./logs/tts_voice"
seed: 42

# TTS Model settings
tts:
  model_type: "tacotron2"  # tacotron2, fastspeech2, vits
  style_dim: 128
  speaker_dim: 256
  num_speakers: 1
  sample_rate: 22050
  n_mel_channels: 80
  hop_length: 256
  win_length: 1024
  n_fft: 1024

# Hardware settings
device: "cuda:0"
mixed_precision: "fp16"

# Genetic algorithm settings for style evolution
genetic:
  population_size: 30
  generations: 100
  mutation_rate: 0.15
  crossover_rate: 0.7
  elite_size: 3
  tournament_size: 3
  slerp_ratio: 0.5
  mutation_scale: 0.02
  adaptive_mutation: true
  mutation_decay: 0.98
  min_mutation_rate: 0.02

# Style evolution settings
style_evolution:
  num_style_tokens: 10
  perturbation_scale: 0.1

# Fitness evaluation settings
fitness:
  target_mcd: 5.0  # Target MCD value
  weight_naturalness: 0.4
  weight_similarity: 0.6
  use_f0_rmse: true
  use_vuv_error: true

# Data settings
data:
  data_dir: "./data/tts"
  max_samples: 1000
  eval_samples: 100

# Logging
use_tensorboard: true
use_wandb: false
wandb_project: "genesis-tts"
log_every_n_steps: 5
eval_every_n_generations: 5
save_every_n_generations: 10
```

<a id="genesis-config-__init__-py"></a>

#### `genesis/config/__init__.py`
*310 bytes Â· ~77 tokens*

```python
"""Configuration management for Genesis."""

from genesis.config.settings import GenesisConfig, GeneticConfig, DistillationConfig
from genesis.config.hardware import HardwareConfig, GPUInfo

__all__ = [
    "GenesisConfig",
    "GeneticConfig",
    "DistillationConfig",
    "HardwareConfig",
    "GPUInfo",
]
```

<a id="genesis-config-hardware-py"></a>

#### `genesis/config/hardware.py`
*8374 bytes Â· ~2,093 tokens*

```python
"""Hardware detection and GPU configuration for Genesis."""

from dataclasses import dataclass
from typing import Optional
import logging

logger = logging.getLogger(__name__)


@dataclass
class GPUInfo:
    """Information about a single GPU."""

    index: int
    name: str
    total_memory: int  # in bytes
    free_memory: int
    compute_capability: tuple[int, int]

    @property
    def total_memory_gb(self) -> float:
        """Return total memory in GB."""
        return self.total_memory / (1024**3)

    @property
    def free_memory_gb(self) -> float:
        """Return free memory in GB."""
        return self.free_memory / (1024**3)

    def __str__(self) -> str:
        return (
            f"GPU {self.index}: {self.name} "
            f"({self.free_memory_gb:.1f}/{self.total_memory_gb:.1f} GB free)"
        )


class HardwareConfig:
    """Hardware configuration and GPU management."""

    def __init__(
        self,
        teacher_device: str = "cuda:0",
        student_device: str = "cuda:1",
        auto_detect: bool = True,
    ):
        self.teacher_device = teacher_device
        self.student_device = student_device
        self._gpus: list[GPUInfo] = []
        self._cuda_available = False
        self._initialized = False

        if auto_detect:
            self._detect_hardware()

    def _detect_hardware(self) -> None:
        """Detect available hardware."""
        try:
            import torch

            self._cuda_available = torch.cuda.is_available()

            if self._cuda_available:
                num_gpus = torch.cuda.device_count()
                for i in range(num_gpus):
                    props = torch.cuda.get_device_properties(i)
                    free_mem, total_mem = torch.cuda.mem_get_info(i)
                    gpu_info = GPUInfo(
                        index=i,
                        name=props.name,
                        total_memory=total_mem,
                        free_memory=free_mem,
                        compute_capability=(props.major, props.minor),
                    )
                    self._gpus.append(gpu_info)
                    logger.info(f"Detected: {gpu_info}")

                # Validate device assignments
                self._validate_devices()
            else:
                logger.warning("CUDA not available. Using CPU.")
                self.teacher_device = "cpu"
                self.student_device = "cpu"

            self._initialized = True

        except ImportError:
            logger.error("PyTorch not installed. Cannot detect hardware.")
            self._cuda_available = False

    def _validate_devices(self) -> None:
        """Validate that configured devices are available."""
        num_gpus = len(self._gpus)

        def parse_device(device: str) -> Optional[int]:
            if device == "cpu":
                return None
            if device.startswith("cuda:"):
                return int(device.split(":")[1])
            if device == "cuda":
                return 0
            return None

        teacher_idx = parse_device(self.teacher_device)
        student_idx = parse_device(self.student_device)

        if teacher_idx is not None and teacher_idx >= num_gpus:
            logger.warning(
                f"Teacher device {self.teacher_device} not available. "
                f"Only {num_gpus} GPU(s) detected. Using cuda:0."
            )
            self.teacher_device = "cuda:0" if num_gpus > 0 else "cpu"

        if student_idx is not None and student_idx >= num_gpus:
            logger.warning(
                f"Student device {self.student_device} not available. "
                f"Only {num_gpus} GPU(s) detected."
            )
            # Fall back to same device as teacher or CPU
            if num_gpus >= 2:
                self.student_device = "cuda:1"
            elif num_gpus == 1:
                self.student_device = "cuda:0"
            else:
                self.student_device = "cpu"

    @property
    def cuda_available(self) -> bool:
        """Check if CUDA is available."""
        return self._cuda_available

    @property
    def num_gpus(self) -> int:
        """Return number of available GPUs."""
        return len(self._gpus)

    @property
    def gpus(self) -> list[GPUInfo]:
        """Return list of GPU information."""
        return self._gpus

    @property
    def dual_gpu_available(self) -> bool:
        """Check if dual GPU setup is available."""
        return self.num_gpus >= 2

    def get_gpu(self, index: int) -> Optional[GPUInfo]:
        """Get GPU info by index."""
        if 0 <= index < len(self._gpus):
            return self._gpus[index]
        return None

    def get_optimal_batch_size(self, model_memory_gb: float, device: str) -> int:
        """Estimate optimal batch size based on available memory."""
        if device == "cpu":
            return 1

        device_idx = int(device.split(":")[1]) if ":" in device else 0
        gpu = self.get_gpu(device_idx)

        if gpu is None:
            return 1

        # Reserve some memory for PyTorch overhead
        available_memory = gpu.free_memory_gb * 0.8
        memory_per_sample = model_memory_gb * 0.1  # Rough estimate

        batch_size = max(1, int(available_memory / memory_per_sample))
        return min(batch_size, 32)  # Cap at 32

    def memory_summary(self) -> str:
        """Return a summary of GPU memory usage."""
        lines = ["GPU Memory Summary:"]
        for gpu in self._gpus:
            lines.append(f"  {gpu}")
        return "\n".join(lines)

    def optimize_device_assignment(self, teacher_memory_gb: float, student_memory_gb: float) -> None:
        """Optimize device assignment based on model sizes and GPU memory."""
        if not self.dual_gpu_available:
            logger.info("Single GPU or CPU mode. Both models on same device.")
            return

        gpu0 = self._gpus[0]
        gpu1 = self._gpus[1]

        # Assign larger model to GPU with more memory
        if teacher_memory_gb > student_memory_gb:
            if gpu0.free_memory_gb >= gpu1.free_memory_gb:
                self.teacher_device = "cuda:0"
                self.student_device = "cuda:1"
            else:
                self.teacher_device = "cuda:1"
                self.student_device = "cuda:0"
        else:
            if gpu0.free_memory_gb >= gpu1.free_memory_gb:
                self.teacher_device = "cuda:1"
                self.student_device = "cuda:0"
            else:
                self.teacher_device = "cuda:0"
                self.student_device = "cuda:1"

        logger.info(f"Optimized device assignment: Teacher={self.teacher_device}, Student={self.student_device}")

    def to_dict(self) -> dict:
        """Convert configuration to dictionary."""
        return {
            "teacher_device": self.teacher_device,
            "student_device": self.student_device,
            "cuda_available": self._cuda_available,
            "num_gpus": self.num_gpus,
            "gpus": [
                {
                    "index": gpu.index,
                    "name": gpu.name,
                    "total_memory_gb": gpu.total_memory_gb,
                    "free_memory_gb": gpu.free_memory_gb,
                    "compute_capability": gpu.compute_capability,
                }
                for gpu in self._gpus
            ],
        }


def get_device(device_str: str) -> "torch.device":
    """Get torch device from string."""
    import torch

    return torch.device(device_str)


def empty_cache(device: Optional[str] = None) -> None:
    """Empty CUDA cache for specified device or all devices."""
    import torch

    if torch.cuda.is_available():
        if device is not None and device.startswith("cuda"):
            device_idx = int(device.split(":")[1]) if ":" in device else 0
            with torch.cuda.device(device_idx):
                torch.cuda.empty_cache()
        else:
            torch.cuda.empty_cache()


def synchronize(device: Optional[str] = None) -> None:
    """Synchronize CUDA operations."""
    import torch

    if torch.cuda.is_available():
        if device is not None and device.startswith("cuda"):
            device_idx = int(device.split(":")[1]) if ":" in device else 0
            torch.cuda.synchronize(device_idx)
        else:
            torch.cuda.synchronize()
```

<a id="genesis-config-settings-py"></a>

#### `genesis/config/settings.py`
*8655 bytes Â· ~2,163 tokens*

```python
"""Global settings and configuration for Genesis."""

from dataclasses import dataclass, field
from typing import Any, Optional
from pathlib import Path
import yaml


@dataclass
class GeneticConfig:
    """Configuration for genetic algorithm parameters."""

    population_size: int = 20
    generations: int = 50
    mutation_rate: float = 0.1
    crossover_rate: float = 0.7
    elite_size: int = 2
    tournament_size: int = 3
    slerp_ratio: float = 0.5

    # Mutation parameters
    mutation_scale: float = 0.01
    mutation_prob_per_weight: float = 0.1

    # Adaptive mutation
    adaptive_mutation: bool = True
    mutation_decay: float = 0.95
    min_mutation_rate: float = 0.01


@dataclass
class DistillationConfig:
    """Configuration for knowledge distillation."""

    temperature: float = 4.0
    alpha: float = 0.5  # Weight for distillation loss vs hard label loss
    learning_rate: float = 2e-5
    warmup_steps: int = 100
    max_steps: int = 1000
    batch_size: int = 8
    gradient_accumulation_steps: int = 4

    # Feature distillation
    use_feature_distillation: bool = False
    feature_layers: list = field(default_factory=lambda: [-1, -2, -3])
    feature_weight: float = 0.1


@dataclass
class PruningConfig:
    """Configuration for model pruning."""

    target_sparsity: float = 0.3
    pruning_method: str = "magnitude"  # magnitude, gradient, taylor
    structured: bool = False
    granularity: str = "element"  # element, row, column, block
    block_size: int = 4
    iterative_steps: int = 1

    # Gradual pruning
    initial_sparsity: float = 0.0
    final_sparsity: float = 0.3
    pruning_schedule: str = "cubic"  # linear, cubic, exponential

    # Layer-wise settings
    skip_layers: list = field(default_factory=list)
    layer_sparsity_overrides: dict = field(default_factory=dict)


@dataclass
class LoRAConfig:
    """Configuration for LoRA adapters."""

    r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    target_modules: list = field(default_factory=lambda: ["q_proj", "v_proj"])
    bias: str = "none"
    task_type: str = "CAUSAL_LM"
    modules_to_save: Optional[list] = None


@dataclass
class GenesisConfig:
    """Main configuration class for Genesis."""

    # Project settings
    project_name: str = "genesis_experiment"
    output_dir: str = "./outputs"
    checkpoint_dir: str = "./checkpoints"
    log_dir: str = "./logs"
    seed: int = 42

    # Model settings
    teacher_model: str = "meta-llama/Llama-2-7b-hf"
    student_model: Optional[str] = None  # If None, derived from teacher
    use_lora: bool = True

    # Hardware settings
    teacher_device: str = "cuda:0"
    student_device: str = "cuda:1"
    mixed_precision: str = "fp16"  # fp16, bf16, fp32

    # Sub-configurations
    genetic: GeneticConfig = field(default_factory=GeneticConfig)
    distillation: DistillationConfig = field(default_factory=DistillationConfig)
    pruning: PruningConfig = field(default_factory=PruningConfig)
    lora: LoRAConfig = field(default_factory=LoRAConfig)

    # Experiment settings
    experiment_type: str = "llm"  # llm, tts
    dataset_name: str = "pubmed_qa"
    max_samples: Optional[int] = None
    eval_samples: int = 100

    # Logging
    use_tensorboard: bool = True
    use_wandb: bool = False
    wandb_project: str = "genesis"
    log_every_n_steps: int = 10
    eval_every_n_generations: int = 5
    save_every_n_generations: int = 10

    def __post_init__(self):
        """Validate and set up configuration."""
        # Create directories
        for dir_path in [self.output_dir, self.checkpoint_dir, self.log_dir]:
            Path(dir_path).mkdir(parents=True, exist_ok=True)

    @classmethod
    def from_yaml(cls, path: str) -> "GenesisConfig":
        """Load configuration from YAML file."""
        with open(path, "r") as f:
            config_dict = yaml.safe_load(f)
        return cls.from_dict(config_dict)

    @classmethod
    def from_dict(cls, config_dict: dict[str, Any]) -> "GenesisConfig":
        """Create configuration from dictionary."""
        # Extract sub-configs
        genetic_dict = config_dict.pop("genetic", {})
        distillation_dict = config_dict.pop("distillation", {})
        pruning_dict = config_dict.pop("pruning", {})
        lora_dict = config_dict.pop("lora", {})

        return cls(
            **config_dict,
            genetic=GeneticConfig(**genetic_dict),
            distillation=DistillationConfig(**distillation_dict),
            pruning=PruningConfig(**pruning_dict),
            lora=LoRAConfig(**lora_dict),
        )

    def to_yaml(self, path: str) -> None:
        """Save configuration to YAML file."""
        config_dict = self.to_dict()
        with open(path, "w") as f:
            yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)

    def to_dict(self) -> dict[str, Any]:
        """Convert configuration to dictionary."""
        return {
            "project_name": self.project_name,
            "output_dir": self.output_dir,
            "checkpoint_dir": self.checkpoint_dir,
            "log_dir": self.log_dir,
            "seed": self.seed,
            "teacher_model": self.teacher_model,
            "student_model": self.student_model,
            "use_lora": self.use_lora,
            "teacher_device": self.teacher_device,
            "student_device": self.student_device,
            "mixed_precision": self.mixed_precision,
            "experiment_type": self.experiment_type,
            "dataset_name": self.dataset_name,
            "max_samples": self.max_samples,
            "eval_samples": self.eval_samples,
            "use_tensorboard": self.use_tensorboard,
            "use_wandb": self.use_wandb,
            "wandb_project": self.wandb_project,
            "log_every_n_steps": self.log_every_n_steps,
            "eval_every_n_generations": self.eval_every_n_generations,
            "save_every_n_generations": self.save_every_n_generations,
            "genetic": {
                "population_size": self.genetic.population_size,
                "generations": self.genetic.generations,
                "mutation_rate": self.genetic.mutation_rate,
                "crossover_rate": self.genetic.crossover_rate,
                "elite_size": self.genetic.elite_size,
                "tournament_size": self.genetic.tournament_size,
                "slerp_ratio": self.genetic.slerp_ratio,
                "mutation_scale": self.genetic.mutation_scale,
                "mutation_prob_per_weight": self.genetic.mutation_prob_per_weight,
                "adaptive_mutation": self.genetic.adaptive_mutation,
                "mutation_decay": self.genetic.mutation_decay,
                "min_mutation_rate": self.genetic.min_mutation_rate,
            },
            "distillation": {
                "temperature": self.distillation.temperature,
                "alpha": self.distillation.alpha,
                "learning_rate": self.distillation.learning_rate,
                "warmup_steps": self.distillation.warmup_steps,
                "max_steps": self.distillation.max_steps,
                "batch_size": self.distillation.batch_size,
                "gradient_accumulation_steps": self.distillation.gradient_accumulation_steps,
                "use_feature_distillation": self.distillation.use_feature_distillation,
                "feature_layers": self.distillation.feature_layers,
                "feature_weight": self.distillation.feature_weight,
            },
            "pruning": {
                "target_sparsity": self.pruning.target_sparsity,
                "pruning_method": self.pruning.pruning_method,
                "structured": self.pruning.structured,
                "granularity": self.pruning.granularity,
                "block_size": self.pruning.block_size,
                "iterative_steps": self.pruning.iterative_steps,
                "initial_sparsity": self.pruning.initial_sparsity,
                "final_sparsity": self.pruning.final_sparsity,
                "pruning_schedule": self.pruning.pruning_schedule,
                "skip_layers": self.pruning.skip_layers,
                "layer_sparsity_overrides": self.pruning.layer_sparsity_overrides,
            },
            "lora": {
                "r": self.lora.r,
                "lora_alpha": self.lora.lora_alpha,
                "lora_dropout": self.lora.lora_dropout,
                "target_modules": self.lora.target_modules,
                "bias": self.lora.bias,
                "task_type": self.lora.task_type,
                "modules_to_save": self.lora.modules_to_save,
            },
        }
```


### ğŸ“œ Scripts & Migrations

<a id="scripts-download_models-py"></a>

#### `scripts/download_models.py`
*6506 bytes Â· ~1,626 tokens*

```python
#!/usr/bin/env python3
"""
Model Download Helper for Genesis

Downloads and caches popular models for experiments.
"""

import argparse
import os
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)


# Available models for download
AVAILABLE_MODELS = {
    # LLM Models
    "gpt2": "gpt2",
    "gpt2-medium": "gpt2-medium",
    "gpt2-large": "gpt2-large",
    "llama-2-7b": "meta-llama/Llama-2-7b-hf",
    "llama-2-13b": "meta-llama/Llama-2-13b-hf",
    "mistral-7b": "mistralai/Mistral-7B-v0.1",
    "phi-2": "microsoft/phi-2",
    "tinyllama": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",

    # Qwen3 Models
    "qwen3-1.7b": "Qwen/Qwen3-1.7B",
    "qwen3-4b": "Qwen/Qwen3-4B",
    "qwen3-8b": "Qwen/Qwen3-8B",
    "qwen3-14b": "Qwen/Qwen3-14B",
    "qwen3-32b": "Qwen/Qwen3-32B",

    # TTS Models (placeholders)
    "tacotron2": "nvidia/tacotron2",
}


def download_model(model_name: str, cache_dir: str = None) -> bool:
    """
    Download a model from HuggingFace.

    Args:
        model_name: Model name or key from AVAILABLE_MODELS
        cache_dir: Directory to cache model

    Returns:
        True if successful, False otherwise
    """
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
    except ImportError:
        logger.error("transformers not installed. Run: pip install transformers")
        return False

    # Resolve model name
    if model_name in AVAILABLE_MODELS:
        model_path = AVAILABLE_MODELS[model_name]
    else:
        model_path = model_name

    logger.info(f"Downloading model: {model_path}")

    try:
        # Download tokenizer
        logger.info("  Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            cache_dir=cache_dir,
            trust_remote_code=True,
        )

        # Download model
        logger.info("  Downloading model weights...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            cache_dir=cache_dir,
            trust_remote_code=True,
            torch_dtype="auto",
            low_cpu_mem_usage=True,
        )

        logger.info(f"  Model downloaded successfully!")
        logger.info(f"  Parameters: {sum(p.numel() for p in model.parameters()):,}")

        # Clean up
        del model
        del tokenizer

        return True

    except Exception as e:
        logger.error(f"  Failed to download model: {e}")
        return False


def download_dataset(dataset_name: str, cache_dir: str = None) -> bool:
    """
    Download a dataset from HuggingFace.

    Args:
        dataset_name: Dataset name
        cache_dir: Directory to cache dataset

    Returns:
        True if successful, False otherwise
    """
    try:
        from datasets import load_dataset
    except ImportError:
        logger.error("datasets not installed. Run: pip install datasets")
        return False

    logger.info(f"Downloading dataset: {dataset_name}")

    try:
        dataset = load_dataset(dataset_name, cache_dir=cache_dir)
        logger.info(f"  Dataset downloaded successfully!")
        logger.info(f"  Splits: {list(dataset.keys())}")
        return True

    except Exception as e:
        logger.error(f"  Failed to download dataset: {e}")
        return False


def list_available_models():
    """Print available models."""
    print("\nAvailable Models:")
    print("-" * 50)
    for key, path in AVAILABLE_MODELS.items():
        print(f"  {key:20} -> {path}")
    print()


def check_huggingface_token():
    """Check if HuggingFace token is configured."""
    token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN")

    if token:
        logger.info("HuggingFace token found in environment")
        return True

    # Check for token file (both old and new HF CLI locations)
    for token_file in [
        Path.home() / ".cache" / "huggingface" / "token",
        Path.home() / ".huggingface" / "token",
    ]:
        if token_file.exists():
            logger.info(f"HuggingFace token found in {token_file}")
            return True

    logger.warning("No HuggingFace token found.")
    logger.warning("Some models (like Llama) require authentication.")
    logger.warning("Run: huggingface-cli login")
    return False


def main():
    parser = argparse.ArgumentParser(
        description="Download models for Genesis experiments"
    )
    parser.add_argument(
        "models",
        nargs="*",
        help="Model names to download (use --list to see available)",
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="List available models",
    )
    parser.add_argument(
        "--cache-dir",
        type=str,
        default=None,
        help="Cache directory for downloads",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        action="append",
        help="Dataset to download (can be specified multiple times)",
    )
    parser.add_argument(
        "--all-small",
        action="store_true",
        help="Download all small models (gpt2, tinyllama, phi-2)",
    )
    args = parser.parse_args()

    if args.list:
        list_available_models()
        return

    # Check token
    check_huggingface_token()

    # Collect models to download
    models_to_download = list(args.models or [])

    if args.all_small:
        models_to_download.extend(["gpt2", "tinyllama", "phi-2"])

    if not models_to_download and not args.dataset:
        parser.print_help()
        print("\nExamples:")
        print("  python download_models.py gpt2")
        print("  python download_models.py --all-small")
        print("  python download_models.py --dataset pubmed_qa")
        print("  python download_models.py --list")
        return

    # Download models
    success_count = 0
    fail_count = 0

    for model_name in models_to_download:
        if download_model(model_name, args.cache_dir):
            success_count += 1
        else:
            fail_count += 1

    # Download datasets
    if args.dataset:
        for dataset_name in args.dataset:
            if download_dataset(dataset_name, args.cache_dir):
                success_count += 1
            else:
                fail_count += 1

    # Summary
    print()
    print("=" * 50)
    print(f"Downloads complete: {success_count} succeeded, {fail_count} failed")
    print("=" * 50)


if __name__ == "__main__":
    main()
```

<a id="scripts-setup_environment-sh"></a>

#### `scripts/setup_environment.sh`
*2871 bytes Â· ~717 tokens*

```bash
#!/bin/bash
# Genesis Environment Setup Script

set -e

echo "=========================================="
echo "  Genesis AI Evolution Laboratory Setup"
echo "=========================================="
echo

# Check Python version
python_version=$(python3 --version 2>&1 | awk '{print $2}')
major=$(echo $python_version | cut -d. -f1)
minor=$(echo $python_version | cut -d. -f2)

echo "Python version: $python_version"

if [ "$major" -lt 3 ] || ([ "$major" -eq 3 ] && [ "$minor" -lt 9 ]); then
    echo "Error: Python 3.9+ is required"
    exit 1
fi

# Create virtual environment if it doesn't exist
if [ ! -d "venv" ]; then
    echo "Creating virtual environment..."
    python3 -m venv venv
fi

# Activate virtual environment
echo "Activating virtual environment..."
source venv/bin/activate

# Upgrade pip
echo "Upgrading pip..."
pip install --upgrade pip

# Install PyTorch (detect CUDA)
echo "Detecting CUDA..."
if command -v nvidia-smi &> /dev/null; then
    echo "CUDA detected. Installing PyTorch with CUDA support..."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
else
    echo "CUDA not detected. Installing CPU-only PyTorch..."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
fi

# Install requirements
echo "Installing requirements..."
pip install -r requirements.txt

# Install Genesis in development mode
echo "Installing Genesis..."
pip install -e .

# Install optional dependencies
read -p "Install TTS dependencies (librosa, soundfile)? [y/N] " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    pip install librosa soundfile pyworld
fi

read -p "Install development dependencies (pytest, black, ruff)? [y/N] " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    pip install pytest pytest-cov black ruff mypy
fi

# Create necessary directories
echo "Creating directories..."
mkdir -p outputs checkpoints logs data

# Verify installation
echo
echo "Verifying installation..."
python -c "
import genesis
print(f'Genesis version: {genesis.__version__}')

import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU count: {torch.cuda.device_count()}')
    for i in range(torch.cuda.device_count()):
        print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')

from genesis import EvolutionaryOptimizer, GenesisConfig
print('All imports successful!')
"

echo
echo "=========================================="
echo "  Setup Complete!"
echo "=========================================="
echo
echo "To activate the environment, run:"
echo "  source venv/bin/activate"
echo
echo "To run tests:"
echo "  pytest tests/"
echo
echo "To start an experiment:"
echo "  python experiments/llm_medical/run_evolution.py --dry-run"
```

<a id="scripts-train_qwen3-py"></a>

#### `scripts/train_qwen3.py`
*17021 bytes Â· ~4,195 tokens*

```python
#!/usr/bin/env python
"""End-to-end Qwen3 KD distillation + evolutionary optimization training entry point."""

from __future__ import annotations

import argparse
import logging
import os
import sys
from datetime import datetime
from pathlib import Path

# Ensure project root is on the path
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
from rich.console import Console
from rich.table import Table

console = Console()

STUDENT_PATH = "/media/ttech-main/42A4266DA426639F/Models/Qwen3-1.7B"


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Train Qwen3-1.7B student via KD from Ollama teacher, then evolve."
    )
    parser.add_argument("--steps", type=int, default=20, help="Distillation optimizer steps")
    parser.add_argument("--generations", type=int, default=5, help="Evolutionary generations (0 = skip)")
    parser.add_argument("--pop-size", type=int, default=4, help="Population size for evolution phase")
    parser.add_argument("--seq-len", type=int, default=128, help="Token sequence length")
    parser.add_argument("--batch-size", type=int, default=1, help="Batch size")
    parser.add_argument(
        "--output-dir",
        type=str,
        default=f"./outputs/run_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        help="Output directory for checkpoints, plots, and report",
    )
    parser.add_argument(
        "--ollama-host",
        type=str,
        default=os.environ.get("OLLAMA_HOST", "http://localhost:11434"),
        help="Ollama server base URL",
    )
    parser.add_argument(
        "--ollama-model",
        type=str,
        default=os.environ.get("OLLAMA_MODEL", "qwen3.5"),
        help="Ollama model tag to use as primary teacher",
    )
    parser.add_argument(
        "--fallback-model",
        type=str,
        default=os.environ.get("OLLAMA_FALLBACK_MODEL", None),
        help="Ollama model tag to fall back to when primary returns 429 (e.g. qwen3:30b)",
    )
    parser.add_argument("--elite-size", type=int, default=2, help="Number of elite individuals preserved each generation")
    parser.add_argument("--mutation-scale", type=float, default=0.05, help="Scale of Gaussian mutation noise (higher = more exploration)")
    parser.add_argument("--mutation-rate", type=float, default=0.3, help="Per-generation probability of mutation occurring")
    parser.add_argument("--student-path", type=str, default=STUDENT_PATH, help="Path to student model")
    parser.add_argument("--device", type=str, default="cuda:0", help="Device for student model (distillation)")
    parser.add_argument("--eval-device", type=str, default="cuda:1", help="Device for population fitness evaluation (evolution phase)")
    parser.add_argument("--log-level", type=str, default="INFO", help="Logging level")
    return parser.parse_args()


# ---------------------------------------------------------------------------
# Dataset helpers
# ---------------------------------------------------------------------------

def build_dataloaders(tokenizer, seq_len: int, batch_size: int):
    """Load wikitext-2-raw-v1 and return (train_dataloader, eval_dataloader)."""
    from datasets import load_dataset
    from torch.utils.data import DataLoader, TensorDataset

    console.print("[cyan]Loading wikitext-2-raw-v1 datasetâ€¦[/cyan]")
    raw = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")

    train_texts = [raw[i]["text"] for i in range(min(200, len(raw))) if raw[i]["text"].strip()]
    eval_texts = [raw[i]["text"] for i in range(200, min(250, len(raw))) if raw[i]["text"].strip()]

    def tokenize(texts: list[str]) -> dict:
        enc = tokenizer(
            texts,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=seq_len,
        )
        enc["labels"] = enc["input_ids"].clone()
        # Mask padding positions so they don't contribute to the CE loss
        enc["labels"][enc["attention_mask"] == 0] = -100
        return enc

    console.print(f"[cyan]Tokenizing {len(train_texts)} train + {len(eval_texts)} eval samplesâ€¦[/cyan]")
    train_enc = tokenize(train_texts)
    eval_enc = tokenize(eval_texts)

    train_ds = TensorDataset(train_enc["input_ids"], train_enc["attention_mask"], train_enc["labels"])
    eval_ds = TensorDataset(eval_enc["input_ids"], eval_enc["attention_mask"], eval_enc["labels"])

    def collate(batch):
        ids, masks, labels = zip(*batch)
        return {
            "input_ids": torch.stack(ids),
            "attention_mask": torch.stack(masks),
            "labels": torch.stack(labels),
        }

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)
    eval_loader = DataLoader(eval_ds, batch_size=batch_size, shuffle=False, collate_fn=collate)
    return train_loader, eval_loader


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main() -> None:
    args = parse_args()

    # Logging setup
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper(), logging.INFO),
        format="%(asctime)s | %(levelname)-8s | %(name)s | %(message)s",
        datefmt="%H:%M:%S",
    )
    # Silence noisy libraries
    for noisy in ("transformers", "datasets", "peft", "urllib3", "filelock"):
        logging.getLogger(noisy).setLevel(logging.WARNING)

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    console.rule(f"[bold blue]Genesis Qwen3 Training[/bold blue]")
    console.print(f"Output dir : [green]{output_dir}[/green]")
    fallback_str = f" â†’ [yellow]{args.fallback_model}[/yellow]" if args.fallback_model else ""
    console.print(f"Teacher    : [cyan]{args.ollama_model}[/cyan]{fallback_str} @ {args.ollama_host}")
    console.print(f"Student    : [cyan]{args.student_path}[/cyan] on {args.device}")
    console.print(f"Steps      : {args.steps}  |  Generations: {args.generations}  |  Pop: {args.pop_size}")
    console.print(f"Elite size : {args.elite_size}  |  Mutation scale: {args.mutation_scale}  |  Mutation rate: {args.mutation_rate}")
    console.print(f"Devices    : distillation=[cyan]{args.device}[/cyan]  evolution eval=[cyan]{args.eval_device}[/cyan]")

    # ------------------------------------------------------------------
    # 1. Dashboard
    # ------------------------------------------------------------------
    from genesis.utils.dashboard import TrainingDashboard

    dashboard = TrainingDashboard()
    dashboard.start()

    try:
        _run_training(args, output_dir, dashboard)
    finally:
        dashboard.stop()


def _cpu_diversity(population) -> float:
    """Compute population diversity with all tensors moved to CPU."""
    import numpy as np

    individuals = population.individuals
    if len(individuals) < 2:
        return 0.0

    keys = list(individuals[0].state_dict.keys())
    total_std = 0.0
    count = 0

    for key in keys:
        try:
            params = torch.stack([ind.state_dict[key].cpu().float() for ind in individuals])
            total_std += params.std(dim=0).mean().item()
            count += 1
        except Exception:
            pass

    return total_std / count if count > 0 else 0.0


def _run_training(args, output_dir: Path, dashboard) -> None:
    from genesis.models.ollama_teacher import OllamaTeacher
    from genesis.models.student import StudentModel
    from genesis.models.lora_manager import LoRAConfig
    from genesis.distillation.trainer import DistillationTrainer, TrainingConfig

    # ------------------------------------------------------------------
    # 2. Build teacher
    # ------------------------------------------------------------------
    console.print("\n[bold]Setting up teacher (OllamaTeacher)â€¦[/bold]")
    teacher = OllamaTeacher(
        model_name=args.ollama_model,
        base_url=args.ollama_host,
        tokenizer_path=args.student_path,  # shared Qwen3 tokenizer
        top_logprobs=20,
        fallback_model=args.fallback_model,
    )
    teacher.load()
    fallback_note = f" (fallback: [yellow]{args.fallback_model}[/yellow])" if args.fallback_model else ""
    console.print(f"[green]Teacher ready.[/green]{fallback_note}")

    # ------------------------------------------------------------------
    # 3. Build student
    # ------------------------------------------------------------------
    console.print("[bold]Loading student model (Qwen3-1.7B + LoRA)â€¦[/bold]")
    lora_config = LoRAConfig(r=16, lora_alpha=32)
    student = StudentModel(
        model_name_or_path=args.student_path,
        device=args.device,
        dtype=torch.bfloat16,
        use_lora=True,
        lora_config=lora_config,
    )
    student.load()
    console.print("[green]Student ready.[/green]")

    # ------------------------------------------------------------------
    # 4. Dataset
    # ------------------------------------------------------------------
    train_loader, eval_loader = build_dataloaders(
        student.tokenizer, args.seq_len, args.batch_size
    )

    # ------------------------------------------------------------------
    # 5. Phase 1 â€” KD Distillation
    # ------------------------------------------------------------------
    console.print("\n[bold yellow]Phase 1: Knowledge Distillation[/bold yellow]")

    training_config = TrainingConfig(
        max_steps=args.steps,
        logging_steps=1,
        eval_steps=max(1, args.steps // 5),   # evaluate every ~20% of training
        gradient_accumulation_steps=1,
        mixed_precision="bf16",
        temperature=4.0,
        alpha=0.5,
        output_dir=str(output_dir),
        warmup_steps=max(1, args.steps // 10),
    )

    trainer = DistillationTrainer(
        teacher=teacher,
        student=student,
        train_dataloader=train_loader,
        eval_dataloader=eval_loader,
        config=training_config,
    )

    distill_results = trainer.train(callback=dashboard.update_distillation)

    console.print(
        f"[green]Distillation done. Steps={distill_results['global_step']}, "
        f"Best eval loss={distill_results['best_eval_loss']:.4f}[/green]"
    )

    # ------------------------------------------------------------------
    # 6. Phase 2 â€” Evolutionary Optimization
    # ------------------------------------------------------------------
    best_fitness = 0.0
    generations_done = 0

    if args.generations > 0:
        console.print("\n[bold yellow]Phase 2: Evolutionary Optimization[/bold yellow]")

        from genesis.core.population import Population
        from genesis.core.genetics import Genetics, mutate as _mutate
        from genesis.core.fitness import PerplexityFitness

        # â”€â”€ Dual-GPU evolution setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # cuda:0: keeps the distilled student for saving.
        # cuda:1: eval model â€” base weights fixed, only LoRA swapped per individual.
        #
        # Population holds ONLY LoRA adapter weights on CPU (~25 MB each).
        # This replaces full model state dicts (~3.4 GB each Ã— 8 = 27 GB RAM OOM).
        # For each individual: inject LoRA weights into cuda:1 eval model â†’ evaluate.
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        console.print(f"\n[bold]Loading eval model on {args.eval_device} for population fitnessâ€¦[/bold]")
        eval_student = StudentModel(
            model_name_or_path=args.student_path,
            device=args.eval_device,
            dtype=torch.bfloat16,
            use_lora=True,
            lora_config=lora_config,
        )
        eval_student.load()

        # Seed eval model base with distilled LoRA weights from cuda:0 student
        lora_mgr_train = student.lora_manager
        lora_mgr_eval  = eval_student.lora_manager
        base_lora_cpu  = {k: v.cpu() for k, v in lora_mgr_train.get_lora_state_dict().items()}
        lora_mgr_eval.set_lora_state_dict({k: v.to(args.eval_device) for k, v in base_lora_cpu.items()}, strict=False)
        console.print(f"[green]Eval model ready on {args.eval_device} "
                      f"({len(base_lora_cpu)} LoRA tensors, ~{sum(v.nbytes for v in base_lora_cpu.values())//1024**2} MB).[/green]")

        eval_fitness = PerplexityFitness(dataloader=eval_loader, device=args.eval_device)

        # Seed population with LoRA-only state dicts (CPU)
        seed_states = [base_lora_cpu] + [
            _mutate(base_lora_cpu, mutation_rate=1.0, mutation_scale=args.mutation_scale)
            for _ in range(args.pop_size - 1)
        ]

        genetics = Genetics(
            mutation_rate=args.mutation_rate,
            mutation_scale=args.mutation_scale,
            adaptive_mutation=True,
            mutation_decay=0.98,
            min_mutation_rate=0.05,
        )
        population = Population(size=args.pop_size, genetics=genetics, elite_size=args.elite_size)
        population.initialize_from_state_dicts(seed_states)
        lora_mb = sum(v.nbytes for v in base_lora_cpu.values()) * args.pop_size // 1024**2
        console.print(f"[cyan]Population of {args.pop_size} LoRA-only individuals initialised "
                      f"(total CPU footprint ~{lora_mb} MB).[/cyan]")

        def fitness_fn(lora_state):
            # Inject LoRA weights onto cuda:1 eval model; base weights stay fixed
            lora_mgr_eval.set_lora_state_dict(
                {k: v.to(args.eval_device) for k, v in lora_state.items()}, strict=False
            )
            return eval_fitness.evaluate(eval_student.model, state_dict=None).score

        for gen in range(1, args.generations + 1):
            console.print(f"  [cyan]Generation {gen}/{args.generations}â€¦[/cyan]")
            population.evaluate(fitness_fn)

            best_ind = population.best
            avg_fit  = population.average_fitness
            div      = _cpu_diversity(population)
            mut_rate = genetics._get_current_mutation_rate()

            dashboard.update_evolution(
                gen=gen,
                best_fitness=best_ind.fitness,
                avg_fitness=avg_fit,
                diversity=div,
                mutation_rate=mut_rate,
            )

            best_fitness     = best_ind.fitness
            generations_done = gen

            if gen < args.generations:
                population.evolve()

        # Load best LoRA weights back onto cuda:0 student for saving
        lora_mgr_train.set_lora_state_dict(
            {k: v.to(args.device) for k, v in population.best.state_dict.items()}, strict=False
        )
        eval_student.unload()
        console.print(f"[green]Evolution done. Best fitness={best_fitness:.4f}[/green]")

    # ------------------------------------------------------------------
    # 7. Save outputs
    # ------------------------------------------------------------------
    console.print("\n[bold]Saving plots and reportâ€¦[/bold]")
    dashboard.save_plots(str(output_dir))
    dashboard.save_report(str(output_dir))

    # Save final student checkpoint
    checkpoint_dir = output_dir / "checkpoint-final"
    student.save(str(checkpoint_dir))
    console.print(f"[green]Final checkpoint saved to {checkpoint_dir}[/green]")

    # ------------------------------------------------------------------
    # 8. Summary table
    # ------------------------------------------------------------------
    _print_summary(
        args=args,
        output_dir=output_dir,
        distill_results=distill_results,
        generations_done=generations_done,
        best_fitness=best_fitness,
    )


def _print_summary(
    args,
    output_dir: Path,
    distill_results: dict,
    generations_done: int,
    best_fitness: float,
) -> None:
    console.rule("[bold green]Training Summary[/bold green]")

    table = Table(show_header=True, header_style="bold magenta", expand=True)
    table.add_column("Metric", style="cyan")
    table.add_column("Value", style="green")

    table.add_row("Total distillation steps", str(distill_results["global_step"]))
    table.add_row(
        "Final distillation loss",
        f"{distill_results['training_logs'][-1]['loss']:.4f}"
        if distill_results["training_logs"]
        else "â€”",
    )
    table.add_row("Best eval loss", f"{distill_results['best_eval_loss']:.4f}")
    table.add_row("Generations completed", str(generations_done))
    table.add_row("Best fitness", f"{best_fitness:.4f}" if generations_done > 0 else "â€”")
    table.add_row("Output directory", str(output_dir))
    table.add_row(
        "TensorBoard",
        f"tensorboard --logdir {output_dir / 'tensorboard'}",
    )

    console.print(table)


if __name__ == "__main__":
    main()
```

<a id="scripts-watch_training-py"></a>

#### `scripts/watch_training.py`
*11034 bytes Â· ~2,534 tokens*

```python
#!/usr/bin/env python3
"""Live Genesis training monitor.

Usage:
    python scripts/watch_training.py                          # default /tmp/genesis_training.log
    python scripts/watch_training.py /path/to/output.log     # explicit file
"""

import re
import sys
import time
import argparse
from pathlib import Path

DEFAULT_LOG = "/tmp/genesis_training.log"

try:
    from rich.live import Live
    from rich.table import Table
    from rich.panel import Panel
    from rich.columns import Columns
    from rich.text import Text
    from rich.console import Console
    from rich.rule import Rule
except ImportError:
    sys.exit("rich not found â€” run: .venv/bin/pip install rich")

console = Console()


# â”€â”€ Parsers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _parse_header(lines: list[str]) -> dict:
    """Extract total_steps and total_gens from the run header line."""
    total_steps, total_gens, pop_size = 0, 0, 0
    for l in lines:
        m = re.search(r"Steps\s*:\s*(\d+)", l)
        if m:
            total_steps = int(m.group(1))
        m = re.search(r"Generations:\s*(\d+)", l)
        if m:
            total_gens = int(m.group(1))
        m = re.search(r"Pop:\s*(\d+)", l)
        if m:
            pop_size = int(m.group(1))
        if total_steps and total_gens:
            break
    return {"total_steps": total_steps or 1, "total_gens": total_gens or 1, "pop_size": pop_size}


def _parse_distillation(lines: list[str]) -> dict:
    """Parse step losses and eval losses."""
    step_lines = [l for l in lines if re.search(r"Step \d+: loss=", l)]
    eval_lines = [l for l in lines if "Evaluation:" in l and "loss" in l]

    step_num = loss = None
    if step_lines:
        last = step_lines[-1]
        m = re.search(r"Step (\d+):", last)
        if m:
            step_num = int(m.group(1))
        m = re.search(r"loss=([\d.]+)", last)
        if m:
            loss = float(m.group(1))

    # Collect all per-step losses for sparkline
    all_losses = []
    for l in step_lines[-15:]:
        m = re.search(r"loss=([\d.]+)", l)
        if m:
            all_losses.append(float(m.group(1)))

    # Best eval loss
    best_eval = None
    eval_losses = []
    for l in eval_lines:
        m = re.search(r"'loss':\s*([\d.]+)", l)
        if m:
            v = float(m.group(1))
            eval_losses.append(v)
            if best_eval is None or v < best_eval:
                best_eval = v

    # Progress bar line (last tqdm line)
    prog = ""
    for l in reversed(lines):
        if ("it/s" in l or "s/it" in l) and "Training" in l:
            prog = l.strip()
            break

    return dict(
        step_num=step_num,
        loss=loss,
        all_losses=all_losses,
        best_eval=best_eval,
        eval_losses=eval_losses,
        prog=prog,
    )


def _parse_evolution(lines: list[str]) -> dict:
    """Parse evolutionary generation stats."""
    gen_lines = [l for l in lines if re.search(r"Generation \d+: Best=", l)]

    gen_num = best_fit = avg_fit = None
    if gen_lines:
        last = gen_lines[-1]
        m = re.search(r"Generation (\d+):", last)
        if m:
            gen_num = int(m.group(1))
        m = re.search(r"Best=([\d.]+)", last)
        if m:
            best_fit = float(m.group(1))
        m = re.search(r"Avg=([\d.]+)", last)
        if m:
            avg_fit = float(m.group(1))

    all_fits = []
    for l in gen_lines:
        m = re.search(r"Best=([\d.]+)", l)
        if m:
            all_fits.append(float(m.group(1)))

    return dict(gen_num=gen_num, best_fit=best_fit, avg_fit=avg_fit, all_fits=all_fits)


def _parse_teacher(lines: list[str], data: str) -> dict:
    """Parse teacher / logprobs status."""
    logprobs_ok = any("logprobs: SUPPORTED" in l for l in lines)
    logprobs_no = any("logprobs: NOT SUPPORTED" in l for l in lines)

    mode = "unknown"
    if logprobs_ok:
        mode = "KD soft-target"
    elif logprobs_no:
        mode = "hard-label CE only"

    return dict(
        mode=mode,
        fallbacks=data.count("Switching to fallback"),
        tmr=data.count("Too Many Requests"),
        exhausted=data.count("All teacher models exhausted"),
    )


def parse(data: str) -> dict:
    lines = data.splitlines()
    header  = _parse_header(lines)
    distill = _parse_distillation(lines)
    evo     = _parse_evolution(lines)
    teacher = _parse_teacher(lines, data)
    done    = "Evolution done" in data or "checkpoint-final" in data
    return {**header, **distill, **evo, **teacher, "done": done, "lines": lines}


# â”€â”€ Sparkline helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _spark(values: list[float], width: int = 12) -> str:
    if len(values) < 2:
        return ""
    mn, mx = min(values), max(values)
    blocks = "â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ"
    return " ".join(blocks[min(7, int(7 * (v - mn) / (mx - mn + 1e-9)))] for v in values[-width:])


# â”€â”€ Panel builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_panel(d: dict, log_path: str) -> Panel:
    total_steps = d["total_steps"]
    total_gens  = d["total_gens"]

    # â”€â”€ Distillation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    dt = Table(show_header=False, box=None, padding=(0, 1))
    dt.add_column("k", style="cyan", no_wrap=True)
    dt.add_column("v", style="green")

    if d["step_num"] is not None:
        pct = 100 * d["step_num"] / total_steps
        bar = "â–ˆ" * int(pct / 5) + "â–‘" * (20 - int(pct / 5))
        dt.add_row("Step",      f"{d['step_num']} / {total_steps}  [{bar}] {pct:.1f}%")
        dt.add_row("Loss",      f"{d['loss']:.4f}" if d["loss"] is not None else "â€”")
        spark = _spark(d["all_losses"])
        if spark:
            dt.add_row("Trend", spark)
        if d["best_eval"] is not None:
            dt.add_row("Best eval loss", f"[bold]{d['best_eval']:.4f}[/bold]")
            if len(d["eval_losses"]) > 1:
                dt.add_row("Eval trend", _spark(d["eval_losses"]))
    else:
        phase = "distillation startingâ€¦"
        if any("Phase 1" in l for l in d["lines"]):
            phase = "loading modelâ€¦"
        dt.add_row("Status", phase)

    if d["prog"]:
        dt.add_row("Progress", d["prog"][-80:])

    # â”€â”€ Evolution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    et = Table(show_header=False, box=None, padding=(0, 1))
    et.add_column("k", style="cyan", no_wrap=True)
    et.add_column("v", style="magenta")

    if d["gen_num"] is not None:
        pct = 100 * (d["gen_num"] + 1) / total_gens
        bar = "â–ˆ" * int(pct / 5) + "â–‘" * (20 - int(pct / 5))
        et.add_row("Gen",          f"{d['gen_num'] + 1} / {total_gens}  [{bar}] {pct:.1f}%")
        et.add_row("Best fitness", f"{d['best_fit']:.6f}" if d["best_fit"] is not None else "â€”")
        et.add_row("Avg fitness",  f"{d['avg_fit']:.6f}"  if d["avg_fit"]  is not None else "â€”")
        if d["best_fit"] is not None:
            ppl = (1.0 / d["best_fit"] - 1.0) if d["best_fit"] > 0 else float("inf")
            et.add_row("Best PPL", f"~{ppl:.1f}")
        spark = _spark(d["all_fits"])
        if spark:
            et.add_row("Trend", spark)
    elif d["done"]:
        et.add_row("Status", "[bold green]complete[/bold green]")
    else:
        et.add_row("Status", "waiting for distillationâ€¦")

    # â”€â”€ Teacher / mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tt = Table(show_header=False, box=None, padding=(0, 1))
    tt.add_column("k", style="cyan", no_wrap=True)
    tt.add_column("v")

    mode_color = "green" if d["mode"] == "KD soft-target" else "yellow"
    tt.add_row("Training mode", Text(d["mode"], style=f"bold {mode_color}"))

    color_fb  = "green" if d["fallbacks"] == 0 else "yellow"
    color_tmr = "green" if d["tmr"] == 0 else "red"
    color_ex  = "green" if d["exhausted"] == 0 else "red"
    tt.add_row("Fallbacks",        Text(str(d["fallbacks"]), style=color_fb))
    tt.add_row("429s",             Text(str(d["tmr"]),       style=color_tmr))
    tt.add_row("Full exhaustions", Text(str(d["exhausted"]), style=color_ex))

    status = "[bold green]DONE[/bold green]" if d["done"] else "[yellow]runningâ€¦[/yellow]"
    tt.add_row("Run status", status)

    return Panel(
        Columns([
            Panel(dt, title="[bold yellow]Distillation[/bold yellow]",  width=58),
            Panel(et, title="[bold magenta]Evolution[/bold magenta]",   width=42),
            Panel(tt, title="[bold cyan]Teacher / Mode[/bold cyan]",    width=32),
        ]),
        title="[bold blue]Genesis Training Monitor[/bold blue]",
        subtitle=f"[dim]{log_path}  |  refresh: 2s  |  Ctrl-C to exit[/dim]",
    )


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(description="Live Genesis training monitor")
    parser.add_argument(
        "log",
        nargs="?",
        default=DEFAULT_LOG,
        help=f"Path to training log file (default: {DEFAULT_LOG})",
    )
    args = parser.parse_args()

    log_path = args.log
    path = Path(log_path)

    with Live(console=console, refresh_per_second=0.5, screen=False) as live:
        while True:
            try:
                data = path.read_text(errors="replace")
                d = parse(data)
                live.update(build_panel(d, log_path))
                if d["done"]:
                    time.sleep(2)
                    break
            except FileNotFoundError:
                live.update(Panel(
                    f"[yellow]Waiting for log file:[/yellow]\n[dim]{log_path}[/dim]\n\n"
                    "[dim]Start training with:[/dim]\n"
                    "[cyan]OLLAMA_HOST=http://192.168.1.97:11434 OLLAMA_MODEL=qwen3.5:cloud \\\n"
                    "  .venv/bin/python scripts/train_qwen3.py --steps 100 --generations 10 "
                    "--pop-size 4 > /tmp/genesis_training.log 2>&1 &[/cyan]",
                    title="[bold blue]Genesis Training Monitor[/bold blue]",
                ))
            except KeyboardInterrupt:
                break
            time.sleep(2)


if __name__ == "__main__":
    main()
```


### ğŸ§ª Tests

<a id="scripts-test_evolved_model-py"></a>

#### `scripts/test_evolved_model.py`
*12304 bytes Â· ~2,917 tokens*

```python
#!/usr/bin/env python3
"""Evaluate and compare the best evolved model from a Genesis training run.

Usage:
    python scripts/test_evolved_model.py                          # latest run
    python scripts/test_evolved_model.py --run-dir outputs/run_X  # specific run
    python scripts/test_evolved_model.py --run-dir outputs/run_X --prompts 5

Exit code:
    0  evolved model perplexity â‰¤ base model perplexity  (improvement / no regression)
    1  evolved model is worse than base model            (regression detected)
"""

from __future__ import annotations

import argparse
import json
import math
import sys
from pathlib import Path

# project root on path
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
from rich.console import Console
from rich.table import Table

console = Console()

STUDENT_PATH = "/media/ttech-main/42A4266DA426639F/Models/Qwen3-1.7B"

GENERATION_PROMPTS = [
    "The history of artificial intelligence begins",
    "In quantum mechanics, the uncertainty principle states",
    "The primary causes of the French Revolution were",
    "To train a neural network effectively, one must",
    "The Amazon rainforest is important because",
]


# â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _find_run_dir(run_dir: str | None) -> Path:
    """Return the requested run dir, or auto-detect the latest one."""
    outputs = Path("outputs")
    if run_dir:
        path = Path(run_dir)
        if not path.exists():
            sys.exit(f"[error] run dir not found: {path}")
        return path
    runs = sorted(
        [d for d in outputs.iterdir() if d.is_dir() and d.name.startswith("run_")],
        key=lambda d: d.stat().st_mtime,
    )
    if not runs:
        sys.exit("[error] No run dirs found under outputs/")
    return runs[-1]


def _best_checkpoint(run_dir: Path) -> Path:
    """Return the best *evolved* checkpoint.

    Priority:
      1. checkpoint-final  â€” saved after evolution with population.best LoRA weights.
                             This IS the best evolved model.
      2. checkpoint-best   â€” saved mid-distillation when eval loss was lowest.
                             Only used as a fallback when evolution was skipped.
    """
    final = run_dir / "checkpoint-final"
    best  = run_dir / "checkpoint-best"
    if final.exists() and (final / "adapter_model.safetensors").exists():
        return final
    if best.exists() and (best / "adapter_model.safetensors").exists():
        return best
    sys.exit(f"[error] No valid checkpoint found in {run_dir}")


def _load_report(run_dir: Path) -> dict:
    report_path = run_dir / "training_report.json"
    if report_path.exists():
        with open(report_path) as f:
            return json.load(f)
    return {}


def _build_eval_loader(tokenizer, seq_len: int, batch_size: int):
    from datasets import load_dataset
    from torch.utils.data import DataLoader, TensorDataset

    raw = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
    eval_texts = [raw[i]["text"] for i in range(200, min(250, len(raw)))
                  if raw[i]["text"].strip()]
    enc = tokenizer(
        eval_texts,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=seq_len,
    )
    labels = enc["input_ids"].clone()
    labels[enc["attention_mask"] == 0] = -100
    ds = TensorDataset(enc["input_ids"], enc["attention_mask"], labels)

    def collate(batch):
        ids, masks, lbls = zip(*batch)
        return {
            "input_ids": torch.stack(ids),
            "attention_mask": torch.stack(masks),
            "labels": torch.stack(lbls),
        }

    return DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)


@torch.no_grad()
def _evaluate_perplexity(model, dataloader, device: str) -> tuple[float, float]:
    """Return (mean_ce_loss, perplexity) on the given dataloader."""
    model.eval()
    total_loss = 0.0
    total_tokens = 0

    for batch in dataloader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss  # mean CE over non-masked positions
        num_tokens = (labels != -100).sum().item()
        if num_tokens > 0:
            total_loss += loss.item() * num_tokens
            total_tokens += num_tokens

    avg_loss = total_loss / total_tokens if total_tokens > 0 else float("inf")
    ppl = math.exp(min(avg_loss, 20))   # cap at exp(20) to avoid overflow display
    return avg_loss, ppl


@torch.no_grad()
def _generate(model, tokenizer, prompt: str, device: str, max_new: int = 60) -> str:
    ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
    out = model.generate(
        ids,
        max_new_tokens=max_new,
        do_sample=False,
        temperature=1.0,
        pad_token_id=tokenizer.eos_token_id,
    )
    new_ids = out[0, ids.shape[1]:]
    return tokenizer.decode(new_ids, skip_special_tokens=True)


# â”€â”€ main evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def evaluate(run_dir_arg: str | None, num_prompts: int, seq_len: int, batch_size: int) -> int:
    run_dir    = _find_run_dir(run_dir_arg)
    ckpt_path  = _best_checkpoint(run_dir)
    report     = _load_report(run_dir)

    console.rule(f"[bold blue]Genesis Model Evaluation[/bold blue]")
    console.print(f"Run dir    : [green]{run_dir}[/green]")
    console.print(f"Checkpoint : [cyan]{ckpt_path.name}[/cyan]")

    device = "cuda:1" if torch.cuda.is_available() and torch.cuda.device_count() > 1 else \
             "cuda:0" if torch.cuda.is_available() else "cpu"

    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel

    tokenizer = AutoTokenizer.from_pretrained(STUDENT_PATH)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    eval_loader = _build_eval_loader(tokenizer, seq_len, batch_size)
    console.print(f"[dim]Eval set: {len(eval_loader.dataset)} samples, seq_len={seq_len}[/dim]")

    prompts = GENERATION_PROMPTS[:num_prompts]

    # â”€â”€ Load base model once, then wrap with LoRA (+~50 MB, no reload) â”€â”€â”€
    console.print("\n[bold]Loading base model (no LoRA)â€¦[/bold]")
    base_model = AutoModelForCausalLM.from_pretrained(
        STUDENT_PATH, dtype=torch.bfloat16
    ).to(device)

    console.print("[bold]Evaluating base modelâ€¦[/bold]")
    base_loss, base_ppl = _evaluate_perplexity(base_model, eval_loader, device)
    base_fitness = 1.0 / (1.0 + base_ppl)

    base_gens = [_generate(base_model, tokenizer, p, device) for p in prompts]

    # Wrap existing base model with LoRA adapter â€” loads weights to CPU first,
    # then moves to device. This avoids PEFT's direct-to-GPU loading which can OOM
    # on cards already occupied by Ollama or other processes.
    console.print("[bold]Applying LoRA adapter to base modelâ€¦[/bold]")
    from safetensors.torch import load_file as _sft_load
    from peft import PeftConfig

    peft_config = PeftConfig.from_pretrained(str(ckpt_path))
    evolved_model = PeftModel(base_model, peft_config)

    adapter_path = ckpt_path / "adapter_model.safetensors"
    cpu_weights = _sft_load(str(adapter_path), device="cpu")
    # PEFT saves keys as "...lora_A.weight" but PeftModel state_dict uses
    # "...lora_A.default.weight" (inserting the adapter name "default").
    # Remap before loading so weights are actually injected.
    remapped = {}
    for k, v in cpu_weights.items():
        new_k = k.replace("lora_A.weight", "lora_A.default.weight") \
                 .replace("lora_B.weight", "lora_B.default.weight")
        remapped[new_k] = v.to(device)
    missing, unexpected = evolved_model.load_state_dict(remapped, strict=False)
    if unexpected:
        console.print(f"[yellow]Warning: {len(unexpected)} unexpected adapter keys[/yellow]")
    # missing_keys here are the base model weights â€” expected, adapter only contains LoRA A/B.
    del cpu_weights, remapped
    torch.cuda.empty_cache()

    console.print("[bold]Evaluating evolved modelâ€¦[/bold]")
    evo_loss, evo_ppl = _evaluate_perplexity(evolved_model, eval_loader, device)
    evo_fitness = 1.0 / (1.0 + evo_ppl)

    evo_gens = [_generate(evolved_model, tokenizer, p, device) for p in prompts]

    del evolved_model
    torch.cuda.empty_cache()

    # â”€â”€ 3. Metrics table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    delta_ppl  = evo_ppl  - base_ppl
    delta_loss = evo_loss - base_loss
    improved   = evo_ppl <= base_ppl

    console.rule("[bold green]Results[/bold green]")
    t = Table(show_header=True, header_style="bold magenta", expand=False)
    t.add_column("Metric",        style="cyan",  no_wrap=True)
    t.add_column("Base model",    style="white", justify="right")
    t.add_column("Evolved model", style="white", justify="right")
    t.add_column("Î”",             justify="right")

    def _delta(v, invert=False):
        good = v < 0 if not invert else v > 0
        color = "green" if good else "red"
        sign  = "+" if v >= 0 else ""
        return f"[{color}]{sign}{v:.4f}[/{color}]"

    t.add_row("CE loss",  f"{base_loss:.4f}", f"{evo_loss:.4f}",  _delta(delta_loss))
    t.add_row("PPL",      f"{base_ppl:.2f}",  f"{evo_ppl:.2f}",   _delta(delta_ppl))
    t.add_row("Fitness",  f"{base_fitness:.6f}", f"{evo_fitness:.6f}", _delta(evo_fitness - base_fitness, invert=True))

    if report:
        evo_report_gens = report.get("evolution", [])
        if evo_report_gens:
            reported_best = max(g.get("best_fitness", 0) for g in evo_report_gens)
            t.add_row("Reported best fitness (train)", "â€”", f"{reported_best:.6f}", "")

    console.print(t)

    verdict = "[bold green]PASS â€” evolved â‰¤ base perplexity[/bold green]" \
              if improved else \
              "[bold red]FAIL â€” evolved > base perplexity (regression)[/bold red]"
    console.print(f"\nVerdict: {verdict}")

    # â”€â”€ 4. Generation comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if prompts:
        console.rule("[bold yellow]Generation Comparison[/bold yellow]")
        gen_table = Table(show_header=True, header_style="bold", expand=True)
        gen_table.add_column("Prompt",        style="cyan",  width=30, no_wrap=True)
        gen_table.add_column("Base model",    style="white", width=55)
        gen_table.add_column("Evolved model", style="green", width=55)

        for prompt, base_gen, evo_gen in zip(prompts, base_gens, evo_gens):
            gen_table.add_row(
                prompt[:28] + "â€¦" if len(prompt) > 28 else prompt,
                base_gen[:200],
                evo_gen[:200],
            )

        console.print(gen_table)

    return 0 if improved else 1


# â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(description="Evaluate best evolved Genesis model")
    parser.add_argument(
        "--run-dir", default=None,
        help="Training run directory (default: latest outputs/run_*)"
    )
    parser.add_argument("--prompts",    type=int, default=3,   help="Generation prompts to show (0=skip)")
    parser.add_argument("--seq-len",    type=int, default=128, help="Token sequence length for eval")
    parser.add_argument("--batch-size", type=int, default=4,   help="Eval batch size")
    args = parser.parse_args()

    exit_code = evaluate(args.run_dir, args.prompts, args.seq_len, args.batch_size)
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
```

<a id="tests-__init__-py"></a>

#### `tests/__init__.py`
*25 bytes Â· ~6 tokens*

```python
"""Tests for Genesis."""
```

<a id="tests-test_checkpointing-py"></a>

#### `tests/test_checkpointing.py`
*7037 bytes Â· ~1,674 tokens*

```python
"""Tests for genesis/utils/checkpointing.py."""

import pytest
import torch
import torch.nn as nn
from torch.optim import SGD

from genesis.utils.checkpointing import (
    save_checkpoint,
    load_checkpoint,
    CheckpointManager,
)


# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _simple_model():
    return nn.Linear(4, 2)


def _simple_optimizer(model):
    return SGD(model.parameters(), lr=0.01)


# â”€â”€ save_checkpoint / load_checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestSaveLoadCheckpoint:

    def test_save_creates_file(self, tmp_path):
        model = _simple_model()
        path = str(tmp_path / "ckpt.pt")
        save_checkpoint(path, model=model, epoch=1, step=10)
        assert (tmp_path / "ckpt.pt").exists()

    def test_load_restores_model_weights(self, tmp_path):
        model = _simple_model()
        # Set a known weight
        with torch.no_grad():
            model.weight.fill_(3.14)
        path = str(tmp_path / "ckpt.pt")
        save_checkpoint(path, model=model)

        # Load into a fresh model
        model2 = _simple_model()
        load_checkpoint(path, model=model2)
        assert torch.allclose(model2.weight, model.weight)

    def test_load_restores_epoch_and_step(self, tmp_path):
        path = str(tmp_path / "ckpt.pt")
        save_checkpoint(path, epoch=5, step=100)
        ckpt = load_checkpoint(path)
        assert ckpt["epoch"] == 5
        assert ckpt["step"] == 100

    def test_load_restores_optimizer_state(self, tmp_path):
        model = _simple_model()
        opt = _simple_optimizer(model)
        # Run a step to populate optimizer state
        out = model(torch.randn(2, 4))
        out.sum().backward()
        opt.step()

        path = str(tmp_path / "ckpt.pt")
        save_checkpoint(path, model=model, optimizer=opt)

        model2 = _simple_model()
        opt2 = _simple_optimizer(model2)
        load_checkpoint(path, model=model2, optimizer=opt2)
        # Optimizer state restored (e.g. SGD has no running state, but no error)

    def test_save_with_config(self, tmp_path):
        path = str(tmp_path / "ckpt.pt")
        save_checkpoint(path, config={"lr": 0.01, "batch_size": 32})
        ckpt = load_checkpoint(path)
        assert ckpt["config"]["lr"] == 0.01

    def test_save_with_extra_kwargs(self, tmp_path):
        path = str(tmp_path / "ckpt.pt")
        save_checkpoint(path, custom_key="hello")
        ckpt = load_checkpoint(path)
        assert ckpt["custom_key"] == "hello"

    def test_checkpoint_contains_timestamp(self, tmp_path):
        path = str(tmp_path / "ckpt.pt")
        save_checkpoint(path)
        ckpt = load_checkpoint(path)
        assert "timestamp" in ckpt

    def test_saves_to_nested_dir(self, tmp_path):
        path = str(tmp_path / "deep" / "nested" / "ckpt.pt")
        save_checkpoint(path)
        assert (tmp_path / "deep" / "nested" / "ckpt.pt").exists()


# â”€â”€ CheckpointManager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestCheckpointManager:

    def test_save_creates_checkpoint_file(self, tmp_path):
        manager = CheckpointManager(str(tmp_path))
        model = _simple_model()
        path = manager.save(model, step=1, metric=0.5)
        assert path is not None
        import os; assert os.path.exists(path)

    def test_save_returns_path_string(self, tmp_path):
        manager = CheckpointManager(str(tmp_path))
        path = manager.save(_simple_model(), step=1)
        assert isinstance(path, str)

    def test_best_checkpoint_written_on_improvement(self, tmp_path):
        manager = CheckpointManager(str(tmp_path), mode="min")
        manager.save(_simple_model(), step=1, metric=1.0)
        manager.save(_simple_model(), step=2, metric=0.5)  # better
        assert manager.best_checkpoint is not None
        assert (tmp_path / "checkpoint_best.pt").exists()

    def test_best_metric_tracked_min(self, tmp_path):
        manager = CheckpointManager(str(tmp_path), mode="min")
        manager.save(_simple_model(), step=1, metric=1.0)
        manager.save(_simple_model(), step=2, metric=0.3)
        assert manager.best_metric == pytest.approx(0.3)

    def test_best_metric_tracked_max(self, tmp_path):
        manager = CheckpointManager(str(tmp_path), mode="max")
        manager.save(_simple_model(), step=1, metric=0.5)
        manager.save(_simple_model(), step=2, metric=0.9)
        assert manager.best_metric == pytest.approx(0.9)

    def test_cleanup_keeps_max_checkpoints(self, tmp_path):
        manager = CheckpointManager(str(tmp_path), max_checkpoints=3)
        for i in range(6):
            manager.save(_simple_model(), step=i, metric=float(i))
        # Should not exceed max_checkpoints non-best files
        assert len(manager.get_checkpoint_list()) <= 3

    def test_load_best_restores_model(self, tmp_path):
        manager = CheckpointManager(str(tmp_path), mode="min")
        model = _simple_model()
        with torch.no_grad():
            model.weight.fill_(7.0)
        manager.save(model, step=1, metric=0.1)

        model2 = _simple_model()
        manager.load_best(model2)
        assert torch.allclose(model2.weight, model.weight)

    def test_load_best_raises_if_none(self, tmp_path):
        manager = CheckpointManager(str(tmp_path))
        with pytest.raises(FileNotFoundError):
            manager.load_best(_simple_model())

    def test_load_latest_restores_model(self, tmp_path):
        manager = CheckpointManager(str(tmp_path))
        model = _simple_model()
        with torch.no_grad():
            model.weight.fill_(2.5)
        manager.save(model, step=1)
        manager.save(_simple_model(), step=2)

        # Load latest (step 2)
        model2 = _simple_model()
        manager.load_latest(model2)

    def test_load_latest_raises_if_no_checkpoints(self, tmp_path):
        manager = CheckpointManager(str(tmp_path))
        with pytest.raises(FileNotFoundError):
            manager.load_latest(_simple_model())

    def test_latest_checkpoint_property(self, tmp_path):
        manager = CheckpointManager(str(tmp_path))
        assert manager.latest_checkpoint is None
        manager.save(_simple_model(), step=1)
        assert manager.latest_checkpoint is not None

    def test_checkpoint_info_persists_across_instances(self, tmp_path):
        """Checkpoint list is saved to JSON and reloaded."""
        manager1 = CheckpointManager(str(tmp_path))
        manager1.save(_simple_model(), step=10, metric=0.5)

        manager2 = CheckpointManager(str(tmp_path))
        assert len(manager2.get_checkpoint_list()) == 1
        assert manager2.get_checkpoint_list()[0]["step"] == 10
```

<a id="tests-test_cli-py"></a>

#### `tests/test_cli.py`
*7623 bytes Â· ~1,697 tokens*

```python
"""Tests for the genesis CLI parser (genesis/cli.py)."""

import sys
import pytest
import importlib.util
from pathlib import Path


# Load cli module in isolation â€” avoids pulling in torch/transformers at import time
def _load_cli():
    spec = importlib.util.spec_from_file_location(
        "genesis.cli",
        Path(__file__).parent.parent / "genesis" / "cli.py",
    )
    mod = importlib.util.module_from_spec(spec)
    sys.modules["genesis.cli"] = mod
    spec.loader.exec_module(mod)
    return mod


CLI = _load_cli()


# â”€â”€ Parser construction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestParserConstruction:

    def test_build_parser_returns_parser(self):
        p = CLI.build_parser()
        assert p is not None

    def test_subcommand_required(self):
        p = CLI.build_parser()
        with pytest.raises(SystemExit):
            p.parse_args([])  # no subcommand â†’ error


# â”€â”€ `run` subcommand â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestRunSubcommand:

    def _parse(self, *args):
        return CLI.build_parser().parse_args(["run", *args])

    def test_minimal(self):
        ns = self._parse()
        assert ns.command == "run"

    def test_config_flag(self):
        ns = self._parse("--config", "experiment.yaml")
        assert ns.config == "experiment.yaml"

    def test_config_short_flag(self):
        ns = self._parse("-c", "cfg.yaml")
        assert ns.config == "cfg.yaml"

    def test_output_dir_flag(self):
        ns = self._parse("--output-dir", "/tmp/out")
        assert ns.output_dir == "/tmp/out"

    def test_generations_flag(self):
        ns = self._parse("--generations", "25")
        assert ns.generations == 25

    def test_generations_short_flag(self):
        ns = self._parse("-g", "10")
        assert ns.generations == 10

    def test_population_size_flag(self):
        ns = self._parse("--population-size", "15")
        assert ns.population_size == 15

    def test_teacher_model_flag(self):
        ns = self._parse("--teacher-model", "gpt2")
        assert ns.teacher_model == "gpt2"

    def test_student_model_flag(self):
        ns = self._parse("--student-model", "distilgpt2")
        assert ns.student_model == "distilgpt2"

    def test_no_lora_flag(self):
        ns = self._parse("--no-lora")
        assert ns.no_lora is True

    def test_no_lora_absent_is_false(self):
        ns = self._parse()
        assert ns.no_lora is False

    def test_all_flags_together(self):
        ns = self._parse(
            "--config", "cfg.yaml",
            "--output-dir", "/out",
            "--generations", "5",
            "--population-size", "8",
            "--teacher-model", "gpt2",
            "--student-model", "distilgpt2",
            "--no-lora",
        )
        assert ns.config == "cfg.yaml"
        assert ns.output_dir == "/out"
        assert ns.generations == 5
        assert ns.population_size == 8
        assert ns.teacher_model == "gpt2"
        assert ns.student_model == "distilgpt2"
        assert ns.no_lora is True

    def test_func_is_callable(self):
        ns = self._parse()
        assert callable(ns.func)


# â”€â”€ `distill` subcommand â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestDistillSubcommand:

    def _parse(self, *args):
        return CLI.build_parser().parse_args(["distill", *args])

    def test_minimal(self):
        ns = self._parse()
        assert ns.command == "distill"

    def test_config_flag(self):
        ns = self._parse("-c", "d.yaml")
        assert ns.config == "d.yaml"

    def test_steps_flag(self):
        ns = self._parse("--steps", "500")
        assert ns.steps == 500

    def test_steps_short_flag(self):
        ns = self._parse("-s", "100")
        assert ns.steps == 100

    def test_teacher_model_flag(self):
        ns = self._parse("--teacher-model", "llama")
        assert ns.teacher_model == "llama"

    def test_student_model_flag(self):
        ns = self._parse("--student-model", "small")
        assert ns.student_model == "small"

    def test_func_is_callable(self):
        ns = self._parse()
        assert callable(ns.func)


# â”€â”€ `prune` subcommand â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestPruneSubcommand:

    def _parse(self, *args):
        return CLI.build_parser().parse_args(["prune", *args])

    def test_minimal(self):
        ns = self._parse()
        assert ns.command == "prune"

    def test_sparsity_flag(self):
        ns = self._parse("--sparsity", "0.3")
        assert ns.sparsity == pytest.approx(0.3)

    def test_method_flag(self):
        for method in ["magnitude", "gradient", "taylor", "fisher"]:
            ns = self._parse("--method", method)
            assert ns.method == method

    def test_invalid_method_raises(self):
        with pytest.raises(SystemExit):
            CLI.build_parser().parse_args(["prune", "--method", "unknown"])

    def test_config_flag(self):
        ns = self._parse("--config", "p.yaml")
        assert ns.config == "p.yaml"

    def test_student_model_flag(self):
        ns = self._parse("--student-model", "mymodel")
        assert ns.student_model == "mymodel"

    def test_func_is_callable(self):
        ns = self._parse()
        assert callable(ns.func)


# â”€â”€ `info` subcommand â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestInfoSubcommand:

    def _parse(self, *args):
        return CLI.build_parser().parse_args(["info", *args])

    def test_minimal(self):
        ns = self._parse()
        assert ns.command == "info"

    def test_func_is_callable(self):
        ns = self._parse()
        assert callable(ns.func)


# â”€â”€ Global flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestGlobalFlags:

    def test_log_level_default(self):
        ns = CLI.build_parser().parse_args(["info"])
        assert ns.log_level == "INFO"

    @pytest.mark.parametrize("level", ["DEBUG", "INFO", "WARNING", "ERROR"])
    def test_log_level_choices(self, level):
        ns = CLI.build_parser().parse_args(["--log-level", level, "info"])
        assert ns.log_level == level

    def test_invalid_log_level_raises(self):
        with pytest.raises(SystemExit):
            CLI.build_parser().parse_args(["--log-level", "TRACE", "info"])


# â”€â”€ Each subcommand maps to the right function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestSubcommandFunctions:

    @pytest.mark.parametrize("cmd,fn_name", [
        (["run"], "_cmd_run"),
        (["distill"], "_cmd_distill"),
        (["prune"], "_cmd_prune"),
        (["info"], "_cmd_info"),
    ])
    def test_func_name(self, cmd, fn_name):
        ns = CLI.build_parser().parse_args(cmd)
        assert ns.func.__name__ == fn_name
```

<a id="tests-test_config-py"></a>

#### `tests/test_config.py`
*9109 bytes Â· ~2,080 tokens*

```python
"""Tests for GenesisConfig, sub-configs, and HardwareConfig."""

import pytest
import torch
import yaml

from genesis.config.settings import (
    GenesisConfig,
    GeneticConfig,
    DistillationConfig,
    PruningConfig,
    LoRAConfig,
)
from genesis.config.hardware import HardwareConfig


# â”€â”€ GeneticConfig â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestGeneticConfig:

    def test_defaults(self):
        cfg = GeneticConfig()
        assert cfg.population_size == 20
        assert cfg.generations == 50
        assert 0.0 < cfg.mutation_rate < 1.0
        assert cfg.elite_size >= 1

    def test_custom_values(self):
        cfg = GeneticConfig(population_size=10, generations=5, mutation_rate=0.2)
        assert cfg.population_size == 10
        assert cfg.generations == 5
        assert cfg.mutation_rate == 0.2

    def test_adaptive_mutation_fields(self):
        cfg = GeneticConfig()
        assert hasattr(cfg, "adaptive_mutation")
        assert hasattr(cfg, "mutation_decay")
        assert hasattr(cfg, "min_mutation_rate")


# â”€â”€ DistillationConfig â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestDistillationConfig:

    def test_defaults(self):
        cfg = DistillationConfig()
        assert cfg.temperature > 1.0
        assert 0.0 < cfg.alpha <= 1.0

    def test_custom_temperature(self):
        cfg = DistillationConfig(temperature=2.0, alpha=0.3)
        assert cfg.temperature == 2.0
        assert cfg.alpha == 0.3


# â”€â”€ PruningConfig â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestPruningConfig:

    def test_defaults(self):
        cfg = PruningConfig()
        assert 0.0 < cfg.target_sparsity < 1.0
        assert cfg.pruning_method in {"magnitude", "gradient", "taylor"}

    def test_has_all_pruner_fields(self):
        """settings.PruningConfig must be field-compatible with pruner.PruningConfig."""
        cfg = PruningConfig()
        # Fields added during the high-severity bug fix
        assert hasattr(cfg, "block_size")
        assert hasattr(cfg, "skip_layers")
        assert hasattr(cfg, "layer_sparsity_overrides")
        assert isinstance(cfg.skip_layers, list)
        assert isinstance(cfg.layer_sparsity_overrides, dict)


# â”€â”€ LoRAConfig â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestLoRAConfig:

    def test_defaults(self):
        cfg = LoRAConfig()
        assert cfg.r > 0
        assert cfg.lora_alpha > 0
        assert isinstance(cfg.target_modules, list)

    def test_has_modules_to_save(self):
        """Bug-fix check: modules_to_save must exist."""
        cfg = LoRAConfig()
        assert hasattr(cfg, "modules_to_save")
        assert cfg.modules_to_save is None

    def test_custom_modules_to_save(self):
        cfg = LoRAConfig(modules_to_save=["embed_tokens", "lm_head"])
        assert cfg.modules_to_save == ["embed_tokens", "lm_head"]


# â”€â”€ GenesisConfig â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestGenesisConfig:

    def test_default_construction(self, tmp_path):
        cfg = GenesisConfig(
            output_dir=str(tmp_path / "outputs"),
            checkpoint_dir=str(tmp_path / "checkpoints"),
            log_dir=str(tmp_path / "logs"),
        )
        assert cfg.project_name == "genesis_experiment"
        assert isinstance(cfg.genetic, GeneticConfig)
        assert isinstance(cfg.distillation, DistillationConfig)
        assert isinstance(cfg.pruning, PruningConfig)
        assert isinstance(cfg.lora, LoRAConfig)

    def test_post_init_creates_directories(self, tmp_path):
        out = tmp_path / "out"
        ckpt = tmp_path / "ckpt"
        logs = tmp_path / "logs"
        GenesisConfig(
            output_dir=str(out),
            checkpoint_dir=str(ckpt),
            log_dir=str(logs),
        )
        assert out.exists()
        assert ckpt.exists()
        assert logs.exists()

    def test_from_dict_basic(self, tmp_path):
        d = {
            "project_name": "test_proj",
            "output_dir": str(tmp_path / "out"),
            "checkpoint_dir": str(tmp_path / "ckpt"),
            "log_dir": str(tmp_path / "logs"),
        }
        cfg = GenesisConfig.from_dict(d)
        assert cfg.project_name == "test_proj"

    def test_from_dict_with_sub_configs(self, tmp_path):
        d = {
            "output_dir": str(tmp_path / "out"),
            "checkpoint_dir": str(tmp_path / "ckpt"),
            "log_dir": str(tmp_path / "logs"),
            "genetic": {"population_size": 8, "generations": 3},
            "distillation": {"temperature": 2.0},
            "pruning": {},
            "lora": {},
        }
        cfg = GenesisConfig.from_dict(d)
        assert cfg.genetic.population_size == 8
        assert cfg.genetic.generations == 3
        assert cfg.distillation.temperature == 2.0

    def test_to_dict_roundtrip(self, tmp_path):
        cfg = GenesisConfig(
            project_name="roundtrip_test",
            output_dir=str(tmp_path / "out"),
            checkpoint_dir=str(tmp_path / "ckpt"),
            log_dir=str(tmp_path / "logs"),
        )
        d = cfg.to_dict()
        assert d["project_name"] == "roundtrip_test"
        assert "genetic" in d
        assert "distillation" in d
        assert "pruning" in d
        assert "lora" in d

    def test_to_dict_contains_all_pruning_fields(self, tmp_path):
        """Bug-fix check: to_dict() must include the new pruning fields."""
        cfg = GenesisConfig(
            output_dir=str(tmp_path / "out"),
            checkpoint_dir=str(tmp_path / "ckpt"),
            log_dir=str(tmp_path / "logs"),
        )
        pruning_dict = cfg.to_dict()["pruning"]
        assert "block_size" in pruning_dict
        assert "skip_layers" in pruning_dict
        assert "layer_sparsity_overrides" in pruning_dict

    def test_to_dict_slerp_ratio_from_genetic(self, tmp_path):
        """Bug-fix check: slerp_ratio must come from genetic, no dead hasattr branch."""
        cfg = GenesisConfig(
            output_dir=str(tmp_path / "out"),
            checkpoint_dir=str(tmp_path / "ckpt"),
            log_dir=str(tmp_path / "logs"),
        )
        d = cfg.to_dict()
        assert d["genetic"]["slerp_ratio"] == cfg.genetic.slerp_ratio

    def test_yaml_roundtrip(self, tmp_path):
        cfg = GenesisConfig(
            project_name="yaml_test",
            output_dir=str(tmp_path / "out"),
            checkpoint_dir=str(tmp_path / "ckpt"),
            log_dir=str(tmp_path / "logs"),
        )
        cfg.genetic.population_size = 7
        cfg.distillation.temperature = 3.5

        yaml_path = str(tmp_path / "config.yaml")
        cfg.to_yaml(yaml_path)

        loaded = GenesisConfig.from_yaml(yaml_path)
        assert loaded.project_name == "yaml_test"
        assert loaded.genetic.population_size == 7
        assert abs(loaded.distillation.temperature - 3.5) < 1e-6

    def test_lora_dict_includes_modules_to_save(self, tmp_path):
        cfg = GenesisConfig(
            output_dir=str(tmp_path / "out"),
            checkpoint_dir=str(tmp_path / "ckpt"),
            log_dir=str(tmp_path / "logs"),
        )
        lora_dict = cfg.to_dict()["lora"]
        assert "modules_to_save" in lora_dict


# â”€â”€ HardwareConfig â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestHardwareConfig:

    def test_default_construction(self):
        hw = HardwareConfig()
        assert hw.teacher_device is not None
        assert hw.student_device is not None

    def test_memory_summary_returns_string(self):
        hw = HardwareConfig()
        summary = hw.memory_summary()
        assert isinstance(summary, str)
        assert len(summary) > 0

    def test_cpu_fallback_when_no_cuda(self):
        """If CUDA is unavailable, devices must fall back to cpu."""
        if torch.cuda.is_available():
            pytest.skip("CUDA is available â€” fallback not triggered")
        hw = HardwareConfig(teacher_device="cuda:0", student_device="cuda:1")
        assert "cpu" in hw.teacher_device or "cpu" in hw.student_device

    def test_explicit_cpu_devices(self):
        hw = HardwareConfig(teacher_device="cpu", student_device="cpu")
        assert hw.teacher_device == "cpu"
        assert hw.student_device == "cpu"
```

<a id="tests-test_datasets-py"></a>

#### `tests/test_datasets.py`
*15198 bytes Â· ~3,615 tokens*

```python
"""Tests for genesis/data/datasets.py and preprocessing.py."""

import os
import tempfile
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest
import torch
import numpy as np
from torch.utils.data import DataLoader

from genesis.data.preprocessing import (
    TextPreprocessor,
    AudioPreprocessor,
    pad_sequence,
    create_attention_mask,
)


# â”€â”€ TextPreprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestTextPreprocessor:

    def test_lowercase(self):
        p = TextPreprocessor(lowercase=True)
        assert p.preprocess("Hello World") == "hello world"

    def test_no_lowercase_preserves_case(self):
        p = TextPreprocessor(lowercase=False)
        assert p.preprocess("Hello World") == "Hello World"

    def test_normalize_whitespace_collapses_spaces(self):
        p = TextPreprocessor(normalize_whitespace=True)
        result = p.preprocess("hello   world")
        assert result == "hello world"

    def test_normalize_whitespace_removes_newlines(self):
        p = TextPreprocessor(normalize_whitespace=True)
        result = p.preprocess("hello\nworld")
        assert "\n" not in result

    def test_normalize_whitespace_removes_tabs(self):
        p = TextPreprocessor(normalize_whitespace=True)
        result = p.preprocess("hello\tworld")
        assert "\t" not in result

    def test_remove_special_chars_removes_uncommon(self):
        # regex keeps a-zA-Z0-9 and .,!?;:'"-  â€” removes @, #, etc.
        p = TextPreprocessor(remove_special_chars=True)
        result = p.preprocess("Hello @World #2024")
        assert "@" not in result
        assert "#" not in result

    def test_remove_special_chars_keeps_allowed(self):
        p = TextPreprocessor(remove_special_chars=True)
        result = p.preprocess("Hello, World!")
        assert "," in result  # comma is in the allowed set
        assert "!" in result  # exclamation is in the allowed set

    def test_preprocess_returns_string(self):
        p = TextPreprocessor()
        result = p.preprocess("  some text  ")
        assert isinstance(result, str)

    def test_tokenize_no_tokenizer_raises(self):
        p = TextPreprocessor()
        with pytest.raises(ValueError, match="Tokenizer"):
            p.tokenize("hello")

    def test_tokenize_with_mock_tokenizer(self):
        mock_tok = MagicMock()
        mock_tok.return_value = {
            "input_ids": torch.tensor([[1, 2, 3]]),
            "attention_mask": torch.tensor([[1, 1, 1]]),
        }
        p = TextPreprocessor(tokenizer=mock_tok, max_length=16)
        result = p.tokenize("hello world")
        assert "input_ids" in result

    def test_batch_tokenize_no_tokenizer_raises(self):
        p = TextPreprocessor()
        with pytest.raises(ValueError, match="Tokenizer"):
            p.batch_tokenize(["hello", "world"])

    def test_batch_tokenize_with_tokenizer(self):
        mock_tok = MagicMock()
        mock_tok.return_value = {
            "input_ids": torch.randint(0, 100, (2, 16)),
            "attention_mask": torch.ones(2, 16, dtype=torch.long),
        }
        p = TextPreprocessor(tokenizer=mock_tok, max_length=16)
        result = p.batch_tokenize(["hello", "world"])
        assert "input_ids" in result

    def test_empty_string(self):
        p = TextPreprocessor()
        result = p.preprocess("")
        assert isinstance(result, str)

    def test_combined_lower_and_whitespace(self):
        p = TextPreprocessor(lowercase=True, normalize_whitespace=True)
        result = p.preprocess("  HELLO   WORLD  ")
        assert result == "hello world"

    def test_create_qa_input_requires_tokenizer(self):
        p = TextPreprocessor()
        with pytest.raises(ValueError, match="Tokenizer"):
            p.create_qa_input("Q?", "Context")

    def test_create_qa_input_with_tokenizer(self):
        mock_tok = MagicMock()
        mock_tok.return_value = {
            "input_ids": torch.randint(0, 100, (1, 16)),
            "attention_mask": torch.ones(1, 16, dtype=torch.long),
        }
        p = TextPreprocessor(tokenizer=mock_tok)
        result = p.create_qa_input("What?", "Context here", answer="Yes")
        assert "target_text" in result
        assert result["target_text"] == "Yes"


# â”€â”€ AudioPreprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestAudioPreprocessor:

    def test_default_init(self):
        proc = AudioPreprocessor()
        assert proc.sample_rate == 22050
        assert proc.n_mels == 80

    def test_custom_params(self):
        proc = AudioPreprocessor(sample_rate=16000, n_mels=40)
        assert proc.sample_rate == 16000
        assert proc.n_mels == 40

    def test_normalize_mel_range(self):
        proc = AudioPreprocessor()
        mel = torch.randn(80, 50)
        normed = proc.normalize_mel(mel)
        assert normed.min() >= 0.0 - 1e-6
        assert normed.max() <= 1.0 + 1e-6

    def test_normalize_mel_uniform_input(self):
        proc = AudioPreprocessor()
        mel = torch.ones(80, 50)
        normed = proc.normalize_mel(mel)
        # All same value â†’ normalized to 0 (or near 0 due to eps)
        assert normed.max().item() < 1e-3

    def test_denormalize_mel_restores_range(self):
        proc = AudioPreprocessor()
        mel = torch.rand(80, 50)
        denormed = proc.denormalize_mel(mel, mel_min=-100, mel_max=0)
        assert denormed.min().item() >= -100 - 1e-3
        assert denormed.max().item() <= 0 + 1e-3

    def test_fmax_default(self):
        proc = AudioPreprocessor(sample_rate=22050)
        assert proc.fmax == 22050 / 2

    def test_compute_mel_without_librosa_raises(self):
        proc = AudioPreprocessor()
        audio = np.random.randn(22050)
        # If librosa is not installed, should raise ImportError or ModuleNotFoundError
        try:
            import librosa
            # If librosa is installed, test passes without error
        except ImportError:
            with pytest.raises((ImportError, ModuleNotFoundError)):
                proc.compute_mel_spectrogram(audio)


# â”€â”€ pad_sequence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestPadSequence:

    def test_1d_sequences_padded_to_max(self):
        seqs = [torch.tensor([1, 2, 3]), torch.tensor([4, 5])]
        padded = pad_sequence(seqs)
        assert padded.shape == (2, 3)
        assert padded[1, 2] == 0.0  # padded with zeros

    def test_custom_padding_value(self):
        seqs = [torch.tensor([1.0, 2.0]), torch.tensor([3.0])]
        padded = pad_sequence(seqs, padding_value=-1.0)
        assert padded[1, 1] == -1.0

    def test_max_length_truncates(self):
        seqs = [torch.tensor([1, 2, 3, 4, 5]), torch.tensor([6, 7])]
        padded = pad_sequence(seqs, max_length=3)
        assert padded.shape == (2, 3)

    def test_2d_sequences(self):
        seqs = [torch.randn(4, 10), torch.randn(4, 7)]
        padded = pad_sequence(seqs)
        assert padded.shape == (2, 4, 10)

    def test_same_length_no_padding(self):
        seqs = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6])]
        padded = pad_sequence(seqs)
        assert padded.shape == (2, 3)
        assert torch.equal(padded[0], torch.tensor([1, 2, 3]))


# â”€â”€ create_attention_mask â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestCreateAttentionMask:

    def test_no_padding(self):
        ids = torch.tensor([[1, 2, 3, 4]])
        mask = create_attention_mask(ids, pad_token_id=0)
        assert torch.equal(mask, torch.ones(1, 4, dtype=torch.long))

    def test_with_padding(self):
        ids = torch.tensor([[1, 2, 0, 0]])
        mask = create_attention_mask(ids, pad_token_id=0)
        expected = torch.tensor([[1, 1, 0, 0]], dtype=torch.long)
        assert torch.equal(mask, expected)

    def test_all_padding(self):
        ids = torch.tensor([[0, 0, 0]])
        mask = create_attention_mask(ids, pad_token_id=0)
        assert mask.sum() == 0

    def test_custom_pad_token_id(self):
        ids = torch.tensor([[1, 2, -1, -1]])
        mask = create_attention_mask(ids, pad_token_id=-1)
        expected = torch.tensor([[1, 1, 0, 0]], dtype=torch.long)
        assert torch.equal(mask, expected)

    def test_batch_dimension(self):
        ids = torch.tensor([[1, 0], [1, 2]])
        mask = create_attention_mask(ids)
        assert mask.shape == (2, 2)
        assert mask[0, 1] == 0
        assert mask[1, 1] == 1


# â”€â”€ TTSDataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestTTSDataset:

    def test_empty_directory(self, tmp_path):
        from genesis.data.datasets import TTSDataset
        dataset = TTSDataset(data_dir=str(tmp_path))
        assert len(dataset) == 0

    def test_loads_from_metadata_file(self, tmp_path):
        from genesis.data.datasets import TTSDataset
        meta = tmp_path / "metadata.csv"
        meta.write_text("audio1.wav|Hello world\naudio2.wav|Goodbye world\n")
        (tmp_path / "audio1.wav").write_bytes(b"\x00" * 100)
        (tmp_path / "audio2.wav").write_bytes(b"\x00" * 100)

        dataset = TTSDataset(data_dir=str(tmp_path))
        assert len(dataset) == 2

    def test_max_samples_respected(self, tmp_path):
        from genesis.data.datasets import TTSDataset
        meta = tmp_path / "metadata.csv"
        meta.write_text("a.wav|t1\nb.wav|t2\nc.wav|t3\n")
        dataset = TTSDataset(data_dir=str(tmp_path), max_samples=2)
        assert len(dataset) == 2

    def test_scans_directory_for_audio_files(self, tmp_path):
        from genesis.data.datasets import TTSDataset
        (tmp_path / "clip1.wav").write_bytes(b"\x00" * 100)
        (tmp_path / "clip2.wav").write_bytes(b"\x00" * 100)
        (tmp_path / "notes.txt").write_text("not audio")

        dataset = TTSDataset(data_dir=str(tmp_path))
        assert len(dataset) == 2

    def test_text_set_from_metadata(self, tmp_path):
        from genesis.data.datasets import TTSDataset
        meta = tmp_path / "metadata.csv"
        meta.write_text("audio1.wav|Hello world\n")
        (tmp_path / "audio1.wav").write_bytes(b"\x00" * 100)

        dataset = TTSDataset(data_dir=str(tmp_path))
        assert dataset._samples[0]["text"] == "Hello world"


# â”€â”€ DatasetLoader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestDatasetLoader:

    def test_init_stores_params(self):
        from genesis.data.datasets import DatasetLoader
        loader = DatasetLoader(
            dataset_name="test_dataset",
            max_length=256,
            split="validation",
            max_samples=100,
        )
        assert loader.dataset_name == "test_dataset"
        assert loader.max_length == 256
        assert loader.split == "validation"
        assert loader.max_samples == 100

    @patch("genesis.data.datasets.load_dataset")
    def test_load_calls_huggingface(self, mock_load):
        from genesis.data.datasets import DatasetLoader
        mock_ds = MagicMock()
        mock_ds.__len__.return_value = 50
        mock_ds.select.return_value = mock_ds
        mock_load.return_value = mock_ds

        loader = DatasetLoader(dataset_name="wikitext", max_samples=10)
        loader.load()
        mock_load.assert_called_once_with("wikitext", split="train", cache_dir=None)

    @patch("genesis.data.datasets.load_dataset")
    def test_load_applies_max_samples(self, mock_load):
        from genesis.data.datasets import DatasetLoader
        mock_ds = MagicMock()
        mock_ds.__len__.return_value = 100
        mock_ds.select.return_value = mock_ds
        mock_load.return_value = mock_ds

        loader = DatasetLoader(dataset_name="wikitext", max_samples=20)
        loader.load()
        mock_ds.select.assert_called_once()

    @patch("genesis.data.datasets.load_dataset")
    def test_load_no_max_samples(self, mock_load):
        from genesis.data.datasets import DatasetLoader
        mock_ds = MagicMock()
        mock_ds.__len__.return_value = 100
        mock_load.return_value = mock_ds

        loader = DatasetLoader(dataset_name="wikitext")
        loader.load()
        mock_ds.select.assert_not_called()

    def test_default_collate_raises_without_tokenizer(self):
        from genesis.data.datasets import DatasetLoader
        loader = DatasetLoader("test")
        with pytest.raises(ValueError, match="Tokenizer"):
            loader._default_collate([{"text": "hello"}])

    def test_default_collate_with_text_field(self):
        from genesis.data.datasets import DatasetLoader
        mock_tok = MagicMock()
        mock_tok.return_value = {
            "input_ids": torch.randint(0, 100, (2, 16)),
            "attention_mask": torch.ones(2, 16, dtype=torch.long),
        }
        loader = DatasetLoader("test", tokenizer=mock_tok)
        batch = [{"text": "hello"}, {"text": "world"}]
        result = loader._default_collate(batch)
        assert "input_ids" in result
        assert "attention_mask" in result
        assert "labels" in result

    def test_default_collate_question_context_format(self):
        from genesis.data.datasets import DatasetLoader
        mock_tok = MagicMock()
        mock_tok.return_value = {
            "input_ids": torch.randint(0, 100, (1, 16)),
            "attention_mask": torch.ones(1, 16, dtype=torch.long),
        }
        loader = DatasetLoader("test", tokenizer=mock_tok)
        batch = [{"question": "What is 2+2?", "context": "math basics"}]
        result = loader._default_collate(batch)
        assert "input_ids" in result

    def test_default_collate_fallback_format(self):
        from genesis.data.datasets import DatasetLoader
        mock_tok = MagicMock()
        mock_tok.return_value = {
            "input_ids": torch.randint(0, 100, (1, 16)),
            "attention_mask": torch.ones(1, 16, dtype=torch.long),
        }
        loader = DatasetLoader("test", tokenizer=mock_tok)
        batch = [{"answer": "Paris", "category": "geography"}]
        result = loader._default_collate(batch)
        assert "input_ids" in result

    @patch("genesis.data.datasets.load_dataset")
    def test_load_sets_dataset(self, mock_load):
        from genesis.data.datasets import DatasetLoader
        mock_ds = MagicMock()
        mock_ds.__len__.return_value = 5
        mock_load.return_value = mock_ds

        loader = DatasetLoader("wikitext")
        assert loader._dataset is None
        loader.load()
        assert loader._dataset is not None
```

<a id="tests-test_distillation-py"></a>

#### `tests/test_distillation.py`
*7860 bytes Â· ~1,965 tokens*

```python
"""Tests for distillation module."""

import pytest
import torch
import torch.nn as nn

from genesis.distillation.kd_loss import (
    kl_divergence_loss,
    soft_target_loss,
    feature_distillation_loss,
    KDLoss,
    ProgressiveKDLoss,
)


class TestKLDivergenceLoss:
    """Tests for KL divergence loss."""

    def test_kl_divergence_basic(self):
        """Test basic KL divergence computation."""
        student_logits = torch.randn(2, 10, 100)  # batch, seq, vocab
        teacher_logits = torch.randn(2, 10, 100)

        loss = kl_divergence_loss(student_logits, teacher_logits, temperature=4.0)

        assert loss.dim() == 0  # Scalar
        assert loss >= 0  # KL divergence is non-negative

    def test_kl_divergence_same_logits(self):
        """Test KL divergence with identical logits."""
        logits = torch.randn(2, 10, 100)

        loss = kl_divergence_loss(logits, logits.clone(), temperature=1.0)

        # KL divergence should be ~0 for identical distributions
        assert loss < 1e-5

    def test_kl_divergence_temperature_scaling(self):
        """Test that higher temperature produces different loss."""
        student_logits = torch.randn(2, 10, 100)
        teacher_logits = torch.randn(2, 10, 100)

        loss_t1 = kl_divergence_loss(student_logits, teacher_logits, temperature=1.0)
        loss_t4 = kl_divergence_loss(student_logits, teacher_logits, temperature=4.0)

        # Losses should be different with different temperatures
        assert not torch.allclose(loss_t1, loss_t4)


class TestSoftTargetLoss:
    """Tests for soft target loss."""

    def test_soft_target_basic(self):
        """Test basic soft target loss."""
        student_logits = torch.randn(2, 10, 100)
        teacher_logits = torch.randn(2, 10, 100)

        loss = soft_target_loss(student_logits, teacher_logits, temperature=4.0)

        assert loss.dim() == 0
        assert loss >= 0

    def test_soft_target_with_mask(self):
        """Test soft target loss with attention mask."""
        student_logits = torch.randn(2, 10, 100)
        teacher_logits = torch.randn(2, 10, 100)
        mask = torch.ones(2, 10)
        mask[:, 5:] = 0  # Mask out last 5 positions

        loss = soft_target_loss(
            student_logits, teacher_logits, temperature=4.0, attention_mask=mask
        )

        assert loss.dim() == 0


class TestFeatureDistillationLoss:
    """Tests for feature distillation loss."""

    def test_feature_loss_mse(self):
        """Test MSE feature distillation loss."""
        student_hidden = torch.randn(2, 10, 256)
        teacher_hidden = torch.randn(2, 10, 256)

        loss = feature_distillation_loss(
            student_hidden, teacher_hidden, loss_type="mse"
        )

        assert loss.dim() == 0
        assert loss >= 0

    def test_feature_loss_cosine(self):
        """Test cosine feature distillation loss."""
        student_hidden = torch.randn(2, 10, 256)
        teacher_hidden = torch.randn(2, 10, 256)

        loss = feature_distillation_loss(
            student_hidden, teacher_hidden, loss_type="cosine"
        )

        assert loss.dim() == 0
        assert 0 <= loss <= 2  # Cosine loss range

    def test_feature_loss_with_projection(self):
        """Test feature loss with dimension projection."""
        student_hidden = torch.randn(2, 10, 128)  # Smaller dimension
        teacher_hidden = torch.randn(2, 10, 256)  # Larger dimension

        projection = nn.Linear(128, 256)

        loss = feature_distillation_loss(
            student_hidden, teacher_hidden, projection=projection
        )

        assert loss.dim() == 0


class TestKDLoss:
    """Tests for KDLoss class."""

    def test_kdloss_basic(self):
        """Test basic KDLoss forward pass."""
        kd_loss = KDLoss(temperature=4.0, alpha=0.5)

        student_logits = torch.randn(2, 10, 100)
        teacher_logits = torch.randn(2, 10, 100)

        losses = kd_loss(student_logits, teacher_logits)

        assert "total_loss" in losses
        assert "kd_loss" in losses

    def test_kdloss_with_hard_labels(self):
        """Test KDLoss with hard labels."""
        kd_loss = KDLoss(temperature=4.0, alpha=0.5)

        student_logits = torch.randn(2, 10, 100)
        teacher_logits = torch.randn(2, 10, 100)
        labels = torch.randint(0, 100, (2, 10))

        losses = kd_loss(student_logits, teacher_logits, hard_labels=labels)

        assert "hard_loss" in losses
        assert "total_loss" in losses

    def test_kdloss_with_feature_distillation(self):
        """Test KDLoss with feature distillation."""
        kd_loss = KDLoss(
            temperature=4.0,
            alpha=0.5,
            use_feature_distillation=True,
            feature_weight=0.1,
            feature_layers=[-1],
        )

        student_logits = torch.randn(2, 10, 100)
        teacher_logits = torch.randn(2, 10, 100)
        student_hidden = (torch.randn(2, 10, 256),)
        teacher_hidden = (torch.randn(2, 10, 256),)

        losses = kd_loss(
            student_logits,
            teacher_logits,
            student_hidden_states=student_hidden,
            teacher_hidden_states=teacher_hidden,
        )

        assert "feature_loss" in losses

    def test_kdloss_alpha_weighting(self):
        """Test that alpha correctly weights losses."""
        # All KD loss
        kd_loss_all_kd = KDLoss(alpha=1.0)
        # All hard loss
        kd_loss_all_hard = KDLoss(alpha=0.0)

        student_logits = torch.randn(2, 10, 100)
        teacher_logits = torch.randn(2, 10, 100)
        labels = torch.randint(0, 100, (2, 10))

        losses_all_kd = kd_loss_all_kd(
            student_logits, teacher_logits, hard_labels=labels
        )
        losses_all_hard = kd_loss_all_hard(
            student_logits, teacher_logits, hard_labels=labels
        )

        # Total loss should equal KD loss when alpha=1
        assert torch.allclose(
            losses_all_kd["total_loss"], losses_all_kd["kd_loss"], atol=1e-5
        )


class TestProgressiveKDLoss:
    """Tests for ProgressiveKDLoss."""

    def test_progressive_initialization(self):
        """Test progressive KD loss initialization."""
        loss = ProgressiveKDLoss(
            initial_temperature=10.0,
            final_temperature=1.0,
            initial_alpha=0.9,
            final_alpha=0.1,
            total_steps=100,
        )

        assert loss.temperature == 10.0
        assert loss.alpha == 0.9

    def test_progressive_step(self):
        """Test progressive stepping."""
        loss = ProgressiveKDLoss(
            initial_temperature=10.0,
            final_temperature=1.0,
            initial_alpha=0.9,
            final_alpha=0.1,
            total_steps=100,
        )

        initial_temp = loss.temperature
        initial_alpha = loss.alpha

        for _ in range(50):
            loss.step()

        # Should be halfway through progression
        assert loss.temperature < initial_temp
        assert loss.alpha < initial_alpha

    def test_progressive_full_progression(self):
        """Test full progression to final values."""
        loss = ProgressiveKDLoss(
            initial_temperature=10.0,
            final_temperature=1.0,
            initial_alpha=0.9,
            final_alpha=0.1,
            total_steps=100,
        )

        for _ in range(100):
            loss.step()

        assert abs(loss.temperature - 1.0) < 0.1
        assert abs(loss.alpha - 0.1) < 0.1

    def test_progressive_reset(self):
        """Test progressive reset."""
        loss = ProgressiveKDLoss(
            initial_temperature=10.0,
            final_temperature=1.0,
            total_steps=100,
        )

        for _ in range(50):
            loss.step()

        loss.reset()

        assert loss.temperature == 10.0
        assert loss._current_step == 0
```

<a id="tests-test_distillation_trainer-py"></a>

#### `tests/test_distillation_trainer.py`
*12209 bytes Â· ~2,867 tokens*

```python
"""Tests for DistillationTrainer training loop."""

import os
import pytest
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

from genesis.distillation.trainer import DistillationTrainer, TrainingConfig
from genesis.distillation.kd_loss import KDLoss


# â”€â”€ Mock teacher / student â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

VOCAB = 50
SEQ = 8
HIDDEN = 32


class _StudentNN(nn.Module):
    """Underlying nn.Module for the mock student."""

    def __init__(self):
        super().__init__()
        self.embed = nn.Embedding(VOCAB, HIDDEN)
        self.proj = nn.Linear(HIDDEN, VOCAB)


class MockStudent:
    """Thin wrapper that matches the StudentModel interface used by the trainer."""

    def __init__(self):
        self._model = _StudentNN()
        self.device = "cpu"

    @property
    def model(self) -> nn.Module:
        return self._model

    def forward(self, input_ids, attention_mask=None, output_hidden_states=False,
                labels=None, **_):
        input_ids = input_ids.to(self.device)
        hidden = self._model.embed(input_ids)
        logits = self._model.proj(hidden)
        result: dict = {"logits": logits}
        if output_hidden_states:
            result["hidden_states"] = (hidden,)
        return result

    def save(self, path: str) -> None:
        os.makedirs(path, exist_ok=True)
        torch.save(self._model.state_dict(), os.path.join(path, "model.pt"))


class MockTeacher:
    """Thin wrapper that matches the TeacherModel interface used by the trainer."""

    def __init__(self):
        self.model = nn.Linear(1, 1)  # only needs .eval()

    def forward(self, input_ids, attention_mask=None, output_hidden_states=False,
                **_):
        batch, seq = input_ids.shape[:2]
        logits = torch.randn(batch, seq, VOCAB)
        result: dict = {"logits": logits}
        if output_hidden_states:
            result["hidden_states"] = (torch.randn(batch, seq, HIDDEN),)
        return result


def _make_dataloader(num_samples: int = 8, batch_size: int = 4) -> DataLoader:
    ids = torch.randint(0, VOCAB, (num_samples, SEQ))
    mask = torch.ones_like(ids)
    ds = TensorDataset(ids, mask, ids.clone())  # labels = input_ids

    def collate(batch):
        ids_b, mask_b, labels_b = zip(*batch)
        return {
            "input_ids": torch.stack(ids_b),
            "attention_mask": torch.stack(mask_b),
            "labels": torch.stack(labels_b),
        }

    return DataLoader(ds, batch_size=batch_size, collate_fn=collate)


def _fp32_config(**kwargs) -> TrainingConfig:
    """TrainingConfig with fp32 mixed precision (avoids GradScaler on CPU)."""
    defaults = dict(
        learning_rate=1e-3,
        max_steps=4,
        warmup_steps=1,
        gradient_accumulation_steps=1,
        logging_steps=2,
        eval_steps=4,
        mixed_precision="fp32",
    )
    defaults.update(kwargs)
    return TrainingConfig(**defaults)


# â”€â”€ Initialisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestDistillationTrainerInit:

    def test_creates_optimizer(self):
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=_fp32_config(),
        )
        assert trainer.optimizer is not None

    def test_creates_scheduler(self):
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=_fp32_config(),
        )
        assert trainer.scheduler is not None

    def test_scaler_none_for_fp32(self):
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=_fp32_config(mixed_precision="fp32"),
        )
        assert trainer.scaler is None

    def test_default_kd_loss_created(self):
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=_fp32_config(),
        )
        assert isinstance(trainer.kd_loss, KDLoss)

    def test_custom_kd_loss_accepted(self):
        custom_loss = KDLoss(temperature=2.0, alpha=0.3)
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=_fp32_config(),
            kd_loss=custom_loss,
        )
        assert trainer.kd_loss is custom_loss

    def test_initial_global_step_zero(self):
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=_fp32_config(),
        )
        assert trainer.global_step == 0


# â”€â”€ Training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestDistillationTrainerLoop:

    def _trainer(self, **cfg_kwargs):
        return DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(num_samples=16, batch_size=4),
            config=_fp32_config(**cfg_kwargs),
        )

    def test_train_advances_global_step(self):
        trainer = self._trainer(max_steps=4, gradient_accumulation_steps=1)
        results = trainer.train()
        assert results["global_step"] == 4

    def test_train_returns_dict(self):
        trainer = self._trainer(max_steps=2)
        results = trainer.train()
        assert "global_step" in results
        assert "best_eval_loss" in results
        assert "training_logs" in results

    def test_train_logs_are_recorded(self):
        trainer = self._trainer(max_steps=4, logging_steps=2,
                                gradient_accumulation_steps=1)
        results = trainer.train()
        # Should have logged at steps 2 and 4
        assert len(results["training_logs"]) == 2

    def test_log_entry_has_required_keys(self):
        trainer = self._trainer(max_steps=2, logging_steps=1,
                                gradient_accumulation_steps=1)
        results = trainer.train()
        log = results["training_logs"][0]
        assert "step" in log
        assert "loss" in log
        assert "lr" in log

    def test_logged_loss_is_finite(self):
        trainer = self._trainer(max_steps=4, logging_steps=2,
                                gradient_accumulation_steps=1)
        results = trainer.train()
        for entry in results["training_logs"]:
            assert torch.isfinite(torch.tensor(entry["loss"]))

    def test_gradient_accumulation_reduces_steps(self):
        trainer = self._trainer(max_steps=4, gradient_accumulation_steps=2)
        results = trainer.train()
        # 4 optimizer steps â€” each needs 2 forward passes
        assert results["global_step"] == 4

    def test_num_steps_override(self):
        trainer = self._trainer(max_steps=100)
        results = trainer.train(num_steps=3)
        assert results["global_step"] == 3

    def test_callback_is_called(self):
        calls = []
        trainer = self._trainer(max_steps=3, gradient_accumulation_steps=1)
        trainer.train(callback=lambda r: calls.append(r))
        assert len(calls) == 3


# â”€â”€ Log window loss correctness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestLogWindowLoss:
    """
    Verify the accumulated_loss fix: the logged average must reflect only the
    logging window, not all steps since training started.
    """

    def test_log_loss_resets_between_windows(self):
        """Log entries at different windows must differ (not ever-growing)."""
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(num_samples=32, batch_size=4),
            config=_fp32_config(
                max_steps=6,
                logging_steps=2,
                gradient_accumulation_steps=1,
            ),
        )
        results = trainer.train()
        logs = results["training_logs"]
        assert len(logs) == 3

        # If loss were accumulating without reset, each entry's loss would be
        # monotonically larger (roughly). Check that they are distinct and not
        # simply N Ã— previous.
        losses = [e["loss"] for e in logs]
        # The second loss should not be â‰ˆ 2Ã— the first (accumulation symptom)
        assert not (abs(losses[1] - 2 * losses[0]) < 0.01 * losses[0])


# â”€â”€ Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestDistillationTrainerEval:

    def test_evaluate_returns_loss(self):
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            eval_dataloader=_make_dataloader(num_samples=8, batch_size=4),
            config=_fp32_config(max_steps=1),
        )
        results = trainer.evaluate()
        assert "loss" in results
        assert results["loss"] > 0

    def test_evaluate_empty_without_dataloader(self):
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=_fp32_config(),
        )
        results = trainer.evaluate()
        assert results == {}

    def test_get_training_state(self):
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=_fp32_config(max_steps=2),
        )
        trainer.train()
        state = trainer.get_training_state()
        assert "global_step" in state
        assert "epoch" in state
        assert "best_eval_loss" in state
        assert "current_lr" in state


# â”€â”€ Checkpoint save / load â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestDistillationTrainerCheckpoint:

    def test_save_checkpoint(self, tmp_path):
        cfg = _fp32_config(max_steps=3, warmup_steps=1, output_dir=str(tmp_path))
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=cfg,
        )
        trainer.train()
        trainer._save_checkpoint("test")
        ckpt_dir = tmp_path / "checkpoint-test"
        assert (ckpt_dir / "training_state.pt").exists()

    def test_load_checkpoint_restores_step(self, tmp_path):
        cfg = _fp32_config(max_steps=3, output_dir=str(tmp_path),
                           gradient_accumulation_steps=1)
        trainer = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(num_samples=16),
            config=cfg,
        )
        trainer.train()
        trainer._save_checkpoint("restore")

        # Create fresh trainer and load the checkpoint
        trainer2 = DistillationTrainer(
            teacher=MockTeacher(),
            student=MockStudent(),
            train_dataloader=_make_dataloader(),
            config=_fp32_config(output_dir=str(tmp_path)),
        )
        trainer2.load_checkpoint(str(tmp_path / "checkpoint-restore"))
        assert trainer2.global_step == 3
```

<a id="tests-test_fitness-py"></a>

#### `tests/test_fitness.py`
*14419 bytes Â· ~3,603 tokens*

```python
"""Tests for fitness evaluation module."""

import pytest
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

from genesis.core.fitness import (
    FitnessEvaluator,
    FitnessResult,
    PerplexityFitness,
    AccuracyFitness,
    QAFitness,
    CompositeFitness,
    CustomFitness,
    create_fitness_evaluator,
)


class DummyModel(nn.Module):
    """Simple dummy model for testing."""

    def __init__(self, vocab_size=100, hidden_size=64):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.linear = nn.Linear(hidden_size, vocab_size)
        self.vocab_size = vocab_size

    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):
        hidden = self.embedding(input_ids)
        logits = self.linear(hidden)

        loss = None
        if labels is not None:
            loss = nn.functional.cross_entropy(
                logits.view(-1, self.vocab_size),
                labels.view(-1),
                ignore_index=-100,
            )

        class Output:
            pass

        output = Output()
        output.logits = logits
        output.loss = loss

        return output


def create_dummy_dataloader(batch_size=4, seq_len=10, vocab_size=100, num_batches=2):
    """Create a dummy dataloader for testing."""
    input_ids = torch.randint(0, vocab_size, (batch_size * num_batches, seq_len))
    attention_mask = torch.ones_like(input_ids)
    labels = input_ids.clone()

    dataset = TensorDataset(input_ids, attention_mask, labels)
    return DataLoader(dataset, batch_size=batch_size)


class TestFitnessResult:
    """Tests for FitnessResult dataclass."""

    def test_fitness_result_creation(self):
        """Test FitnessResult creation."""
        result = FitnessResult(
            score=0.85,
            metrics={"accuracy": 0.85, "loss": 0.5},
        )

        assert result.score == 0.85
        assert result.metrics["accuracy"] == 0.85

    def test_fitness_result_float_conversion(self):
        """Test FitnessResult float conversion."""
        result = FitnessResult(score=0.75)

        assert float(result) == 0.75


class TestPerplexityFitness:
    """Tests for PerplexityFitness evaluator."""

    def test_perplexity_fitness_evaluation(self):
        """Test perplexity fitness evaluation."""
        model = DummyModel()
        dataloader = create_dummy_dataloader()

        # Wrap dataloader to return dicts
        def dict_dataloader():
            for batch in dataloader:
                yield {
                    "input_ids": batch[0],
                    "attention_mask": batch[1],
                    "labels": batch[2],
                }

        evaluator = PerplexityFitness(
            dataloader=list(dict_dataloader()),
            device="cpu",
            max_samples=10,
        )

        result = evaluator.evaluate(model)

        assert isinstance(result, FitnessResult)
        assert 0 <= result.score <= 1  # Fitness should be bounded
        assert "perplexity" in result.metrics
        assert "loss" in result.metrics

    def test_perplexity_fitness_lower_perplexity_higher_fitness(self):
        """Test that lower perplexity gives higher fitness."""
        # Use labels that are all class-0 so we can control which model wins.
        dataloader = [
            {
                "input_ids": torch.zeros(4, 10, dtype=torch.long),
                "attention_mask": torch.ones(4, 10, dtype=torch.long),
                "labels": torch.zeros(4, 10, dtype=torch.long),  # all token 0
            }
        ]

        # good_model: zero embeddings + bias strongly predicts class 0 â†’ low loss
        good_model = DummyModel()
        good_model.embedding.weight.data.zero_()
        good_model.linear.weight.data.zero_()
        good_model.linear.bias.data.zero_()
        good_model.linear.bias.data[0] = 20.0  # always predicts class 0

        # bad_model: same setup but strongly predicts class 1 (wrong) â†’ high loss
        bad_model = DummyModel()
        bad_model.embedding.weight.data.zero_()
        bad_model.linear.weight.data.zero_()
        bad_model.linear.bias.data.zero_()
        bad_model.linear.bias.data[1] = 20.0  # always predicts class 1

        good_result = PerplexityFitness(dataloader=dataloader, device="cpu").evaluate(good_model)
        bad_result = PerplexityFitness(dataloader=dataloader, device="cpu").evaluate(bad_model)

        assert good_result.score > bad_result.score


class TestAccuracyFitness:
    """Tests for AccuracyFitness evaluator."""

    def test_accuracy_fitness_evaluation(self):
        """Test accuracy fitness evaluation."""

        class ClassificationModel(nn.Module):
            def __init__(self, num_classes=10):
                super().__init__()
                self.linear = nn.Linear(32, num_classes)

            def forward(self, input_ids, attention_mask=None, **kwargs):
                # Dummy: just use input_ids sum as features
                features = input_ids.float().mean(dim=-1, keepdim=True).expand(-1, 32)
                logits = self.linear(features)

                class Output:
                    pass

                output = Output()
                output.logits = logits
                return output

        model = ClassificationModel()

        # Create dataloader with labels
        input_ids = torch.randint(0, 100, (20, 10))
        labels = torch.randint(0, 10, (20,))

        def dict_dataloader():
            for i in range(0, 20, 4):
                yield {
                    "input_ids": input_ids[i : i + 4],
                    "attention_mask": torch.ones(4, 10),
                    "labels": labels[i : i + 4],
                }

        evaluator = AccuracyFitness(
            dataloader=list(dict_dataloader()),
            device="cpu",
        )

        result = evaluator.evaluate(model)

        assert isinstance(result, FitnessResult)
        assert 0 <= result.score <= 1
        assert "accuracy" in result.metrics


class TestCompositeFitness:
    """Tests for CompositeFitness evaluator."""

    def test_composite_fitness(self):
        """Test composite fitness evaluation."""

        class DummyEvaluator(FitnessEvaluator):
            def __init__(self, fixed_score):
                self.fixed_score = fixed_score

            def evaluate(self, model, state_dict=None):
                return FitnessResult(score=self.fixed_score)

        eval1 = DummyEvaluator(0.8)
        eval2 = DummyEvaluator(0.6)

        composite = CompositeFitness(
            evaluators=[eval1, eval2],
            weights=[0.5, 0.5],
        )

        result = composite.evaluate(nn.Linear(1, 1))

        # Weighted average: 0.5 * 0.8 + 0.5 * 0.6 = 0.7
        assert abs(result.score - 0.7) < 1e-6

    def test_composite_fitness_unequal_weights(self):
        """Test composite fitness with unequal weights."""

        class DummyEvaluator(FitnessEvaluator):
            def __init__(self, fixed_score):
                self.fixed_score = fixed_score

            def evaluate(self, model, state_dict=None):
                return FitnessResult(score=self.fixed_score)

        eval1 = DummyEvaluator(1.0)
        eval2 = DummyEvaluator(0.0)

        composite = CompositeFitness(
            evaluators=[eval1, eval2],
            weights=[0.75, 0.25],
        )

        result = composite.evaluate(nn.Linear(1, 1))

        # Weighted average: 0.75 * 1.0 + 0.25 * 0.0 = 0.75
        assert abs(result.score - 0.75) < 1e-6


class TestCustomFitness:
    """Tests for CustomFitness wrapper."""

    def test_custom_fitness(self):
        """Test custom fitness function wrapper."""

        def my_fitness(model):
            # Count parameters as a dummy fitness
            return sum(p.numel() for p in model.parameters()) / 1000

        evaluator = CustomFitness(my_fitness)
        model = nn.Linear(10, 5)  # 55 parameters

        result = evaluator.evaluate(model)

        assert result.score == 0.055  # 55 / 1000

    def test_custom_fitness_with_state_dict(self):
        """Test custom fitness with state dict loading."""

        def my_fitness(model):
            return model.weight.mean().item()

        evaluator = CustomFitness(my_fitness)
        model = nn.Linear(5, 5, bias=False)

        state_dict = {"weight": torch.ones(5, 5) * 0.5}
        result = evaluator.evaluate(model, state_dict=state_dict)

        assert abs(result.score - 0.5) < 1e-6


class TestFitnessEvaluatorFactory:
    """Tests for fitness evaluator factory."""

    def test_create_perplexity_evaluator(self):
        """Test creating perplexity evaluator."""

        def dict_dataloader():
            yield {"input_ids": torch.randint(0, 100, (4, 10))}

        evaluator = create_fitness_evaluator(
            fitness_type="perplexity",
            dataloader=list(dict_dataloader()),
            device="cpu",
        )

        assert isinstance(evaluator, PerplexityFitness)

    def test_create_unknown_evaluator_raises(self):
        """Test that unknown type raises error."""
        with pytest.raises(ValueError):
            create_fitness_evaluator(
                fitness_type="unknown",
                dataloader=[],
            )


class TestQAFitness:
    """Tests for QAFitness evaluator."""

    def _make_model_with_generate(self, vocab_size=50, seq_len=5):
        """Create a model with a generate() method that returns fixed tokens."""
        class GenerateModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.embedding = nn.Embedding(vocab_size, 32)
                self.linear = nn.Linear(32, vocab_size)

            def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):
                h = self.embedding(input_ids)
                logits = self.linear(h)
                class Out: pass
                o = Out(); o.logits = logits; o.loss = None
                return o

            def generate(self, input_ids, attention_mask=None, max_new_tokens=10, **kwargs):
                # Always return the input unchanged (echo model)
                return input_ids

        return GenerateModel()

    def _make_tokenizer(self):
        mock_tok = type("Tok", (), {})()
        # batch_decode just returns the token IDs as strings
        mock_tok.batch_decode = lambda ids, **kw: [" ".join(str(t) for t in row.tolist()) for row in ids]
        return mock_tok

    def test_evaluate_returns_fitness_result(self):
        model = self._make_model_with_generate()
        tok = self._make_tokenizer()
        dataloader = [
            {
                "input_ids": torch.randint(0, 50, (2, 5)),
                "attention_mask": torch.ones(2, 5, dtype=torch.long),
                "target_text": ["yes", "no"],
            }
        ]
        evaluator = QAFitness(dataloader=dataloader, tokenizer=tok, device="cpu")
        result = evaluator.evaluate(model)
        assert isinstance(result, FitnessResult)
        assert 0.0 <= result.score <= 1.0

    def test_evaluate_metrics_keys(self):
        model = self._make_model_with_generate()
        tok = self._make_tokenizer()
        dataloader = [
            {
                "input_ids": torch.randint(0, 50, (1, 5)),
                "attention_mask": torch.ones(1, 5, dtype=torch.long),
                "target_text": ["yes"],
            }
        ]
        evaluator = QAFitness(dataloader=dataloader, tokenizer=tok, device="cpu")
        result = evaluator.evaluate(model)
        assert "exact_match" in result.metrics
        assert "f1" in result.metrics
        assert "samples_evaluated" in result.metrics

    def test_normalize_lowercases_and_strips(self):
        tok = self._make_tokenizer()
        eval_ = QAFitness(dataloader=[], tokenizer=tok, device="cpu")
        assert eval_._normalize("  HELLO World  ") == "hello world"

    def test_compute_f1_identical(self):
        tok = self._make_tokenizer()
        eval_ = QAFitness(dataloader=[], tokenizer=tok, device="cpu")
        assert eval_._compute_f1("cat sat", "cat sat") == pytest.approx(1.0)

    def test_compute_f1_no_overlap(self):
        tok = self._make_tokenizer()
        eval_ = QAFitness(dataloader=[], tokenizer=tok, device="cpu")
        assert eval_._compute_f1("hello world", "foo bar") == pytest.approx(0.0)

    def test_compute_f1_partial_overlap(self):
        tok = self._make_tokenizer()
        eval_ = QAFitness(dataloader=[], tokenizer=tok, device="cpu")
        f1 = eval_._compute_f1("cat sat on", "cat sat mat")
        assert 0.0 < f1 < 1.0

    def test_max_samples_limits_evaluation(self):
        model = self._make_model_with_generate()
        tok = self._make_tokenizer()
        # 3 batches of 2 each = 6 total samples
        dataloader = [
            {
                "input_ids": torch.randint(0, 50, (2, 5)),
                "attention_mask": torch.ones(2, 5, dtype=torch.long),
                "target_text": ["yes", "no"],
            }
        ] * 3
        evaluator = QAFitness(
            dataloader=dataloader,
            tokenizer=tok,
            device="cpu",
            max_samples=2,  # Only process 1 batch
        )
        result = evaluator.evaluate(model)
        assert result.metrics["samples_evaluated"] <= 2

    def test_exact_match_when_model_echoes_target(self):
        """If model echoes input tokens and those match target_text, EM > 0."""
        tok = self._make_tokenizer()

        class EchoModel(nn.Module):
            def forward(self, input_ids, **kwargs):
                class Out: pass
                o = Out(); o.logits = None; o.loss = None
                return o
            def generate(self, input_ids, **kwargs):
                return input_ids  # echo exactly

        # target_text must match what batch_decode would return for input_ids
        input_ids = torch.tensor([[1, 2, 3]])
        expected = tok.batch_decode(input_ids)[0]  # "1 2 3"
        dataloader = [
            {
                "input_ids": input_ids,
                "attention_mask": torch.ones(1, 3, dtype=torch.long),
                "target_text": [expected],
            }
        ]
        evaluator = QAFitness(dataloader=dataloader, tokenizer=tok, device="cpu")
        result = evaluator.evaluate(EchoModel())
        assert result.metrics["exact_match"] == pytest.approx(1.0)
```

<a id="tests-test_genetics-py"></a>

#### `tests/test_genetics.py`
*8010 bytes Â· ~2,002 tokens*

```python
"""Tests for genetics module."""

import pytest
import torch
import numpy as np

from genesis.core.genetics import (
    slerp,
    crossover,
    mutate,
    Genetics,
    blend_state_dicts,
)


class TestSLERP:
    """Tests for SLERP interpolation."""

    def test_slerp_basic(self):
        """Test basic SLERP interpolation."""
        v0 = torch.tensor([1.0, 0.0, 0.0])
        v1 = torch.tensor([0.0, 1.0, 0.0])

        # At t=0, should be close to v0
        result = slerp(0.0, v0, v1)
        assert torch.allclose(result, v0, atol=1e-5)

        # At t=1, should be close to v1
        result = slerp(1.0, v0, v1)
        assert torch.allclose(result, v1, atol=1e-5)

    def test_slerp_midpoint(self):
        """Test SLERP at midpoint."""
        v0 = torch.tensor([1.0, 0.0])
        v1 = torch.tensor([0.0, 1.0])

        result = slerp(0.5, v0, v1)

        # At midpoint, magnitude should be preserved
        expected_magnitude = (v0.norm() + v1.norm()) / 2
        assert abs(result.norm().item() - expected_magnitude) < 0.1

    def test_slerp_preserves_shape(self):
        """Test that SLERP preserves tensor shape."""
        v0 = torch.randn(10, 20)
        v1 = torch.randn(10, 20)

        result = slerp(0.5, v0, v1)
        assert result.shape == v0.shape

    def test_slerp_parallel_vectors(self):
        """Test SLERP with nearly parallel vectors."""
        v0 = torch.tensor([1.0, 0.0, 0.0])
        v1 = torch.tensor([1.0 + 1e-9, 0.0, 0.0])

        # Should not crash with parallel vectors
        result = slerp(0.5, v0, v1)
        assert result.shape == v0.shape


class TestCrossover:
    """Tests for crossover operations."""

    def test_crossover_slerp(self):
        """Test SLERP crossover method."""
        parent1 = {"weight": torch.randn(10, 10)}
        parent2 = {"weight": torch.randn(10, 10)}

        child = crossover(parent1, parent2, crossover_rate=1.0, method="slerp")

        assert "weight" in child
        assert child["weight"].shape == parent1["weight"].shape

    def test_crossover_uniform(self):
        """Test uniform crossover method."""
        parent1 = {"w1": torch.ones(5), "w2": torch.ones(5)}
        parent2 = {"w1": torch.zeros(5), "w2": torch.zeros(5)}

        child = crossover(parent1, parent2, crossover_rate=1.0, method="uniform")

        assert "w1" in child
        assert "w2" in child

    def test_crossover_single_point(self):
        """Test single-point crossover method."""
        parent1 = {"a": torch.ones(5), "b": torch.ones(5), "c": torch.ones(5)}
        parent2 = {"a": torch.zeros(5), "b": torch.zeros(5), "c": torch.zeros(5)}

        child = crossover(parent1, parent2, crossover_rate=1.0, method="single_point")

        assert len(child) == 3

    def test_crossover_no_crossover(self):
        """Test that crossover rate 0 returns copy of parent1."""
        parent1 = {"weight": torch.ones(5)}
        parent2 = {"weight": torch.zeros(5)}

        child = crossover(parent1, parent2, crossover_rate=0.0)

        assert torch.allclose(child["weight"], parent1["weight"])


class TestMutation:
    """Tests for mutation operations."""

    def test_mutate_gaussian(self):
        """Test Gaussian mutation."""
        state_dict = {"weight": torch.zeros(100)}

        mutated = mutate(
            state_dict,
            mutation_rate=1.0,
            mutation_scale=0.1,
            mutation_prob_per_weight=1.0,
            method="gaussian",
        )

        # With all weights mutated, should not be all zeros
        assert not torch.allclose(mutated["weight"], torch.zeros(100))

    def test_mutate_uniform(self):
        """Test uniform mutation."""
        state_dict = {"weight": torch.zeros(100)}

        mutated = mutate(
            state_dict,
            mutation_rate=1.0,
            mutation_scale=0.1,
            mutation_prob_per_weight=1.0,
            method="uniform",
        )

        assert not torch.allclose(mutated["weight"], torch.zeros(100))

    def test_mutate_adaptive(self):
        """Test adaptive mutation."""
        state_dict = {"weight": torch.ones(100)}

        mutated = mutate(
            state_dict,
            mutation_rate=1.0,
            mutation_scale=0.1,
            mutation_prob_per_weight=1.0,
            method="adaptive",
        )

        assert mutated["weight"].shape == state_dict["weight"].shape

    def test_mutate_no_mutation(self):
        """Test that mutation rate 0 returns copy."""
        state_dict = {"weight": torch.ones(5)}

        mutated = mutate(state_dict, mutation_rate=0.0)

        assert torch.allclose(mutated["weight"], state_dict["weight"])

    def test_mutate_preserves_shape(self):
        """Test that mutation preserves tensor shapes."""
        state_dict = {
            "layer1": torch.randn(10, 20),
            "layer2": torch.randn(20, 30),
        }

        mutated = mutate(state_dict, mutation_rate=1.0)

        assert mutated["layer1"].shape == state_dict["layer1"].shape
        assert mutated["layer2"].shape == state_dict["layer2"].shape


class TestGenetics:
    """Tests for Genetics class."""

    def test_genetics_initialization(self):
        """Test Genetics class initialization."""
        genetics = Genetics(
            crossover_rate=0.7,
            mutation_rate=0.1,
            adaptive_mutation=True,
        )

        assert genetics.crossover_rate == 0.7
        assert genetics.mutation_rate == 0.1
        assert genetics.generation == 0

    def test_genetics_create_offspring(self):
        """Test offspring creation."""
        genetics = Genetics()

        parent1 = {"weight": torch.randn(10)}
        parent2 = {"weight": torch.randn(10)}

        child = genetics.create_offspring(parent1, parent2)

        assert "weight" in child
        assert child["weight"].shape == parent1["weight"].shape

    def test_genetics_generation_step(self):
        """Test generation stepping."""
        genetics = Genetics(adaptive_mutation=True)

        initial_gen = genetics.generation
        genetics.step_generation()

        assert genetics.generation == initial_gen + 1

    def test_genetics_adaptive_mutation_decay(self):
        """Test that adaptive mutation decays correctly."""
        genetics = Genetics(
            mutation_rate=0.1,
            adaptive_mutation=True,
            mutation_decay=0.9,
            min_mutation_rate=0.01,
        )

        rate1 = genetics._get_current_mutation_rate()
        genetics.step_generation()
        rate2 = genetics._get_current_mutation_rate()

        assert rate2 < rate1

    def test_genetics_reset(self):
        """Test genetics reset."""
        genetics = Genetics()
        genetics.step_generation()
        genetics.step_generation()

        genetics.reset()

        assert genetics.generation == 0


class TestBlendStateDicts:
    """Tests for blend_state_dicts function."""

    def test_blend_two_dicts(self):
        """Test blending two state dicts."""
        sd1 = {"weight": torch.ones(5)}
        sd2 = {"weight": torch.zeros(5)}

        blended = blend_state_dicts([sd1, sd2])

        # Equal weights should give average
        expected = torch.full((5,), 0.5)
        assert torch.allclose(blended["weight"], expected)

    def test_blend_with_weights(self):
        """Test blending with custom weights."""
        sd1 = {"weight": torch.ones(5)}
        sd2 = {"weight": torch.zeros(5)}

        blended = blend_state_dicts([sd1, sd2], weights=[0.8, 0.2])

        expected = torch.full((5,), 0.8)
        assert torch.allclose(blended["weight"], expected)

    def test_blend_single_dict(self):
        """Test blending single dict returns copy."""
        sd1 = {"weight": torch.ones(5)}

        blended = blend_state_dicts([sd1])

        assert torch.allclose(blended["weight"], sd1["weight"])

    def test_blend_empty_raises(self):
        """Test that empty list raises error."""
        with pytest.raises(ValueError):
            blend_state_dicts([])
```

<a id="tests-test_integration-py"></a>

#### `tests/test_integration.py`
*12140 bytes Â· ~2,872 tokens*

```python
"""
End-to-end integration tests using real HuggingFace models.

Uses sshleifer/tiny-gpt2 (~11MB) â€” the smallest GPT-2 variant â€” so that the
full Genesis pipeline can be validated on CPU in seconds without requiring GPUs.

All tests are skipped automatically when transformers / peft / internet are
unavailable.
"""

import pytest
import torch
from torch.utils.data import DataLoader, TensorDataset

# Skip the whole module if the heavy deps aren't installed
transformers = pytest.importorskip("transformers", reason="transformers not installed")
peft = pytest.importorskip("peft", reason="peft not installed")

MODEL = "sshleifer/tiny-gpt2"  # 4 layers, vocab=50257, ~11MB download
VOCAB = 50257
SEQ = 16
BATCH = 2


# â”€â”€ shared fixtures (module-scope = loaded once per session) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _batches(num=3):
    """Synthetic token batches for the tiny model."""
    ids = torch.randint(0, VOCAB, (num * BATCH, SEQ))
    mask = torch.ones_like(ids)
    labels = ids.clone()
    ds = TensorDataset(ids, mask, labels)

    def collate(batch):
        ids_b, mask_b, lab_b = zip(*batch)
        return {
            "input_ids": torch.stack(ids_b),
            "attention_mask": torch.stack(mask_b),
            "labels": torch.stack(lab_b),
        }

    return DataLoader(ds, batch_size=BATCH, collate_fn=collate)


@pytest.fixture(scope="module")
def teacher():
    from genesis.models.teacher import TeacherModel
    try:
        t = TeacherModel(MODEL, device="cpu", dtype=torch.float32)
        t.load()
        return t
    except Exception as e:
        pytest.skip(f"Could not load teacher model: {e}")


@pytest.fixture(scope="module")
def student():
    from genesis.models.student import StudentModel
    from genesis.models.lora_manager import LoRAConfig
    lora_cfg = LoRAConfig(
        r=4,
        lora_alpha=8,
        target_modules=["c_attn"],
        lora_dropout=0.0,
    )
    try:
        s = StudentModel(
            MODEL,
            device="cpu",
            dtype=torch.float32,
            use_lora=True,
            lora_config=lora_cfg,
        )
        s.load()
        return s
    except Exception as e:
        pytest.skip(f"Could not load student model: {e}")


# â”€â”€ TeacherModel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestTeacherModel:

    def test_model_loaded(self, teacher):
        assert teacher.model is not None

    def test_tokenizer_loaded(self, teacher):
        assert teacher.tokenizer is not None
        assert teacher.tokenizer.pad_token is not None

    def test_forward_returns_logits(self, teacher):
        ids = torch.randint(0, VOCAB, (BATCH, SEQ))
        mask = torch.ones_like(ids)
        out = teacher.forward(ids, mask)
        assert "logits" in out
        assert out["logits"].shape == (BATCH, SEQ, VOCAB)

    def test_forward_with_hidden_states(self, teacher):
        ids = torch.randint(0, VOCAB, (BATCH, SEQ))
        out = teacher.forward(ids, output_hidden_states=True)
        assert "hidden_states" in out
        assert len(out["hidden_states"]) > 0

    def test_get_soft_targets_shape(self, teacher):
        ids = torch.randint(0, VOCAB, (BATCH, SEQ))
        soft = teacher.get_soft_targets(ids, temperature=2.0)
        assert soft.shape == (BATCH, SEQ, VOCAB)
        # Each position should sum to ~1.0
        assert torch.allclose(soft.sum(dim=-1), torch.ones(BATCH, SEQ), atol=1e-4)

    def test_get_soft_targets_temperature_effect(self, teacher):
        ids = torch.randint(0, VOCAB, (1, SEQ))
        sharp = teacher.get_soft_targets(ids, temperature=0.1)
        smooth = teacher.get_soft_targets(ids, temperature=10.0)
        # Smoother distribution has higher entropy (more uniform)
        entropy_sharp = -(sharp * sharp.clamp(min=1e-9).log()).sum(-1).mean()
        entropy_smooth = -(smooth * smooth.clamp(min=1e-9).log()).sum(-1).mean()
        assert entropy_smooth > entropy_sharp

    def test_get_config(self, teacher):
        cfg = teacher.get_config()
        assert "model_name_or_path" in cfg
        assert "num_parameters" in cfg
        assert cfg["num_parameters"] > 0

    def test_teacher_is_in_eval_mode(self, teacher):
        assert not teacher.model.training


# â”€â”€ StudentModel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestStudentModel:

    def test_model_loaded(self, student):
        assert student.model is not None

    def test_lora_manager_created(self, student):
        assert student.lora_manager is not None

    def test_lora_params_trainable(self, student):
        trainable = sum(p.numel() for p in student.model.parameters() if p.requires_grad)
        assert trainable > 0

    def test_base_params_frozen(self, student):
        # LoRA: base weights should be frozen, only lora_A/lora_B trainable
        frozen = sum(p.numel() for p in student.model.parameters() if not p.requires_grad)
        assert frozen > 0

    def test_forward_returns_logits(self, student):
        ids = torch.randint(0, VOCAB, (BATCH, SEQ))
        mask = torch.ones_like(ids)
        out = student.forward(ids, mask)
        assert "logits" in out
        assert out["logits"].shape == (BATCH, SEQ, VOCAB)

    def test_forward_with_labels_returns_loss(self, student):
        ids = torch.randint(0, VOCAB, (BATCH, SEQ))
        out = student.forward(ids, labels=ids)
        assert "loss" in out
        assert out["loss"] is not None
        assert out["loss"].item() > 0

    def test_forward_with_hidden_states(self, student):
        ids = torch.randint(0, VOCAB, (BATCH, SEQ))
        out = student.forward(ids, output_hidden_states=True)
        assert "hidden_states" in out

    def test_get_lora_state_dict(self, student):
        sd = student.lora_manager.get_lora_state_dict()
        assert len(sd) > 0
        for key in sd:
            assert "lora" in key.lower()

    def test_set_and_get_state_dict_roundtrip(self, student):
        original = student.lora_manager.get_lora_state_dict()
        # Zero out LoRA weights
        zeroed = {k: torch.zeros_like(v) for k, v in original.items()}
        student.lora_manager.set_lora_state_dict(zeroed)
        # Restore
        student.lora_manager.set_lora_state_dict(original)
        restored = student.lora_manager.get_lora_state_dict()
        for k in original:
            assert torch.allclose(original[k], restored[k])


# â”€â”€ KD Loss with real logits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestKDLossIntegration:

    def test_kd_loss_with_real_logits(self, teacher, student):
        from genesis.distillation.kd_loss import KDLoss
        ids = torch.randint(0, VOCAB, (BATCH, SEQ))
        mask = torch.ones_like(ids)

        with torch.no_grad():
            teacher_out = teacher.forward(ids, mask)

        student_out = student.forward(ids, mask, labels=ids)

        kd = KDLoss(temperature=2.0, alpha=0.5)
        losses = kd(
            student_logits=student_out["logits"],
            teacher_logits=teacher_out["logits"],
            hard_labels=ids,
        )
        assert "total_loss" in losses
        total = losses["total_loss"]
        assert torch.isfinite(total)
        assert total.item() > 0


# â”€â”€ DistillationTrainer with real models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestDistillationTrainerIntegration:

    def test_train_loop_runs(self, teacher, student):
        from genesis.distillation.trainer import DistillationTrainer, TrainingConfig

        cfg = TrainingConfig(
            learning_rate=1e-4,
            max_steps=3,
            warmup_steps=1,
            gradient_accumulation_steps=1,
            logging_steps=1,
            mixed_precision="fp32",
        )

        trainer = DistillationTrainer(
            teacher=teacher,
            student=student,
            train_dataloader=list(_batches(num=4)),
            config=cfg,
        )

        results = trainer.train()
        assert results["global_step"] == 3
        assert torch.isfinite(torch.tensor(results["best_eval_loss"]
                                           if results["best_eval_loss"] != float("inf")
                                           else 0.0))

    def test_logged_loss_is_finite(self, teacher, student):
        from genesis.distillation.trainer import DistillationTrainer, TrainingConfig

        cfg = TrainingConfig(
            learning_rate=1e-4,
            max_steps=2,
            warmup_steps=1,
            gradient_accumulation_steps=1,
            logging_steps=1,
            mixed_precision="fp32",
        )

        trainer = DistillationTrainer(
            teacher=teacher,
            student=student,
            train_dataloader=list(_batches(num=4)),
            config=cfg,
        )

        results = trainer.train()
        for entry in results["training_logs"]:
            assert torch.isfinite(torch.tensor(entry["loss"]))


# â”€â”€ Population with real model weights â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestPopulationIntegration:

    def test_initialize_population_from_real_model(self, student):
        from genesis.core.population import Population
        from genesis.core.genetics import Genetics

        genetics = Genetics(mutation_rate=1.0, mutation_scale=0.01)
        pop = Population(size=3, genetics=genetics, elite_size=1)
        pop.initialize_from_model(student.model, perturbation_scale=0.01)

        assert len(pop) == 3
        for ind in pop:
            assert ind.state_dict is not None
            assert len(ind.state_dict) > 0

    def test_population_individuals_are_distinct(self, student):
        from genesis.core.population import Population
        from genesis.core.genetics import Genetics

        genetics = Genetics(mutation_rate=1.0, mutation_scale=0.05)
        pop = Population(size=3, genetics=genetics, elite_size=1)
        pop.initialize_from_model(student.model, perturbation_scale=0.05)

        # Pick a LoRA parameter that should differ between individuals
        first_key = next(
            k for k in pop[0].state_dict
            if "lora" in k.lower() and pop[0].state_dict[k].numel() > 1
        )
        vals = [pop[i].state_dict[first_key] for i in range(3)]
        # At least some should differ
        assert not (torch.allclose(vals[0], vals[1]) and torch.allclose(vals[1], vals[2]))

    def test_evolve_does_not_crash(self, student):
        from genesis.core.population import Population
        from genesis.core.genetics import Genetics

        genetics = Genetics(mutation_rate=0.5, mutation_scale=0.01)
        pop = Population(size=4, genetics=genetics, elite_size=1)
        pop.initialize_from_model(student.model, perturbation_scale=0.01)

        # Assign dummy fitnesses
        for i, ind in enumerate(pop):
            ind.fitness = float(i) / len(pop)

        pop.evolve()
        assert len(pop) == 4


# â”€â”€ Perplexity fitness with real model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestPerplexityFitnessIntegration:

    def test_perplexity_fitness_on_real_student(self, student):
        from genesis.core.fitness import PerplexityFitness

        evaluator = PerplexityFitness(
            dataloader=list(_batches(num=2)),
            device="cpu",
            max_samples=4,
        )

        result = evaluator.evaluate(student.model)
        assert 0 <= result.score <= 1
        assert result.metrics["perplexity"] > 1.0  # random model has high perplexity
        assert torch.isfinite(torch.tensor(result.score))
```

<a id="tests-test_logging_utils-py"></a>

#### `tests/test_logging_utils.py`
*6681 bytes Â· ~1,482 tokens*

```python
"""Tests for genesis/utils/logging.py and genesis/data/preprocessing.py."""

import logging
import pytest

from genesis.utils.logging import (
    setup_logging,
    get_logger,
    LoggerAdapter,
    TrainingLogger,
    ProgressLogger,
)
from genesis.data.preprocessing import TextPreprocessor


# â”€â”€ setup_logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestSetupLogging:

    def test_returns_logger(self):
        logger = setup_logging(name="test_setup_unique")
        assert isinstance(logger, logging.Logger)

    def test_log_level_debug(self):
        logger = setup_logging(log_level="DEBUG", name="test_debug_unique")
        assert logger.level == logging.DEBUG

    def test_log_level_warning(self):
        logger = setup_logging(log_level="WARNING", name="test_warn_unique")
        assert logger.level == logging.WARNING

    def test_file_handler_created(self, tmp_path):
        log_file = str(tmp_path / "test.log")
        logger = setup_logging(log_file=log_file, name="test_file_unique")
        handler_types = [type(h).__name__ for h in logger.handlers]
        assert "FileHandler" in handler_types
        assert (tmp_path / "test.log").exists()

    def test_log_dir_creates_timestamped_file(self, tmp_path):
        setup_logging(log_dir=str(tmp_path), name="test_logdir_unique")
        log_files = list(tmp_path.glob("*.log"))
        assert len(log_files) == 1

    def test_console_handler_always_added(self):
        logger = setup_logging(name="test_console_unique2")
        handler_types = [type(h).__name__ for h in logger.handlers]
        assert "StreamHandler" in handler_types


# â”€â”€ get_logger â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestGetLogger:

    def test_returns_logger_instance(self):
        logger = get_logger("test_get_unique")
        assert isinstance(logger, logging.Logger)

    def test_same_name_returns_same_logger(self):
        l1 = get_logger("test_same_unique")
        l2 = get_logger("test_same_unique")
        assert l1 is l2


# â”€â”€ LoggerAdapter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestLoggerAdapter:

    def test_prefix_prepended(self):
        base = logging.getLogger("test_adapter_base")
        adapter = LoggerAdapter(base, prefix="GEN5")
        msg, _ = adapter.process("hello", {})
        assert msg == "[GEN5] hello"

    def test_empty_prefix_no_change(self):
        base = logging.getLogger("test_adapter_noprefix")
        adapter = LoggerAdapter(base, prefix="")
        msg, _ = adapter.process("hello", {})
        assert msg == "hello"


# â”€â”€ ProgressLogger â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestProgressLogger:

    def test_log_generation_stores_entry(self):
        pl = ProgressLogger(total_generations=10)
        pl.log_generation(1, best_fitness=0.7, avg_fitness=0.5)
        assert len(pl.get_history()) == 1

    def test_history_entry_has_required_keys(self):
        pl = ProgressLogger(total_generations=10)
        pl.log_generation(1, best_fitness=0.7, avg_fitness=0.5, diversity=0.3)
        entry = pl.get_history()[0]
        assert entry["generation"] == 1
        assert entry["best_fitness"] == pytest.approx(0.7)
        assert entry["avg_fitness"] == pytest.approx(0.5)
        assert entry["diversity"] == pytest.approx(0.3)

    def test_extra_kwargs_stored_in_history(self):
        pl = ProgressLogger(total_generations=5)
        pl.log_generation(0, best_fitness=0.5, avg_fitness=0.4, custom=42)
        assert pl.get_history()[0]["custom"] == 42

    def test_multiple_generations_logged(self):
        pl = ProgressLogger(total_generations=5)
        for i in range(5):
            pl.log_generation(i, best_fitness=float(i), avg_fitness=0.0)
        assert len(pl.get_history()) == 5


# â”€â”€ TrainingLogger â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestTrainingLogger:

    def test_creates_without_log_dir(self):
        tl = TrainingLogger(name="test_tl_nodisk", use_tensorboard=False)
        assert tl.tb_writer is None

    def test_log_metrics_does_not_raise(self):
        tl = TrainingLogger(name="test_tl_metrics", use_tensorboard=False)
        tl.log_metrics({"loss": 0.5, "acc": 0.9}, step=1)

    def test_close_does_not_raise(self):
        tl = TrainingLogger(name="test_tl_close", use_tensorboard=False)
        tl.close()


# â”€â”€ TextPreprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestTextPreprocessor:

    def test_lowercase(self):
        tp = TextPreprocessor(lowercase=True)
        assert tp.preprocess("Hello WORLD") == "hello world"

    def test_normalize_whitespace(self):
        tp = TextPreprocessor(normalize_whitespace=True)
        assert tp.preprocess("  hello   world  ") == "hello world"

    def test_remove_special_chars(self):
        tp = TextPreprocessor(remove_special_chars=True)
        result = tp.preprocess("Hello, @world! #test")
        assert "@" not in result
        assert "#" not in result

    def test_no_ops_by_default(self):
        tp = TextPreprocessor(lowercase=False, remove_special_chars=False,
                              normalize_whitespace=True)
        assert tp.preprocess("Hello World") == "Hello World"

    def test_all_options_combined(self):
        tp = TextPreprocessor(lowercase=True, remove_special_chars=True,
                              normalize_whitespace=True)
        result = tp.preprocess("  HELLO @World!  ")
        assert result == result.lower()
        assert "@" not in result

    def test_tokenize_without_tokenizer_raises(self):
        tp = TextPreprocessor()
        with pytest.raises(ValueError):
            tp.tokenize("hello world")

    def test_batch_tokenize_without_tokenizer_raises(self):
        tp = TextPreprocessor()
        with pytest.raises(ValueError):
            tp.batch_tokenize(["hello", "world"])
```

<a id="tests-test_lora_manager-py"></a>

#### `tests/test_lora_manager.py`
*9951 bytes Â· ~2,374 tokens*

```python
"""Tests for LoRAManager and LoRAConfig."""

import pytest
import torch
import torch.nn as nn

from genesis.models.lora_manager import LoRAManager, LoRAConfig


# â”€â”€ Mock model with LoRA-named parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class MockLoRALayer(nn.Module):
    """A layer that exposes lora_A / lora_B weights â€” discovered by LoRAManager."""

    def __init__(self, in_features: int = 10, out_features: int = 10, r: int = 4):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.lora_A = nn.Parameter(torch.randn(r, in_features))
        self.lora_B = nn.Parameter(torch.randn(out_features, r))


class MockLoRAModel(nn.Module):
    """Minimal model with LoRA layers."""

    def __init__(self, in_features: int = 10, out_features: int = 10, r: int = 4):
        super().__init__()
        self.q_proj = MockLoRALayer(in_features, out_features, r)
        self.v_proj = MockLoRALayer(in_features, out_features, r)


def _make_manager(r: int = 4) -> tuple[MockLoRAModel, LoRAManager]:
    model = MockLoRAModel(r=r)
    # Use empty target_modules so only 'lora_' name matching is used
    config = LoRAConfig(r=r, target_modules=[])
    manager = LoRAManager(model, config)
    return model, manager


# â”€â”€ LoRAConfig â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestLoRAConfig:

    def test_default_construction(self):
        cfg = LoRAConfig()
        assert cfg.r == 16
        assert cfg.lora_alpha == 32
        assert cfg.modules_to_save is None

    def test_custom_construction(self):
        cfg = LoRAConfig(r=8, lora_alpha=16, modules_to_save=["embed_tokens"])
        assert cfg.r == 8
        assert cfg.modules_to_save == ["embed_tokens"]

    def test_settings_loraconfig_has_modules_to_save(self):
        """settings.LoRAConfig must now also expose modules_to_save (bug fix check)."""
        from genesis.config.settings import LoRAConfig as SettingsLoRAConfig
        cfg = SettingsLoRAConfig()
        assert hasattr(cfg, "modules_to_save")
        assert cfg.modules_to_save is None


# â”€â”€ LoRAManager discovery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestLoRAManagerDiscovery:

    def test_discovers_lora_modules(self):
        _, manager = _make_manager()
        # Should discover at least the two MockLoRALayer modules
        assert len(manager._lora_modules) > 0

    def test_get_lora_state_dict_contains_lora_keys(self):
        _, manager = _make_manager()
        state = manager.get_lora_state_dict()
        assert len(state) > 0
        for key in state:
            assert "lora_" in key.lower()

    def test_get_lora_state_dict_excludes_base_weights(self):
        _, manager = _make_manager()
        state = manager.get_lora_state_dict()
        for key in state:
            assert "weight" not in key or "lora_" in key.lower()

    def test_clone_lora_weights_is_independent(self):
        _, manager = _make_manager()
        original = manager.get_lora_state_dict()
        cloned = manager.clone_lora_weights()

        # Mutate cloned; original should not change
        for key in cloned:
            cloned[key].fill_(999.0)

        original_after = manager.get_lora_state_dict()
        for key in original:
            assert not torch.allclose(original_after[key],
                                      torch.full_like(original_after[key], 999.0))


# â”€â”€ LoRAManager operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestLoRAManagerInterpolation:

    def test_interpolate_at_zero_returns_first(self):
        _, manager = _make_manager()
        sd1 = manager.get_lora_state_dict()
        sd2 = {k: torch.zeros_like(v) for k, v in sd1.items()}
        result = manager.interpolate_lora(sd1, sd2, ratio=0.0)
        for key in sd1:
            assert torch.allclose(result[key], sd1[key])

    def test_interpolate_at_one_returns_second(self):
        _, manager = _make_manager()
        sd1 = manager.get_lora_state_dict()
        sd2 = {k: torch.zeros_like(v) for k, v in sd1.items()}
        result = manager.interpolate_lora(sd1, sd2, ratio=1.0)
        for key in sd2:
            assert torch.allclose(result[key], sd2[key])

    def test_interpolate_midpoint(self):
        _, manager = _make_manager()
        sd1 = {k: torch.ones_like(v) for k, v in manager.get_lora_state_dict().items()}
        sd2 = {k: torch.zeros_like(v) for k, v in sd1.items()}
        result = manager.interpolate_lora(sd1, sd2, ratio=0.5)
        for key in result:
            assert torch.allclose(result[key], torch.full_like(result[key], 0.5))


class TestLoRAManagerMerge:

    def test_merge_multiple_equal_weights(self):
        _, manager = _make_manager()
        sd1 = {k: torch.ones_like(v) for k, v in manager.get_lora_state_dict().items()}
        sd2 = {k: torch.zeros_like(v) for k, v in sd1.items()}
        merged = manager.merge_multiple_lora([sd1, sd2])
        # Equal weights â†’ average
        for key in merged:
            assert torch.allclose(merged[key], torch.full_like(merged[key], 0.5))

    def test_merge_multiple_custom_weights(self):
        _, manager = _make_manager()
        base = manager.get_lora_state_dict()
        sd1 = {k: torch.ones_like(v) for k, v in base.items()}
        sd2 = {k: torch.zeros_like(v) for k, v in sd1.items()}
        merged = manager.merge_multiple_lora([sd1, sd2], weights=[0.8, 0.2])
        for key in merged:
            assert torch.allclose(merged[key], torch.full_like(merged[key], 0.8),
                                  atol=1e-5)

    def test_merge_empty_raises(self):
        _, manager = _make_manager()
        with pytest.raises(ValueError):
            manager.merge_multiple_lora([])


class TestLoRAManagerSimilarity:

    def test_similarity_identical_dicts(self):
        _, manager = _make_manager()
        sd = manager.get_lora_state_dict()
        sim = manager.compute_lora_similarity(sd, sd)
        assert abs(sim - 1.0) < 1e-4  # cosine sim of identical vectors = 1

    def test_similarity_opposite_dicts(self):
        _, manager = _make_manager()
        sd1 = {k: torch.ones_like(v) for k, v in manager.get_lora_state_dict().items()}
        sd2 = {k: -torch.ones_like(v) for k, v in sd1.items()}
        sim = manager.compute_lora_similarity(sd1, sd2)
        assert sim < -0.5  # should be close to -1

    def test_similarity_range(self):
        _, manager = _make_manager()
        sd1 = manager.get_lora_state_dict()
        sd2 = {k: torch.randn_like(v) for k, v in sd1.items()}
        sim = manager.compute_lora_similarity(sd1, sd2)
        assert -1.1 <= sim <= 1.1


class TestLoRAManagerMagnitudeAndNoise:

    def test_magnitude_is_non_negative(self):
        _, manager = _make_manager()
        sd = manager.get_lora_state_dict()
        mag = manager.get_lora_magnitude(sd)
        assert mag >= 0.0

    def test_magnitude_zero_for_zero_weights(self):
        _, manager = _make_manager()
        sd = {k: torch.zeros_like(v) for k, v in manager.get_lora_state_dict().items()}
        assert manager.get_lora_magnitude(sd) == 0.0

    def test_scale_weights(self):
        _, manager = _make_manager()
        sd = {k: torch.ones_like(v) for k, v in manager.get_lora_state_dict().items()}
        scaled = manager.scale_lora_weights(sd, scale=2.0)
        for key in scaled:
            assert torch.allclose(scaled[key], torch.full_like(scaled[key], 2.0))

    def test_add_noise_changes_weights(self):
        _, manager = _make_manager()
        sd = {k: torch.zeros_like(v) for k, v in manager.get_lora_state_dict().items()}
        noisy = manager.add_noise_to_lora(sd, noise_scale=0.1)
        changed = any(not torch.allclose(noisy[k], sd[k]) for k in sd)
        assert changed

    def test_add_noise_preserves_shape(self):
        _, manager = _make_manager()
        sd = manager.get_lora_state_dict()
        noisy = manager.add_noise_to_lora(sd, noise_scale=0.01)
        for key in sd:
            assert noisy[key].shape == sd[key].shape


class TestLoRAManagerFreeze:

    def test_freeze_base_model(self):
        model, manager = _make_manager()
        manager.freeze_base_model()
        for name, param in model.named_parameters():
            if "lora_" not in name.lower():
                assert not param.requires_grad

    def test_unfreeze_all(self):
        model, manager = _make_manager()
        manager.freeze_base_model()
        manager.unfreeze_all()
        for param in model.parameters():
            assert param.requires_grad

    def test_trainable_param_count(self):
        model, manager = _make_manager()
        trainable, total = manager.get_trainable_param_count()
        assert trainable > 0
        assert total >= trainable


class TestLoRAManagerSetState:

    def test_set_lora_state_dict_updates_weights(self):
        model, manager = _make_manager()
        new_sd = {k: torch.zeros_like(v)
                  for k, v in manager.get_lora_state_dict().items()}
        manager.set_lora_state_dict(new_sd, strict=False)
        current = manager.get_lora_state_dict()
        for key in new_sd:
            assert torch.allclose(current[key], torch.zeros_like(current[key]))

    def test_set_lora_state_dict_unknown_key_strict_raises(self):
        _, manager = _make_manager()
        bad_sd = {"nonexistent_key": torch.zeros(1)}
        with pytest.raises(KeyError):
            manager.set_lora_state_dict(bad_sd, strict=True)
```

<a id="tests-test_metrics-py"></a>

#### `tests/test_metrics.py`
*9679 bytes Â· ~2,234 tokens*

```python
"""Tests for genesis/utils/metrics.py."""

import pytest
import torch
import numpy as np

from genesis.utils.metrics import (
    compute_perplexity,
    compute_accuracy,
    compute_bleu,
    compute_rouge,
    MetricsTracker,
    FitnessTracker,
)


# â”€â”€ compute_perplexity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestComputePerplexity:

    def test_loss_zero_gives_perplexity_one(self):
        assert compute_perplexity(0.0) == pytest.approx(1.0)

    def test_tensor_input(self):
        loss = torch.tensor(1.0)
        result = compute_perplexity(loss)
        assert isinstance(result, float)
        assert result == pytest.approx(np.e)

    def test_higher_loss_higher_perplexity(self):
        assert compute_perplexity(2.0) > compute_perplexity(1.0)

    def test_base_2(self):
        result = compute_perplexity(1.0, base=2.0)
        assert result == pytest.approx(2.0)


# â”€â”€ compute_accuracy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestComputeAccuracy:

    def test_perfect_accuracy(self):
        preds = torch.tensor([0, 1, 2, 3])
        labels = torch.tensor([0, 1, 2, 3])
        assert compute_accuracy(preds, labels) == pytest.approx(1.0)

    def test_zero_accuracy(self):
        preds = torch.tensor([1, 2, 3, 0])
        labels = torch.tensor([0, 1, 2, 3])
        assert compute_accuracy(preds, labels) == pytest.approx(0.0)

    def test_half_accuracy(self):
        preds = torch.tensor([0, 0, 2, 3])
        labels = torch.tensor([0, 1, 2, 4])
        # 0==0 correct, 0!=1 wrong, 2==2 correct, 3!=4 wrong â†’ 2/4 = 0.5
        assert compute_accuracy(preds, labels) == pytest.approx(0.5)

    def test_ignore_index_excluded(self):
        preds = torch.tensor([0, 1, 2])
        labels = torch.tensor([0, -100, 2])
        # Only positions 0 and 2 count (both correct)
        assert compute_accuracy(preds, labels) == pytest.approx(1.0)

    def test_all_ignored_returns_zero(self):
        preds = torch.tensor([0, 1])
        labels = torch.tensor([-100, -100])
        assert compute_accuracy(preds, labels) == pytest.approx(0.0)


# â”€â”€ compute_bleu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestComputeBleu:

    def test_identical_strings_bleu4_one(self):
        preds = ["the cat sat on the mat"]
        refs = ["the cat sat on the mat"]
        result = compute_bleu(preds, refs)
        assert result["bleu_4"] == pytest.approx(1.0, abs=1e-4)

    def test_no_overlap_bleu_zero(self):
        preds = ["abc def ghi jkl"]
        refs = ["xyz uvw rst opq"]
        result = compute_bleu(preds, refs, smooth=False)
        assert result["bleu_1"] == pytest.approx(0.0)

    def test_returns_all_keys(self):
        preds = ["hello world"]
        refs = ["hello world"]
        result = compute_bleu(preds, refs)
        for key in ["bleu_1", "bleu_2", "bleu_3", "bleu_4", "brevity_penalty"]:
            assert key in result

    def test_brevity_penalty_for_short_prediction(self):
        preds = ["cat"]
        refs = ["the cat sat on the mat"]
        result = compute_bleu(preds, refs)
        assert result["brevity_penalty"] < 1.0

    def test_bleu_scores_in_range(self):
        preds = ["the cat sat"]
        refs = ["the cat sat on the mat"]
        result = compute_bleu(preds, refs)
        for n in range(1, 5):
            assert 0.0 <= result[f"bleu_{n}"] <= 1.0


# â”€â”€ compute_rouge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestComputeRouge:

    def test_identical_strings_rouge_one(self):
        preds = ["the cat sat on the mat"]
        refs = ["the cat sat on the mat"]
        result = compute_rouge(preds, refs)
        assert result["rouge_1"] == pytest.approx(1.0, abs=1e-4)
        assert result["rouge_l"] == pytest.approx(1.0, abs=1e-4)

    def test_returns_all_keys(self):
        preds = ["hello world"]
        refs = ["hello world"]
        result = compute_rouge(preds, refs)
        assert "rouge_1" in result
        assert "rouge_2" in result
        assert "rouge_l" in result

    def test_scores_in_range(self):
        preds = ["the cat sat"]
        refs = ["the cat sat on the mat"]
        result = compute_rouge(preds, refs)
        for key in ["rouge_1", "rouge_2", "rouge_l"]:
            assert 0.0 <= result[key] <= 1.0

    def test_no_overlap_is_zero(self):
        preds = ["xyz abc"]
        refs = ["hello world"]
        result = compute_rouge(preds, refs)
        assert result["rouge_1"] == pytest.approx(0.0, abs=1e-6)


# â”€â”€ MetricsTracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestMetricsTracker:

    def test_update_and_get_latest(self):
        tracker = MetricsTracker()
        tracker.update({"loss": 0.5})
        assert tracker.get_latest("loss") == pytest.approx(0.5)

    def test_get_average_all(self):
        tracker = MetricsTracker()
        tracker.update({"loss": 1.0})
        tracker.update({"loss": 3.0})
        assert tracker.get_average("loss") == pytest.approx(2.0)

    def test_get_average_with_window(self):
        tracker = MetricsTracker()
        for v in [10.0, 1.0, 1.0]:
            tracker.update({"loss": v})
        # Window of 2: average of last two values = 1.0
        assert tracker.get_average("loss", window=2) == pytest.approx(1.0)

    def test_unknown_metric_returns_zero(self):
        tracker = MetricsTracker()
        assert tracker.get_latest("nonexistent") == 0.0
        assert tracker.get_average("nonexistent") == 0.0

    def test_step_increments(self):
        tracker = MetricsTracker()
        assert tracker.step == 0
        tracker.update({"loss": 1.0})
        tracker.update({"loss": 2.0})
        assert tracker.step == 2

    def test_tensor_value_converted(self):
        tracker = MetricsTracker()
        tracker.update({"loss": torch.tensor(0.42)})
        assert isinstance(tracker.get_latest("loss"), float)

    def test_reset_clears_all(self):
        tracker = MetricsTracker()
        tracker.update({"loss": 1.0})
        tracker.reset()
        assert tracker.get_latest("loss") == 0.0
        assert tracker.step == 0

    def test_get_summary_structure(self):
        tracker = MetricsTracker()
        tracker.update({"loss": 1.0, "acc": 0.8})
        tracker.update({"loss": 0.5, "acc": 0.9})
        summary = tracker.get_summary()
        assert "loss" in summary
        assert "acc" in summary
        for key in ["mean", "std", "min", "max", "latest", "count"]:
            assert key in summary["loss"]

    def test_get_all_returns_copy(self):
        tracker = MetricsTracker()
        tracker.update({"x": 1.0})
        vals = tracker.get_all("x")
        vals.append(99.0)
        assert tracker.get_all("x") == [1.0]


# â”€â”€ FitnessTracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestFitnessTracker:

    def test_update_and_history_length(self):
        ft = FitnessTracker()
        for i in range(5):
            ft.update(generation=i, best_fitness=float(i), avg_fitness=float(i) * 0.5)
        assert len(ft.best_fitness_history) == 5
        assert len(ft.generation_history) == 5

    def test_diversity_optional(self):
        ft = FitnessTracker()
        ft.update(generation=0, best_fitness=0.5, avg_fitness=0.3)
        assert ft.diversity_history == []
        ft.update(generation=1, best_fitness=0.6, avg_fitness=0.4, diversity=0.7)
        assert len(ft.diversity_history) == 1

    def test_get_improvement_positive(self):
        ft = FitnessTracker()
        for i in range(10):
            ft.update(generation=i, best_fitness=float(i) * 0.1, avg_fitness=0.0)
        improvement = ft.get_improvement(window=10)
        assert improvement > 0

    def test_get_improvement_insufficient_data(self):
        ft = FitnessTracker()
        ft.update(generation=0, best_fitness=0.5, avg_fitness=0.3)
        assert ft.get_improvement(window=10) == 0.0

    def test_is_converged_when_flat(self):
        ft = FitnessTracker()
        for i in range(15):
            ft.update(generation=i, best_fitness=0.9, avg_fitness=0.8)
        assert ft.is_converged(threshold=1e-4, window=10)

    def test_is_not_converged_when_improving(self):
        ft = FitnessTracker()
        for i in range(15):
            ft.update(generation=i, best_fitness=float(i) * 0.1, avg_fitness=0.0)
        assert not ft.is_converged(threshold=1e-4, window=10)

    def test_get_summary_keys(self):
        ft = FitnessTracker()
        ft.update(generation=0, best_fitness=0.7, avg_fitness=0.5)
        summary = ft.get_summary()
        assert "generations" in summary
        assert "best_fitness" in summary
        assert "final_best" in summary
        assert "converged" in summary

    def test_empty_tracker_summary(self):
        ft = FitnessTracker()
        summary = ft.get_summary()
        assert summary["generations"] == 0
        assert summary["best_fitness"] == 0
```

<a id="tests-test_optimizer-py"></a>

#### `tests/test_optimizer.py`
*15978 bytes Â· ~3,729 tokens*

```python
"""Tests for genesis/optimizer.py â€” EvolutionaryOptimizer."""

import json
import tempfile
import types
from pathlib import Path
from unittest.mock import MagicMock, patch, PropertyMock

import pytest
import torch

from genesis.config.settings import GenesisConfig, GeneticConfig
from genesis.core.fitness import FitnessResult
from genesis.core.genetics import Genetics
from genesis.core.population import Population, Individual
from genesis.optimizer import EvolutionaryOptimizer


# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _make_config(tmp_path: Path, **overrides) -> GenesisConfig:
    """Build a minimal GenesisConfig that writes to a tmp dir."""
    return GenesisConfig(
        project_name="test",
        output_dir=str(tmp_path / "outputs"),
        checkpoint_dir=str(tmp_path / "checkpoints"),
        log_dir=str(tmp_path / "logs"),
        genetic=GeneticConfig(
            population_size=4,
            generations=2,
            mutation_rate=0.5,
            mutation_scale=0.01,
            elite_size=1,
            tournament_size=2,
        ),
        use_tensorboard=False,
        use_wandb=False,
        **overrides,
    )


def _fake_state_dict() -> dict:
    """Minimal LoRA-like state dict."""
    return {
        "base_model.model.layer.lora_A.weight": torch.randn(4, 8),
        "base_model.model.layer.lora_B.weight": torch.randn(8, 4),
    }


def _make_mock_student(state_dict=None):
    """Return a mock StudentModel."""
    sd = state_dict or _fake_state_dict()
    student = MagicMock()
    student.model = MagicMock()
    student.model.parameters.return_value = [torch.nn.Parameter(torch.randn(4, 4))]
    student.model.named_parameters.return_value = list(sd.items())
    student.device = "cpu"
    student.get_state_dict.return_value = sd
    student.load_state_dict.return_value = None
    student.save.return_value = None
    student.unload.return_value = None
    return student


def _make_mock_teacher():
    """Return a mock TeacherModel."""
    teacher = MagicMock()
    teacher.model = MagicMock()
    teacher.model.parameters.return_value = [torch.nn.Parameter(torch.randn(4, 4))]
    teacher.device = "cpu"
    teacher.unload.return_value = None
    return teacher


def _make_optimizer(tmp_path: Path, student=None, teacher=None):
    """Build an EvolutionaryOptimizer with mocked models."""
    cfg = _make_config(tmp_path)
    student = student or _make_mock_student()
    teacher = teacher or _make_mock_teacher()
    opt = EvolutionaryOptimizer(
        config=cfg,
        teacher_model=teacher,
        student_model=student,
    )
    return opt, student, teacher


# â”€â”€ Initialization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TestEvolutionaryOptimizerInit:

    def test_creates_with_config(self, tmp_path):
        cfg = _make_config(tmp_path)
        opt = EvolutionaryOptimizer(config=cfg)
        assert opt.config is cfg

    def test_default_config_when_none(self, tmp_path):
        # Use a config with tmp dirs so file handlers succeed
        cfg = _make_config(tmp_path)
        opt = EvolutionaryOptimizer(config=cfg)
        assert isinstance(opt.config, GenesisConfig)

    def test_accepts_pre_built_models(self, tmp_path):
        opt, student, teacher = _make_optimizer(tmp_path)
        assert opt.teacher is teacher
        assert opt.student is student

    def test_not_initialized_before_initialize(self, tmp_path):
        opt, *_ = _make_optimizer(tmp_path)
        assert not opt._initialized

    def test_current_generation_starts_at_zero(self, tmp_path):
        opt, *_ = _make_optimizer(tmp_path)
        assert opt.current_generation == 0

    def test_fitness_tracker_created(self, tmp_path):
        from genesis.utils.metrics import FitnessTracker
        opt, *_ = _make_optimizer(tmp_path)
        assert isinstance(opt.fitness_tracker, FitnessTracker)

    def test_metrics_tracker_created(self, tmp_path):
        from genesis.utils.metrics import MetricsTracker
        opt, *_ = _make_optimizer(tmp_path)
        assert isinstance(opt.metrics_tracker, MetricsTracker)

    def test_best_individual_none_before_init(self, tmp_path):
        opt, *_ = _make_optimizer(tmp_path)
        assert opt.best_individual is None

    def test_population_none_before_init(self, tmp_path):
        opt, *_ = _make_optimizer(tmp_path)
        assert opt.population is None


# â”€â”€ Initialize method â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TestEvolutionaryOptimizerInitialize:

    def test_initialize_sets_flag(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        opt._initialized = False
        # initialize_from_model needs a real state dict
        student.model.state_dict.return_value = _fake_state_dict()
        with patch.object(opt._population.__class__, "initialize_from_model") if opt._population else patch("genesis.core.population.Population.initialize_from_model") as m:
            opt.initialize()
        assert opt._initialized

    def test_initialize_creates_genetics(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        student.model.state_dict.return_value = _fake_state_dict()
        opt.initialize()
        assert opt._genetics is not None

    def test_initialize_creates_population(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        student.model.state_dict.return_value = _fake_state_dict()
        opt.initialize()
        assert opt._population is not None
        assert len(opt._population) == opt.config.genetic.population_size

    def test_initialize_creates_selection_strategy(self, tmp_path):
        from genesis.core.selection import TournamentSelection
        opt, student, _ = _make_optimizer(tmp_path)
        student.model.state_dict.return_value = _fake_state_dict()
        opt.initialize()
        assert isinstance(opt._selection_strategy, TournamentSelection)

    def test_initialize_population_individuals_distinct(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        # Use a larger state dict so perturbation is detectable
        sd = {"lora_A": torch.randn(16, 32), "lora_B": torch.randn(32, 16)}
        student.model.state_dict.return_value = sd
        opt.initialize()
        # All individuals should exist with state dicts
        for ind in opt.population:
            assert ind.state_dict is not None


# â”€â”€ Custom evaluator and strategy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TestSetterMethods:

    def test_set_fitness_evaluator(self, tmp_path):
        from genesis.core.fitness import FitnessEvaluator
        opt, *_ = _make_optimizer(tmp_path)
        evaluator = MagicMock(spec=FitnessEvaluator)
        opt.set_fitness_evaluator(evaluator)
        assert opt._fitness_evaluator is evaluator

    def test_set_selection_strategy(self, tmp_path):
        from genesis.core.selection import ElitismSelection
        opt, *_ = _make_optimizer(tmp_path)
        strategy = ElitismSelection()
        opt.set_selection_strategy(strategy)
        assert opt._selection_strategy is strategy


# â”€â”€ Checkpoint round-trip â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TestCheckpointing:

    def test_save_and_load_checkpoint(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        sd = {"lora_A": torch.randn(4, 8), "lora_B": torch.randn(8, 4)}
        student.model.state_dict.return_value = sd
        opt.initialize()

        # Set a custom generation
        opt._current_generation = 3

        ckpt_path = str(tmp_path / "checkpoints" / "test_ckpt.pt")
        opt._save_checkpoint(generation=3)

        # Find the actual saved file
        ckpt_files = list((tmp_path / "checkpoints").glob("*.pt"))
        assert len(ckpt_files) >= 1

        # Reset state and reload
        opt._current_generation = 0
        opt.load_checkpoint(str(ckpt_files[0]))

        assert opt._current_generation == 3

    def test_checkpoint_contains_config(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        student.model.state_dict.return_value = _fake_state_dict()
        opt.initialize()
        opt._save_checkpoint(generation=0)

        ckpt_files = list((tmp_path / "checkpoints").glob("*.pt"))
        state = torch.load(str(ckpt_files[0]), weights_only=False)
        assert "config" in state
        assert "population_state" in state
        assert "generation" in state


# â”€â”€ Save best model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TestSaveBestModel:

    def test_save_best_creates_metadata(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        sd = _fake_state_dict()
        student.model.state_dict.return_value = sd
        opt.initialize()

        best = opt.population.best
        if best is None:
            for i, ind in enumerate(opt.population):
                ind.fitness = float(i)
            best = opt.population.best

        opt._save_best_model(best)

        meta_path = Path(opt.config.output_dir) / "best_model" / "metadata.json"
        assert meta_path.exists()
        with open(meta_path) as f:
            meta = json.load(f)
        assert "fitness" in meta
        assert "generation" in meta

    def test_save_best_calls_student_save(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        sd = _fake_state_dict()
        student.model.state_dict.return_value = sd
        opt.initialize()

        for i, ind in enumerate(opt.population):
            ind.fitness = float(i)
        best = opt.population.best
        opt._save_best_model(best)

        student.save.assert_called_once()


# â”€â”€ Evaluate population â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TestEvaluatePopulation:

    def test_evaluate_raises_without_evaluator_or_dataloader(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        student.model.state_dict.return_value = _fake_state_dict()
        opt.initialize()
        # No evaluator, no eval_dataloader
        with pytest.raises(ValueError, match="Fitness evaluator"):
            opt._evaluate_population()

    def test_evaluate_with_custom_evaluator(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        sd = _fake_state_dict()
        student.model.state_dict.return_value = sd
        opt.initialize()

        # Inject a mock evaluator
        mock_eval = MagicMock()
        mock_eval.evaluate.return_value = FitnessResult(score=0.8, metrics={})
        opt.set_fitness_evaluator(mock_eval)

        opt._evaluate_population()

        # All individuals should have fitness set
        for ind in opt.population:
            assert ind.fitness == pytest.approx(0.8)


# â”€â”€ Run (minimal, mocked) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TestRunMethod:

    def test_run_calls_initialize_if_needed(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        sd = _fake_state_dict()
        student.model.state_dict.return_value = sd

        mock_eval = MagicMock()
        mock_eval.evaluate.return_value = FitnessResult(score=0.5, metrics={})
        opt.set_fitness_evaluator(mock_eval)

        results = opt.run(num_generations=1)
        assert opt._initialized

    def test_run_returns_dict_with_expected_keys(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        sd = _fake_state_dict()
        student.model.state_dict.return_value = sd

        mock_eval = MagicMock()
        mock_eval.evaluate.return_value = FitnessResult(score=0.5, metrics={})
        opt.set_fitness_evaluator(mock_eval)

        results = opt.run(num_generations=1)
        for key in ["best_fitness", "best_individual", "generations", "fitness_history", "converged"]:
            assert key in results, f"Missing key: {key}"

    def test_run_callback_is_called(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        sd = _fake_state_dict()
        student.model.state_dict.return_value = sd

        mock_eval = MagicMock()
        mock_eval.evaluate.return_value = FitnessResult(score=0.5, metrics={})
        opt.set_fitness_evaluator(mock_eval)

        calls = []
        opt.run(num_generations=2, callback=lambda info: calls.append(info))
        assert len(calls) == 2
        assert "generation" in calls[0]
        assert "best" in calls[0]

    def test_run_fitness_increases_or_stays(self, tmp_path):
        """Best fitness should be monotonically non-decreasing."""
        opt, student, _ = _make_optimizer(tmp_path)
        sd = _fake_state_dict()
        student.model.state_dict.return_value = sd

        counter = {"n": 0}

        def varying_score(_):
            counter["n"] += 1
            return FitnessResult(score=0.1 * (counter["n"] % 3 + 1), metrics={})

        mock_eval = MagicMock()
        mock_eval.evaluate.side_effect = varying_score
        opt.set_fitness_evaluator(mock_eval)

        opt.run(num_generations=3)
        history = opt.fitness_tracker.best_fitness_history
        assert all(b >= 0 for b in history)

    def test_run_skips_distillation_without_train_dataloader(self, tmp_path):
        """_refine_best_individual should be a no-op when no train_dataloader."""
        opt, student, _ = _make_optimizer(tmp_path)
        sd = _fake_state_dict()
        student.model.state_dict.return_value = sd

        mock_eval = MagicMock()
        mock_eval.evaluate.return_value = FitnessResult(score=0.5, metrics={})
        opt.set_fitness_evaluator(mock_eval)

        # Override eval interval so _refine_best_individual is triggered
        opt.config.eval_every_n_generations = 1
        # Should not raise even without a train_dataloader
        opt.run(num_generations=2)


# â”€â”€ Properties â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TestProperties:

    def test_best_individual_after_init(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        student.model.state_dict.return_value = _fake_state_dict()
        opt.initialize()
        # best_individual may be None if fitnesses not set; just shouldn't raise
        _ = opt.best_individual

    def test_population_property(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        student.model.state_dict.return_value = _fake_state_dict()
        opt.initialize()
        assert opt.population is opt._population

    def test_current_generation_updates_during_run(self, tmp_path):
        opt, student, _ = _make_optimizer(tmp_path)
        student.model.state_dict.return_value = _fake_state_dict()

        mock_eval = MagicMock()
        mock_eval.evaluate.return_value = FitnessResult(score=0.5, metrics={})
        opt.set_fitness_evaluator(mock_eval)

        opt.run(num_generations=2)
        assert opt.current_generation >= 1
```

<a id="tests-test_population-py"></a>

#### `tests/test_population.py`
*7990 bytes Â· ~1,997 tokens*

```python
"""Tests for population module."""

import pytest
import torch
import torch.nn as nn

from genesis.core.population import Population, Individual
from genesis.core.genetics import Genetics


class TestIndividual:
    """Tests for Individual class."""

    def test_individual_creation(self):
        """Test individual creation."""
        state_dict = {"weight": torch.randn(10)}
        individual = Individual(state_dict=state_dict, fitness=0.5, generation=1)

        assert individual.fitness == 0.5
        assert individual.generation == 1
        assert "weight" in individual.state_dict

    def test_individual_clone(self):
        """Test individual cloning."""
        state_dict = {"weight": torch.randn(10)}
        individual = Individual(state_dict=state_dict, fitness=0.8)

        clone = individual.clone()

        assert clone.id != individual.id
        assert clone.fitness == individual.fitness
        assert torch.allclose(clone.state_dict["weight"], individual.state_dict["weight"])

    def test_individual_clone_independence(self):
        """Test that cloned individuals are independent."""
        state_dict = {"weight": torch.randn(10)}
        individual = Individual(state_dict=state_dict)

        clone = individual.clone()
        clone.state_dict["weight"][0] = 999.0

        assert individual.state_dict["weight"][0] != 999.0

    def test_individual_repr(self):
        """Test individual string representation."""
        individual = Individual(fitness=0.75, generation=5)
        repr_str = repr(individual)

        assert "0.75" in repr_str
        assert "5" in repr_str


class TestPopulation:
    """Tests for Population class."""

    def test_population_initialization(self):
        """Test population initialization."""
        population = Population(size=10, elite_size=2)

        assert population.size == 10
        assert population.elite_size == 2
        assert len(population) == 0  # Not initialized yet

    def test_initialize_from_model(self):
        """Test population initialization from model."""
        model = nn.Linear(10, 5)
        population = Population(size=5)

        population.initialize_from_model(model, perturbation_scale=0.01)

        assert len(population) == 5
        # First individual should be unperturbed
        assert population[0].metadata.get("origin") == "base"

    def test_population_evaluation(self):
        """Test population fitness evaluation."""
        population = Population(size=5)
        population.initialize_from_model(nn.Linear(5, 5))

        # Simple fitness function
        def fitness_fn(state_dict):
            return torch.mean(state_dict["weight"]).item()

        population.evaluate(fitness_fn)

        # All individuals should have fitness set
        for ind in population:
            assert ind.fitness != 0.0

    def test_population_best(self):
        """Test getting best individual."""
        population = Population(size=3)
        population._individuals = [
            Individual(state_dict={}, fitness=0.5),
            Individual(state_dict={}, fitness=0.9),
            Individual(state_dict={}, fitness=0.3),
        ]

        best = population.best
        assert best.fitness == 0.9

    def test_population_worst(self):
        """Test getting worst individual."""
        population = Population(size=3)
        population._individuals = [
            Individual(state_dict={}, fitness=0.5),
            Individual(state_dict={}, fitness=0.9),
            Individual(state_dict={}, fitness=0.3),
        ]

        worst = population.worst
        assert worst.fitness == 0.3

    def test_population_average_fitness(self):
        """Test average fitness calculation."""
        population = Population(size=3)
        population._individuals = [
            Individual(state_dict={}, fitness=0.3),
            Individual(state_dict={}, fitness=0.6),
            Individual(state_dict={}, fitness=0.9),
        ]

        avg = population.average_fitness
        assert abs(avg - 0.6) < 1e-6

    def test_population_evolution(self):
        """Test population evolution."""
        model = nn.Linear(5, 5)
        population = Population(size=5, elite_size=1)
        population.initialize_from_model(model)

        # Set some fitnesses
        for i, ind in enumerate(population):
            ind.fitness = float(i) / len(population)

        initial_gen = population.generation
        population.evolve()

        assert population.generation == initial_gen + 1
        assert len(population) == 5

    def test_population_elitism(self):
        """Test that elites are preserved."""
        population = Population(size=5, elite_size=2)

        # Create individuals with known fitnesses
        population._individuals = [
            Individual(id="best", state_dict={"w": torch.ones(5)}, fitness=1.0),
            Individual(id="second", state_dict={"w": torch.ones(5) * 2}, fitness=0.8),
            Individual(id="third", state_dict={"w": torch.zeros(5)}, fitness=0.5),
            Individual(id="fourth", state_dict={"w": torch.zeros(5)}, fitness=0.3),
            Individual(id="fifth", state_dict={"w": torch.zeros(5)}, fitness=0.1),
        ]

        population.evolve()

        # Best individuals should have elite origin
        elite_count = sum(1 for ind in population if ind.metadata.get("origin") == "elite")
        assert elite_count >= 1

    def test_population_state_save_load(self):
        """Test saving and loading population state."""
        model = nn.Linear(5, 5)
        population = Population(size=3)
        population.initialize_from_model(model)

        # Set some state
        population._individuals[0].fitness = 0.99
        population._generation = 5

        # Save state
        state = population.get_state()

        # Create new population and load
        new_population = Population(size=3)
        new_population.load_state(state)

        assert new_population.generation == 5
        assert new_population[0].fitness == 0.99

    def test_population_iteration(self):
        """Test population iteration."""
        population = Population(size=3)
        population._individuals = [
            Individual(state_dict={}, fitness=i)
            for i in range(3)
        ]

        fitnesses = [ind.fitness for ind in population]
        assert fitnesses == [0.0, 1.0, 2.0]

    def test_population_indexing(self):
        """Test population indexing."""
        population = Population(size=3)
        population._individuals = [
            Individual(id=f"ind_{i}", state_dict={})
            for i in range(3)
        ]

        assert population[0].id == "ind_0"
        assert population[2].id == "ind_2"


class TestPopulationDiversity:
    """Tests for population diversity measurement."""

    def test_diversity_calculation(self):
        """Test diversity calculation."""
        population = Population(size=3)

        # Create individuals with different weights
        population._individuals = [
            Individual(state_dict={"w": torch.zeros(10)}),
            Individual(state_dict={"w": torch.ones(10)}),
            Individual(state_dict={"w": torch.ones(10) * 2}),
        ]

        diversity = population.diversity
        assert diversity > 0

    def test_diversity_zero_for_identical(self):
        """Test diversity is zero for identical individuals."""
        population = Population(size=3)
        same_weight = torch.ones(10)

        population._individuals = [
            Individual(state_dict={"w": same_weight.clone()})
            for _ in range(3)
        ]

        diversity = population.diversity
        assert diversity < 1e-6

    def test_diversity_with_single_individual(self):
        """Test diversity with single individual."""
        population = Population(size=1)
        population._individuals = [Individual(state_dict={"w": torch.ones(10)})]

        diversity = population.diversity
        assert diversity == 0.0
```

<a id="tests-test_pruning-py"></a>

#### `tests/test_pruning.py`
*12786 bytes Â· ~3,071 tokens*

```python
"""Tests for pruning: saliency calculation and Pruner."""

import pytest
import torch
import torch.nn as nn

from genesis.pruning.saliency import compute_weight_importance, SaliencyCalculator
from genesis.pruning.pruner import Pruner, PruningConfig


# â”€â”€ Shared helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SimpleModel(nn.Module):
    """Minimal model whose forward() produces a loss (needed for gradient methods)."""

    def __init__(self, vocab_size: int = 50, hidden: int = 16):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, hidden)
        self.fc = nn.Linear(hidden, vocab_size)
        self.vocab_size = vocab_size

    def forward(self, input_ids, attention_mask=None, labels=None, **_):
        hidden = self.embed(input_ids)
        logits = self.fc(hidden)
        loss = None
        if labels is not None:
            loss = nn.functional.cross_entropy(
                logits.view(-1, self.vocab_size), labels.view(-1)
            )

        class _Out:
            pass

        out = _Out()
        out.logits = logits
        out.loss = loss
        return out


def _make_batches(batch_size: int = 2, seq_len: int = 8,
                  vocab_size: int = 50, num_batches: int = 2):
    """Return a list of dict batches (acts as an iterable dataloader)."""
    n = batch_size * num_batches
    ids = torch.randint(0, vocab_size, (n, seq_len))
    batches = []
    for i in range(0, n, batch_size):
        chunk = ids[i : i + batch_size]
        batches.append({"input_ids": chunk, "attention_mask": torch.ones_like(chunk),
                        "labels": chunk})
    return batches


# â”€â”€ compute_weight_importance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestComputeWeightImportance:

    def test_magnitude_needs_no_dataloader(self):
        model = SimpleModel()
        imp = compute_weight_importance(model, method="magnitude", device="cpu")
        assert len(imp) > 0

    def test_magnitude_scores_are_non_negative(self):
        model = nn.Linear(5, 5, bias=False)
        model.weight.data.fill_(-3.0)
        imp = compute_weight_importance(model, method="magnitude", device="cpu")
        assert torch.allclose(imp["weight"], torch.full((5, 5), 3.0))

    def test_magnitude_shape_matches_parameters(self):
        model = SimpleModel()
        imp = compute_weight_importance(model, method="magnitude", device="cpu")
        params = dict(model.named_parameters())
        for name, scores in imp.items():
            assert scores.shape == params[name].shape

    def test_gradient_requires_dataloader(self):
        with pytest.raises(ValueError, match="DataLoader"):
            compute_weight_importance(SimpleModel(), method="gradient", device="cpu")

    def test_taylor_requires_dataloader(self):
        with pytest.raises(ValueError, match="DataLoader"):
            compute_weight_importance(SimpleModel(), method="taylor", device="cpu")

    def test_fisher_requires_dataloader(self):
        with pytest.raises(ValueError, match="DataLoader"):
            compute_weight_importance(SimpleModel(), method="fisher", device="cpu")

    def test_unknown_method_raises(self):
        with pytest.raises(ValueError):
            compute_weight_importance(SimpleModel(), method="unknown", device="cpu")

    def test_gradient_with_dataloader(self):
        model = SimpleModel()
        imp = compute_weight_importance(
            model, method="gradient",
            dataloader=_make_batches(), device="cpu", num_samples=4,
        )
        assert len(imp) > 0
        for scores in imp.values():
            assert (scores >= 0).all()

    def test_taylor_with_dataloader(self):
        model = SimpleModel()
        imp = compute_weight_importance(
            model, method="taylor",
            dataloader=_make_batches(), device="cpu", num_samples=4,
        )
        assert len(imp) > 0
        for scores in imp.values():
            assert (scores >= 0).all()

    def test_fisher_with_dataloader(self):
        model = SimpleModel()
        imp = compute_weight_importance(
            model, method="fisher",
            dataloader=_make_batches(), device="cpu", num_samples=4,
        )
        assert len(imp) > 0
        for scores in imp.values():
            assert (scores >= 0).all()


# â”€â”€ SaliencyCalculator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestSaliencyCalculator:

    def test_compute_returns_importance_dict(self):
        calc = SaliencyCalculator(SimpleModel(), method="magnitude", device="cpu")
        imp = calc.compute()
        assert isinstance(imp, dict)
        assert len(imp) > 0

    def test_compute_caches_result(self):
        calc = SaliencyCalculator(SimpleModel(), method="magnitude", device="cpu")
        imp1 = calc.compute()
        imp2 = calc.compute()
        assert imp1 is imp2  # same object â€” cache hit

    def test_force_recompute_returns_new_object(self):
        calc = SaliencyCalculator(SimpleModel(), method="magnitude", device="cpu")
        imp1 = calc.compute()
        imp2 = calc.compute(force_recompute=True)
        assert imp1 is not imp2

    def test_clear_cache(self):
        calc = SaliencyCalculator(SimpleModel(), method="magnitude", device="cpu")
        calc.compute()
        assert calc._importance_cache is not None
        calc.clear_cache()
        assert calc._importance_cache is None

    def test_get_top_k_mask_per_layer_keep_ratio(self):
        model = nn.Linear(20, 20, bias=False)
        calc = SaliencyCalculator(model, method="magnitude", device="cpu")
        masks = calc.get_top_k_mask(k=0.5, per_layer=True)
        assert "weight" in masks
        keep_ratio = masks["weight"].sum().item() / masks["weight"].numel()
        assert 0.4 <= keep_ratio <= 0.6

    def test_get_top_k_mask_global(self):
        model = nn.Linear(10, 10, bias=False)
        calc = SaliencyCalculator(model, method="magnitude", device="cpu")
        masks = calc.get_top_k_mask(k=0.3, per_layer=False)
        assert "weight" in masks
        # At most 30% kept globally
        keep_ratio = masks["weight"].sum().item() / masks["weight"].numel()
        assert keep_ratio <= 0.35  # allow small rounding

    def test_get_pruning_mask_sparsity_complement(self):
        """pruning_mask(sparsity=s) == top_k_mask(k=1-s)."""
        model = nn.Linear(20, 20, bias=False)
        calc = SaliencyCalculator(model, method="magnitude", device="cpu")
        masks = calc.get_pruning_mask(sparsity=0.3, per_layer=True)
        keep_ratio = masks["weight"].sum().item() / masks["weight"].numel()
        # ~70% of weights should survive
        assert 0.65 <= keep_ratio <= 0.75

    def test_masks_are_binary(self):
        model = nn.Linear(10, 10, bias=False)
        calc = SaliencyCalculator(model, method="magnitude", device="cpu")
        masks = calc.get_pruning_mask(sparsity=0.5)
        for mask in masks.values():
            unique_vals = torch.unique(mask)
            assert all(v.item() in {0.0, 1.0} for v in unique_vals)

    def test_importance_ranking_sorted_ascending(self):
        model = nn.Linear(5, 5, bias=False)
        calc = SaliencyCalculator(model, method="magnitude", device="cpu")
        rankings = calc.get_importance_ranking()
        assert len(rankings) == 25  # 5 Ã— 5 weights
        scores = [r[2] for r in rankings]
        assert scores == sorted(scores)  # ascending

    def test_importance_ranking_entry_format(self):
        model = nn.Linear(4, 4, bias=False)
        calc = SaliencyCalculator(model, method="magnitude", device="cpu")
        rankings = calc.get_importance_ranking()
        name, idx, score = rankings[0]
        assert isinstance(name, str)
        assert isinstance(idx, int)
        assert isinstance(score, float)


# â”€â”€ Pruner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestPruner:

    def test_initial_sparsity_is_zero(self):
        pruner = Pruner(nn.Linear(10, 10), PruningConfig(), device="cpu")
        assert pruner.current_sparsity == 0.0

    def test_unstructured_prune_returns_stats(self):
        model = nn.Linear(20, 20, bias=False)
        pruner = Pruner(model, PruningConfig(target_sparsity=0.5), device="cpu")
        stats = pruner.prune()
        assert "actual_sparsity" in stats
        assert "total_params" in stats
        assert "pruned_params" in stats

    def test_prune_zeros_weights(self):
        model = nn.Linear(20, 20, bias=False)
        model.weight.data = torch.rand(20, 20) + 1.0  # all positive
        pruner = Pruner(model, PruningConfig(target_sparsity=0.5), device="cpu")
        pruner.prune()
        zeros = (model.weight.data == 0).sum().item()
        assert zeros > 0

    def test_current_sparsity_updated_after_prune(self):
        model = nn.Linear(20, 20, bias=False)
        pruner = Pruner(model, PruningConfig(target_sparsity=0.3), device="cpu")
        pruner.prune()
        assert pruner.current_sparsity > 0.0

    def test_restore_weights_undoes_pruning(self):
        model = nn.Linear(10, 10, bias=False)
        original = model.weight.data.clone()
        pruner = Pruner(model, PruningConfig(target_sparsity=0.5), device="cpu")
        pruner.prune()
        assert not torch.allclose(model.weight.data, original)
        pruner.restore_weights()
        assert torch.allclose(model.weight.data, original)

    def test_restore_clears_sparsity(self):
        model = nn.Linear(10, 10, bias=False)
        pruner = Pruner(model, PruningConfig(target_sparsity=0.4), device="cpu")
        pruner.prune()
        pruner.restore_weights()
        assert pruner.current_sparsity == 0.0

    def test_get_sparsity_stats(self):
        model = nn.Linear(10, 10, bias=False)
        pruner = Pruner(model, PruningConfig(target_sparsity=0.3), device="cpu")
        pruner.prune()
        stats = pruner.get_sparsity_stats()
        assert "global_sparsity" in stats
        assert "total_params" in stats
        assert "zero_params" in stats
        assert "layer_stats" in stats
        assert stats["global_sparsity"] > 0.0

    def test_make_pruning_permanent_clears_originals(self):
        model = nn.Linear(10, 10, bias=False)
        pruner = Pruner(model, PruningConfig(target_sparsity=0.3), device="cpu")
        pruner.prune()
        assert len(pruner._original_weights) > 0
        pruner.make_pruning_permanent()
        assert len(pruner._original_weights) == 0

    def test_apply_masks_re_zeroes_weights(self):
        model = nn.Linear(10, 10, bias=False)
        pruner = Pruner(model, PruningConfig(target_sparsity=0.5), device="cpu")
        pruner.prune()
        # Manually fill weights back to 1
        model.weight.data.fill_(1.0)
        pruner.apply_masks()
        # Masked weights should be zeroed again
        zeros = (model.weight.data == 0).sum().item()
        assert zeros > 0

    def test_skip_layers_prevents_pruning(self):
        model = nn.Linear(10, 10, bias=False)
        original = model.weight.data.clone()
        config = PruningConfig(target_sparsity=0.5, skip_layers=["weight"])
        pruner = Pruner(model, config, device="cpu")
        pruner.prune()
        assert torch.allclose(model.weight.data, original)

    def test_structured_prune_row(self):
        model = nn.Linear(20, 20, bias=False)
        config = PruningConfig(
            target_sparsity=0.3,
            structured=True,
            granularity="row",
        )
        pruner = Pruner(model, config, device="cpu")
        stats = pruner.prune()
        assert stats["granularity"] == "row"
        assert "actual_sparsity" in stats

    def test_structured_prune_column(self):
        model = nn.Linear(20, 20, bias=False)
        config = PruningConfig(
            target_sparsity=0.3,
            structured=True,
            granularity="column",
        )
        pruner = Pruner(model, config, device="cpu")
        stats = pruner.prune()
        assert stats["granularity"] == "column"

    def test_pruner_masks_property(self):
        model = nn.Linear(10, 10, bias=False)
        pruner = Pruner(model, PruningConfig(target_sparsity=0.4), device="cpu")
        assert isinstance(pruner.masks, dict)
        pruner.prune()
        assert len(pruner.masks) > 0
```

<a id="tests-test_qwen3_integration-py"></a>

#### `tests/test_qwen3_integration.py`
*13370 bytes Â· ~3,139 tokens*

```python
"""
Real integration tests using Qwen3 models.

Teacher : Ollama qwen3.5  (remote/local Ollama server â€” no GPU required)
Student : Qwen/Qwen3-1.7B (GPU 1 â€” RTX 5090, LoRA rank-16)

Skipped automatically when Ollama is not reachable or CUDA is unavailable.
"""

import os
import pytest
import torch
import requests
from pathlib import Path

# â”€â”€ Model paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MODEL_DIR  = Path("/media/ttech-main/42A4266DA426639F/Models")
STUDENT_ID = "Qwen/Qwen3-1.7B"

OLLAMA_URL        = os.environ.get("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_MODEL      = os.environ.get("OLLAMA_MODEL", "qwen3.5")


def _find_local(model_id: str) -> str | None:
    """Resolve a model ID to a local snapshot directory, or None if not cached."""
    # 1. Simple named subdirectory: Models/Qwen3-1.7B/, etc.
    model_short = model_id.split("/")[-1]
    flat_dir = MODEL_DIR / model_short
    if flat_dir.exists() and any(flat_dir.glob("*.safetensors")):
        return str(flat_dir)

    # 2. HF hub cache structure: hub/models--<org>--<name>/snapshots/<hash>/
    safe_name = model_id.replace("/", "--")
    pattern = MODEL_DIR / "hub" / f"models--{safe_name}" / "snapshots"
    if pattern.exists():
        snapshots = sorted(pattern.iterdir())
        if snapshots:
            return str(snapshots[-1])

    return None


def _ollama_reachable() -> bool:
    try:
        r = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        return r.ok
    except Exception:
        return False


def _ollama_model_available() -> bool:
    try:
        r = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        if not r.ok:
            return False
        models = [m["name"] for m in r.json().get("models", [])]
        return any(OLLAMA_MODEL in m for m in models)
    except Exception:
        return False


STUDENT_PATH = _find_local(STUDENT_ID)

# Skip markers
skip_no_ollama = pytest.mark.skipif(
    not _ollama_reachable(),
    reason=f"Ollama server not reachable at {OLLAMA_URL}",
)
skip_no_ollama_model = pytest.mark.skipif(
    not _ollama_model_available(),
    reason=f"Ollama model '{OLLAMA_MODEL}' not pulled yet â€” run: ollama pull {OLLAMA_MODEL}",
)
skip_no_student = pytest.mark.skipif(
    STUDENT_PATH is None,
    reason=f"{STUDENT_ID} not downloaded yet",
)
skip_no_cuda = pytest.mark.skipif(
    not torch.cuda.is_available(),
    reason="CUDA not available â€” needs RTX 5090",
)
skip_no_models = pytest.mark.skipif(
    not _ollama_model_available() or STUDENT_PATH is None,
    reason="Ollama teacher or local student model not available",
)

# â”€â”€ Qwen3 LoRA config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LORA_TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj"]
LORA_RANK           = 16
SEQ_LEN             = 64
BATCH_SIZE          = 1


# â”€â”€ Fixtures â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@pytest.fixture(scope="module")
def teacher_model():
    """Ollama-backed teacher â€” no local GPU memory required."""
    from genesis.models.ollama_teacher import OllamaTeacher
    t = OllamaTeacher(
        model_name=OLLAMA_MODEL,
        base_url=OLLAMA_URL,
        tokenizer_path=STUDENT_PATH,   # shared Qwen3 tokenizer
        vocab_size=151936,
        top_logprobs=20,
    )
    t.load()
    yield t
    t.unload()


@pytest.fixture(scope="module")
def student_model():
    """Load Qwen3-1.7B student with LoRA on GPU 1."""
    from genesis.models.student import StudentModel
    from genesis.models.lora_manager import LoRAConfig

    lora_cfg = LoRAConfig(
        r=LORA_RANK,
        lora_alpha=LORA_RANK * 2,
        target_modules=LORA_TARGET_MODULES,
        lora_dropout=0.05,
        bias="none",
    )
    s = StudentModel(
        model_name_or_path=STUDENT_PATH,
        device="cuda:0",
        dtype=torch.bfloat16,
        use_lora=True,
        lora_config=lora_cfg,
    )
    s.load()
    yield s


def _dummy_batch(vocab_size: int, device: str = "cuda:0"):
    """Tiny synthetic batch for quick forward-pass validation."""
    ids  = torch.randint(0, vocab_size, (BATCH_SIZE, SEQ_LEN), device=device)
    mask = torch.ones_like(ids)
    return ids, mask


# â”€â”€ OllamaTeacher tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@skip_no_ollama
@skip_no_ollama_model
class TestOllamaTeacher:

    def test_ollama_server_reachable(self, teacher_model):
        assert _ollama_reachable()

    def test_model_available(self, teacher_model):
        assert _ollama_model_available()

    def test_get_config_keys(self, teacher_model):
        cfg = teacher_model.get_config()
        assert "model_name" in cfg
        assert "vocab_size" in cfg
        assert cfg["vocab_size"] == 151936

    def test_model_shim_not_training(self, teacher_model):
        assert not teacher_model.model.training

    def test_vocab_size_matches_student(self, teacher_model):
        assert teacher_model.model.config.vocab_size == 151936

    def test_generate_returns_string(self, teacher_model):
        out = teacher_model.generate("Hello, how are you?", max_tokens=20)
        assert isinstance(out, str)
        assert len(out) > 0

    def test_soft_targets_shape(self, teacher_model):
        ids = torch.randint(0, 1000, (BATCH_SIZE, SEQ_LEN))
        soft = teacher_model.get_soft_targets(ids, temperature=1.0)
        assert soft.shape == (BATCH_SIZE, SEQ_LEN, 151936)

    def test_soft_targets_sum_to_one(self, teacher_model):
        ids = torch.randint(0, 1000, (BATCH_SIZE, SEQ_LEN))
        soft = teacher_model.get_soft_targets(ids, temperature=1.0)
        sums = soft.sum(dim=-1)
        assert torch.allclose(sums, torch.ones_like(sums), atol=1e-3)

    def test_soft_targets_non_negative(self, teacher_model):
        ids = torch.randint(0, 1000, (BATCH_SIZE, SEQ_LEN))
        soft = teacher_model.get_soft_targets(ids, temperature=1.0)
        assert (soft >= 0).all()

    def test_forward_logits_shape(self, teacher_model):
        ids = torch.randint(0, 1000, (BATCH_SIZE, SEQ_LEN))
        out = teacher_model.forward(ids)
        assert "logits" in out
        assert out["logits"].shape == (BATCH_SIZE, SEQ_LEN, 151936)


# â”€â”€ StudentModel (Qwen3-1.7B + LoRA) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@skip_no_student
@skip_no_cuda
class TestQwen3Student:

    def test_model_on_correct_device(self, student_model):
        devices = {p.device for p in student_model.model.parameters()}
        assert any(d.type == "cuda" for d in devices)

    def test_lora_manager_present(self, student_model):
        assert student_model.lora_manager is not None

    def test_trainable_params_are_lora_only(self, student_model):
        trainable = [n for n, p in student_model.model.named_parameters()
                     if p.requires_grad]
        assert len(trainable) > 0
        for name in trainable:
            assert "lora_" in name, f"Unexpected trainable param: {name}"

    def test_forward_shape(self, student_model):
        vocab = student_model.model.config.vocab_size
        ids, mask = _dummy_batch(vocab, "cuda:0")
        out = student_model.forward(ids, mask)
        assert out["logits"].shape == (BATCH_SIZE, SEQ_LEN, vocab)

    def test_forward_with_labels_loss_finite(self, student_model):
        vocab = student_model.model.config.vocab_size
        ids, mask = _dummy_batch(vocab, "cuda:0")
        out = student_model.forward(ids, mask, labels=ids)
        assert out["loss"] is not None
        assert torch.isfinite(out["loss"])

    def test_lora_state_dict_keys(self, student_model):
        sd = student_model.lora_manager.get_lora_state_dict()
        assert len(sd) > 0
        for key in sd:
            assert "lora_" in key

    def test_lora_state_dict_roundtrip(self, student_model):
        original = student_model.lora_manager.get_lora_state_dict()
        zeroed   = {k: torch.zeros_like(v) for k, v in original.items()}
        student_model.lora_manager.set_lora_state_dict(zeroed)
        student_model.lora_manager.set_lora_state_dict(original)
        restored = student_model.lora_manager.get_lora_state_dict()
        for k in original:
            assert torch.allclose(original[k].cpu(), restored[k].cpu())


# â”€â”€ KD Loss (Ollama teacher â†’ GPU student) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@skip_no_models
@skip_no_cuda
class TestQwen3KDLoss:

    def test_kd_loss_with_ollama_teacher(self, teacher_model, student_model):
        from genesis.distillation.kd_loss import KDLoss

        vocab = student_model.model.config.vocab_size
        ids, mask = _dummy_batch(vocab, "cuda:0")

        # Get soft targets from Ollama teacher (CPU tensors)
        ids_cpu = ids.cpu()
        teacher_soft = teacher_model.get_soft_targets(ids_cpu, temperature=2.0)
        # Convert to logits and move to student device
        teacher_logits = torch.log(teacher_soft.clamp(min=1e-9)).to("cuda:0")

        s_out = student_model.forward(ids, mask)

        kd = KDLoss(temperature=2.0, alpha=0.5)
        losses = kd(
            student_logits=s_out["logits"],
            teacher_logits=teacher_logits,
            hard_labels=ids,
        )
        assert "total_loss" in losses
        assert torch.isfinite(losses["total_loss"])
        assert losses["total_loss"].item() > 0


# â”€â”€ Distillation training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@skip_no_models
@skip_no_cuda
class TestQwen3DistillationTrainer:

    def _make_dataloader(self, vocab: int, n_batches: int = 4):
        from torch.utils.data import DataLoader, TensorDataset
        ids  = torch.randint(0, vocab, (n_batches * BATCH_SIZE, SEQ_LEN))
        mask = torch.ones_like(ids)

        def collate(batch):
            id_b, m_b, lab_b = zip(*batch)
            return {
                "input_ids":      torch.stack(id_b),
                "attention_mask": torch.stack(m_b),
                "labels":         torch.stack(lab_b),
            }

        ds = TensorDataset(ids, mask, ids.clone())
        return DataLoader(ds, batch_size=BATCH_SIZE, collate_fn=collate)

    def test_distillation_train_steps(self, teacher_model, student_model):
        from genesis.distillation.trainer import DistillationTrainer, TrainingConfig

        vocab = student_model.model.config.vocab_size
        dl    = self._make_dataloader(vocab, n_batches=4)

        cfg = TrainingConfig(
            learning_rate=1e-4,
            max_steps=5,
            warmup_steps=1,
            gradient_accumulation_steps=1,
            logging_steps=1,
            mixed_precision="bf16",
        )

        trainer = DistillationTrainer(
            teacher=teacher_model,
            student=student_model,
            train_dataloader=list(dl),
            config=cfg,
        )

        results = trainer.train()
        assert results["global_step"] == 5
        for entry in results["training_logs"]:
            assert torch.isfinite(torch.tensor(entry["loss"]))


# â”€â”€ Population evolution with real Qwen3-1.7B LoRA weights â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@skip_no_student
@skip_no_cuda
class TestQwen3Population:

    def test_initialize_and_evolve(self, student_model):
        from genesis.core.population import Population
        from genesis.core.genetics import Genetics

        genetics = Genetics(
            mutation_rate=1.0,
            mutation_scale=0.01,
            crossover_rate=0.7,
        )
        pop = Population(size=4, genetics=genetics, elite_size=1)
        pop.initialize_from_model(student_model.model, perturbation_scale=0.01)

        assert len(pop) == 4

        for i, ind in enumerate(pop):
            ind.fitness = float(i) / len(pop)

        pop.evolve()
        assert len(pop) == 4

    def test_perplexity_fitness_on_qwen3(self, student_model):
        from genesis.core.fitness import PerplexityFitness

        vocab = student_model.model.config.vocab_size
        ids   = torch.randint(0, vocab, (BATCH_SIZE * 2, SEQ_LEN))
        mask  = torch.ones_like(ids)

        batches = [
            {"input_ids": ids[i:i+BATCH_SIZE],
             "attention_mask": mask[i:i+BATCH_SIZE],
             "labels": ids[i:i+BATCH_SIZE]}
            for i in range(0, len(ids), BATCH_SIZE)
        ]

        evaluator = PerplexityFitness(
            dataloader=batches,
            device="cuda:0",
            max_samples=BATCH_SIZE * 2,
        )

        result = evaluator.evaluate(student_model.model)
        assert 0 <= result.score <= 1
        assert torch.isfinite(torch.tensor(result.score))
        assert result.metrics["perplexity"] > 1.0
```

<a id="tests-test_selection-py"></a>

#### `tests/test_selection.py`
*10110 bytes Â· ~2,245 tokens*

```python
"""Tests for all selection strategies."""

import pytest
import numpy as np

from genesis.core.population import Individual
from genesis.core.selection import (
    ElitismSelection,
    TournamentSelection,
    RouletteWheelSelection,
    RankSelection,
    TruncationSelection,
    BoltzmannSelection,
    SteadyStateSelection,
    create_selection_strategy,
)


# â”€â”€ Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _pop(fitnesses: list[float]) -> list[Individual]:
    """Create a population list with the given fitness values."""
    return [Individual(state_dict={}, fitness=f) for f in fitnesses]


# â”€â”€ ElitismSelection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestElitismSelection:

    def test_selects_correct_count(self):
        sel = ElitismSelection()
        pop = _pop([0.1, 0.9, 0.5, 0.7, 0.3])
        result = sel.select(pop, 3)
        assert len(result) == 3

    def test_selects_top_by_fitness(self):
        sel = ElitismSelection()
        pop = _pop([0.1, 0.9, 0.5, 0.7, 0.3])
        result = sel.select(pop, 2)
        fitnesses = sorted([ind.fitness for ind in result], reverse=True)
        assert fitnesses[0] == pytest.approx(0.9)
        assert fitnesses[1] == pytest.approx(0.7)

    def test_select_one(self):
        sel = ElitismSelection()
        pop = _pop([0.2, 0.8, 0.5])
        result = sel.select(pop, 1)
        assert result[0].fitness == pytest.approx(0.8)

    def test_callable_interface(self):
        sel = ElitismSelection()
        pop = _pop([0.3, 0.7])
        result = sel(pop, 1)
        assert len(result) == 1


# â”€â”€ TournamentSelection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestTournamentSelection:

    def test_selects_correct_count(self):
        sel = TournamentSelection(tournament_size=3)
        pop = _pop([0.1, 0.5, 0.9, 0.3, 0.7])
        result = sel.select(pop, 4)
        assert len(result) == 4

    def test_winner_is_highest_fitness_in_tournament(self):
        """With a population of one distinct best, it should always be picked."""
        sel = TournamentSelection(tournament_size=5)
        pop = _pop([0.0, 0.0, 0.0, 0.0, 1.0])
        for _ in range(20):
            result = sel.select(pop, 1)
            assert result[0].fitness == pytest.approx(1.0)

    def test_tournament_size_clamped_to_population(self):
        """tournament_size > pop size must not raise."""
        sel = TournamentSelection(tournament_size=10)
        pop = _pop([0.5, 0.8])
        result = sel.select(pop, 2)
        assert len(result) == 2

    def test_single_individual_population(self):
        sel = TournamentSelection(tournament_size=3)
        pop = _pop([0.6])
        result = sel.select(pop, 1)
        assert result[0].fitness == pytest.approx(0.6)


# â”€â”€ RouletteWheelSelection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestRouletteWheelSelection:

    def test_selects_correct_count(self):
        sel = RouletteWheelSelection()
        pop = _pop([0.2, 0.4, 0.6, 0.8])
        result = sel.select(pop, 3)
        assert len(result) == 3

    def test_handles_negative_fitness(self):
        sel = RouletteWheelSelection()
        pop = _pop([-1.0, -0.5, 0.0, 0.5])
        result = sel.select(pop, 2)
        assert len(result) == 2

    def test_rank_scaling(self):
        sel = RouletteWheelSelection(scaling="rank")
        pop = _pop([0.1, 0.5, 0.9])
        result = sel.select(pop, 2)
        assert len(result) == 2

    def test_sigma_scaling(self):
        sel = RouletteWheelSelection(scaling="sigma")
        pop = _pop([0.1, 0.5, 0.9])
        result = sel.select(pop, 2)
        assert len(result) == 2

    def test_all_results_are_from_population(self):
        sel = RouletteWheelSelection()
        pop = _pop([0.1, 0.3, 0.6])
        pop_ids = {id(ind) for ind in pop}
        for ind in sel.select(pop, 10):
            assert id(ind) in pop_ids


# â”€â”€ RankSelection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestRankSelection:

    def test_selects_correct_count(self):
        sel = RankSelection(selection_pressure=1.5)
        pop = _pop([0.2, 0.4, 0.6, 0.8, 1.0])
        result = sel.select(pop, 3)
        assert len(result) == 3

    def test_probabilities_sum_to_one(self):
        """Indirectly: all selected must come from the population."""
        sel = RankSelection()
        pop = _pop([0.1, 0.5, 0.9])
        pop_ids = {id(ind) for ind in pop}
        for ind in sel.select(pop, 20):
            assert id(ind) in pop_ids

    def test_higher_fitness_selected_more_often(self):
        np.random.seed(0)
        sel = RankSelection(selection_pressure=2.0)
        fitnesses = [0.0, 0.0, 0.0, 0.0, 1.0]  # only last one is best
        pop = _pop(fitnesses)
        best = max(pop, key=lambda x: x.fitness)
        selected = sel.select(pop, 100)
        best_count = sum(1 for ind in selected if ind is best)
        # Best individual should be selected significantly more often
        assert best_count > 20


# â”€â”€ TruncationSelection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestTruncationSelection:

    def test_selects_correct_count(self):
        sel = TruncationSelection(truncation_rate=0.5)
        pop = _pop([0.1, 0.3, 0.5, 0.7, 0.9])
        result = sel.select(pop, 4)
        assert len(result) == 4

    def test_only_top_half_selected(self):
        sel = TruncationSelection(truncation_rate=0.5)
        pop = _pop([0.1, 0.2, 0.8, 0.9])
        result = sel.select(pop, 20)
        for ind in result:
            assert ind.fitness >= 0.8

    def test_truncation_rate_one_uses_whole_population(self):
        sel = TruncationSelection(truncation_rate=1.0)
        pop = _pop([0.1, 0.5, 0.9])
        pop_ids = {id(ind) for ind in pop}
        for ind in sel.select(pop, 10):
            assert id(ind) in pop_ids


# â”€â”€ BoltzmannSelection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestBoltzmannSelection:

    def test_selects_correct_count(self):
        sel = BoltzmannSelection(initial_temperature=5.0)
        pop = _pop([0.1, 0.4, 0.7, 1.0])
        result = sel.select(pop, 3)
        assert len(result) == 3

    def test_cool_down_reduces_temperature(self):
        sel = BoltzmannSelection(initial_temperature=10.0, cooling_rate=0.5,
                                 min_temperature=0.1)
        t1 = sel.temperature
        sel.cool_down()
        assert sel.temperature < t1

    def test_cool_down_respects_minimum(self):
        sel = BoltzmannSelection(initial_temperature=0.1, min_temperature=0.1,
                                 cooling_rate=0.5)
        sel.cool_down()
        assert sel.temperature >= 0.1

    def test_all_selected_from_population(self):
        sel = BoltzmannSelection(initial_temperature=1.0)
        pop = _pop([0.2, 0.5, 0.8])
        pop_ids = {id(ind) for ind in pop}
        for ind in sel.select(pop, 10):
            assert id(ind) in pop_ids


# â”€â”€ SteadyStateSelection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestSteadyStateSelection:

    def test_selects_correct_count(self):
        sel = SteadyStateSelection(replacement_rate=0.4)
        pop = _pop([0.1, 0.3, 0.5, 0.7, 0.9])
        result = sel.select(pop, 5)
        assert len(result) == 5

    def test_survivors_are_high_fitness(self):
        sel = SteadyStateSelection(replacement_rate=0.2)
        fitnesses = [0.1, 0.2, 0.3, 0.4, 1.0]
        pop = _pop(fitnesses)
        result = sel.select(pop, 5)
        # The best individual must be in survivors (top 80%)
        best = max(pop, key=lambda x: x.fitness)
        assert any(ind is best for ind in result)

    def test_uses_custom_parent_selection(self):
        parent_sel = ElitismSelection()
        sel = SteadyStateSelection(replacement_rate=0.5,
                                   parent_selection=parent_sel)
        pop = _pop([0.1, 0.5, 0.9, 0.3, 0.7])
        result = sel.select(pop, 5)
        assert len(result) == 5


# â”€â”€ Factory function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestCreateSelectionStrategy:

    @pytest.mark.parametrize("name,cls", [
        ("elitism", ElitismSelection),
        ("tournament", TournamentSelection),
        ("roulette", RouletteWheelSelection),
        ("rank", RankSelection),
        ("truncation", TruncationSelection),
        ("boltzmann", BoltzmannSelection),
        ("steady_state", SteadyStateSelection),
    ])
    def test_factory_returns_correct_type(self, name, cls):
        strategy = create_selection_strategy(name)
        assert isinstance(strategy, cls)

    def test_factory_unknown_raises(self):
        with pytest.raises(ValueError):
            create_selection_strategy("unknown_strategy")

    def test_factory_passes_kwargs(self):
        sel = create_selection_strategy("tournament", tournament_size=7)
        assert sel.tournament_size == 7
```

<a id="tests-test_tts-py"></a>

#### `tests/test_tts.py`
*20052 bytes Â· ~4,759 tokens*

```python
"""Tests for genesis/tts/mcd_fitness.py, style_evolution.py, and tts_child.py."""

import pytest
import torch
import numpy as np

from genesis.tts.mcd_fitness import (
    compute_mcd,
    compute_f0_rmse,
    compute_vuv_error,
    MCDFitness,
)


# â”€â”€ compute_mcd â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestComputeMCD:

    def _mel(self, batch=1, mel_dim=80, time=50):
        """Random positive mel spectrogram (values > 0 for log)."""
        return torch.rand(batch, mel_dim, time) + 0.1

    def test_identical_mels_zero_mcd(self):
        mel = self._mel()
        mcd = compute_mcd(mel, mel, reduction="mean")
        assert mcd.item() == pytest.approx(0.0, abs=1e-5)

    def test_different_mels_positive_mcd(self):
        ref = self._mel()
        syn = torch.rand_like(ref) * 10 + 0.1
        mcd = compute_mcd(ref, syn, reduction="mean")
        assert mcd.item() > 0.0

    def test_reduction_mean_is_scalar(self):
        mel = self._mel()
        mcd = compute_mcd(mel, mel, reduction="mean")
        assert mcd.ndim == 0

    def test_reduction_sum_is_scalar(self):
        mel = self._mel()
        mcd = compute_mcd(mel, mel, reduction="sum")
        assert mcd.ndim == 0

    def test_reduction_none_shape(self):
        mel = self._mel(batch=2, mel_dim=80, time=10)
        mcd = compute_mcd(mel, mel, reduction="none")
        # Should be (batch, time)
        assert mcd.ndim == 2

    def test_handles_different_lengths(self):
        ref = torch.rand(1, 80, 50) + 0.1
        syn = torch.rand(1, 80, 30) + 0.1
        # Should truncate to min length and not raise
        mcd = compute_mcd(ref, syn, reduction="mean")
        assert torch.isfinite(mcd)

    def test_batch_dimension(self):
        mel = self._mel(batch=4)
        mcd = compute_mcd(mel, mel, reduction="mean")
        assert torch.isfinite(mcd)

    def test_higher_distortion_higher_mcd(self):
        ref = self._mel()
        close = (ref + torch.rand_like(ref) * 0.001).clamp(min=0.01)
        far = (ref * 50 + 5.0).clamp(min=0.01)  # clearly different, always positive
        mcd_close = compute_mcd(ref, close, reduction="mean").item()
        mcd_far = compute_mcd(ref, far, reduction="mean").item()
        assert mcd_far > mcd_close


# â”€â”€ compute_f0_rmse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestComputeF0RMSE:

    def test_identical_f0_zero_rmse(self):
        f0 = torch.tensor([110.0, 120.0, 130.0, 0.0])  # last frame unvoiced
        rmse = compute_f0_rmse(f0, f0)
        assert rmse.item() == pytest.approx(0.0, abs=1e-4)

    def test_all_unvoiced_returns_inf(self):
        f0 = torch.zeros(10)
        rmse = compute_f0_rmse(f0, f0)
        assert rmse.item() == float("inf")

    def test_different_f0_positive_rmse(self):
        ref = torch.tensor([100.0, 110.0, 120.0, 130.0])
        syn = torch.tensor([200.0, 220.0, 240.0, 260.0])
        rmse = compute_f0_rmse(ref, syn)
        assert rmse.item() > 0

    def test_handles_length_mismatch(self):
        ref = torch.ones(10) * 100.0
        syn = torch.ones(7) * 100.0
        rmse = compute_f0_rmse(ref, syn)
        # Should not raise; identical F0 in voiced region â†’ 0
        assert rmse.item() == pytest.approx(0.0, abs=1e-4)

    def test_rmse_in_cents(self):
        # One octave difference: ratio 2 â†’ 1200 cents
        ref = torch.tensor([100.0])
        syn = torch.tensor([200.0])
        rmse = compute_f0_rmse(ref, syn)
        assert rmse.item() == pytest.approx(1200.0, abs=1.0)


# â”€â”€ compute_vuv_error â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestComputeVUVError:

    def test_identical_vuv_zero_error(self):
        f0 = torch.tensor([100.0, 0.0, 120.0, 0.0, 130.0])
        err = compute_vuv_error(f0, f0)
        assert err == pytest.approx(0.0)

    def test_all_voiced_vs_all_unvoiced(self):
        ref = torch.ones(10) * 100.0     # all voiced
        syn = torch.zeros(10)            # all unvoiced
        err = compute_vuv_error(ref, syn)
        assert err == pytest.approx(1.0)

    def test_half_mismatch(self):
        ref = torch.tensor([100.0, 100.0, 0.0, 0.0])
        syn = torch.tensor([0.0, 0.0, 100.0, 100.0])
        err = compute_vuv_error(ref, syn)
        assert err == pytest.approx(1.0)

    def test_handles_length_mismatch(self):
        ref = torch.ones(10) * 100.0
        syn = torch.ones(8) * 100.0
        err = compute_vuv_error(ref, syn)
        assert 0.0 <= err <= 1.0


# â”€â”€ MCDFitness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestMCDFitness:

    def _mel(self, batch=1, mel_dim=80, time=30):
        return torch.rand(batch, mel_dim, time) + 0.1

    def test_raises_without_reference(self):
        evaluator = MCDFitness()
        with pytest.raises(ValueError, match="No reference"):
            evaluator.evaluate(self._mel())

    def test_evaluate_identical_high_fitness(self):
        mel = self._mel()
        evaluator = MCDFitness(reference_mels=[mel])
        result = evaluator.evaluate(mel)
        # MCD=0 â†’ similarity_fitness = 1/(1+0) = 1.0
        assert result["similarity_fitness"] == pytest.approx(1.0, abs=1e-5)

    def test_evaluate_returns_expected_keys(self):
        mel = self._mel()
        evaluator = MCDFitness(reference_mels=[mel])
        result = evaluator.evaluate(mel)
        for key in ["fitness", "mcd", "min_mcd", "similarity_fitness", "naturalness_fitness"]:
            assert key in result

    def test_fitness_in_range(self):
        ref = self._mel()
        syn = self._mel()
        evaluator = MCDFitness(reference_mels=[ref])
        result = evaluator.evaluate(syn)
        assert 0.0 <= result["fitness"] <= 1.0

    def test_add_reference(self):
        evaluator = MCDFitness()
        mel = self._mel()
        evaluator.add_reference(mel)
        assert len(evaluator.reference_mels) == 1

    def test_evaluate_with_specific_reference_idx(self):
        mel1 = self._mel()
        mel2 = self._mel() * 5  # clearly different
        evaluator = MCDFitness(reference_mels=[mel1, mel2])
        r0 = evaluator.evaluate(mel1, reference_idx=0)
        assert r0["mcd"] == pytest.approx(0.0, abs=1e-5)

    def test_batch_evaluate_length(self):
        ref = self._mel()
        evaluator = MCDFitness(reference_mels=[ref])
        mels = [self._mel() for _ in range(3)]
        results = evaluator.batch_evaluate(mels)
        assert len(results) == 3

    def test_batch_evaluate_all_have_keys(self):
        ref = self._mel()
        evaluator = MCDFitness(reference_mels=[ref])
        mels = [self._mel() for _ in range(2)]
        results = evaluator.batch_evaluate(mels)
        for r in results:
            assert "fitness" in r

    def test_rank_population_sorted_descending(self):
        ref = self._mel()
        evaluator = MCDFitness(reference_mels=[ref])
        # Include the reference itself to ensure at least one high-fitness entry
        mels = [ref, self._mel() * 10]
        ranked = evaluator.rank_population(mels)
        assert len(ranked) == 2
        # First rank should have higher fitness than second
        assert ranked[0][1] >= ranked[1][1]

    def test_naturalness_score_in_range(self):
        mel = self._mel()
        evaluator = MCDFitness(reference_mels=[mel])
        result = evaluator.evaluate(mel)
        assert 0.0 <= result["naturalness_fitness"] <= 1.0

    def test_lower_mcd_higher_similarity_fitness(self):
        ref = self._mel()
        close = ref + torch.randn_like(ref) * 0.01
        far = self._mel() * 10
        evaluator = MCDFitness(reference_mels=[ref])
        r_close = evaluator.evaluate(close)
        r_far = evaluator.evaluate(far)
        assert r_close["similarity_fitness"] > r_far["similarity_fitness"]

    def test_multiple_references_averages(self):
        ref1 = self._mel()
        ref2 = self._mel()
        syn = self._mel()
        evaluator = MCDFitness(reference_mels=[ref1, ref2])
        result = evaluator.evaluate(syn)
        # Should be the average MCD over both references
        assert result["mcd"] >= 0


# â”€â”€ StyleToken â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestStyleToken:

    def test_clone_is_independent(self):
        from genesis.tts.style_evolution import StyleToken
        emb = torch.randn(10, 64)
        tok = StyleToken(embedding=emb, name="test", fitness=0.5)
        clone = tok.clone()
        clone.embedding[0, 0] = 999.0
        # Original should be untouched
        assert tok.embedding[0, 0] != 999.0

    def test_clone_copies_metadata(self):
        from genesis.tts.style_evolution import StyleToken
        tok = StyleToken(embedding=torch.randn(4, 8), metadata={"key": "value"})
        clone = tok.clone()
        assert clone.metadata["key"] == "value"

    def test_clone_has_same_fitness(self):
        from genesis.tts.style_evolution import StyleToken
        tok = StyleToken(embedding=torch.randn(4, 8), fitness=0.75)
        clone = tok.clone()
        assert clone.fitness == 0.75


# â”€â”€ StyleEvolution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestStyleEvolution:

    def _make_evo(self, pop_size=5, style_dim=32, num_tokens=4):
        from genesis.tts.style_evolution import StyleEvolution
        return StyleEvolution(
            style_dim=style_dim,
            num_tokens=num_tokens,
            population_size=pop_size,
            elite_size=1,
            mutation_rate=0.5,
            mutation_scale=0.05,
            crossover_rate=0.5,
        )

    def test_initialize_random_population(self):
        evo = self._make_evo()
        evo.initialize_population()
        assert len(evo.get_population()) == 5

    def test_initialize_from_base_tokens(self):
        evo = self._make_evo()
        base = torch.randn(4, 32)
        evo.initialize_population(base_tokens=base)
        assert len(evo.get_population()) == 5
        # First individual should match base
        assert torch.allclose(evo.get_individual(0), base)

    def test_set_fitness(self):
        evo = self._make_evo()
        evo.initialize_population()
        evo.set_fitness(0, 0.9)
        assert evo._fitnesses[0] == pytest.approx(0.9)

    def test_set_all_fitnesses(self):
        evo = self._make_evo(pop_size=4)
        evo.initialize_population()
        fits = [0.1, 0.2, 0.3, 0.4]
        evo.set_all_fitnesses(fits)
        assert evo._fitnesses == fits

    def test_best_fitness_property(self):
        evo = self._make_evo()
        evo.initialize_population()
        evo.set_all_fitnesses([0.1, 0.5, 0.3, 0.2, 0.4])
        assert evo.best_fitness == pytest.approx(0.5)

    def test_average_fitness_property(self):
        evo = self._make_evo(pop_size=4)
        evo.initialize_population()
        evo.set_all_fitnesses([0.0, 1.0, 0.0, 1.0])
        assert evo.average_fitness == pytest.approx(0.5)

    def test_generation_starts_at_zero(self):
        evo = self._make_evo()
        assert evo.generation == 0

    def test_evolve_increments_generation(self):
        evo = self._make_evo()
        evo.initialize_population()
        evo.set_all_fitnesses([float(i) for i in range(5)])
        evo.evolve()
        assert evo.generation == 1

    def test_evolve_preserves_population_size(self):
        evo = self._make_evo(pop_size=6)
        evo.initialize_population()
        evo.set_all_fitnesses([float(i) for i in range(6)])
        evo.evolve()
        assert len(evo.get_population()) == 6

    def test_get_best_returns_highest_fitness(self):
        evo = self._make_evo(pop_size=4)
        evo.initialize_population()
        evo.set_all_fitnesses([0.1, 0.9, 0.5, 0.3])
        _, best_fit = evo.get_best()
        assert best_fit == pytest.approx(0.9)

    def test_save_and_load_state(self, tmp_path):
        evo = self._make_evo(pop_size=3)
        evo.initialize_population()
        evo.set_all_fitnesses([0.1, 0.5, 0.3])
        evo._generation = 7

        path = str(tmp_path / "style_evo.pt")
        evo.save_state(path)

        evo2 = self._make_evo(pop_size=3)
        evo2.load_state(path)

        assert evo2.generation == 7
        assert evo2.population_size == 3
        assert len(evo2.get_population()) == 3

    def test_individual_shape_correct(self):
        evo = self._make_evo(style_dim=64, num_tokens=8)
        evo.initialize_population()
        ind = evo.get_individual(0)
        assert ind.shape == (8, 64)


# â”€â”€ MultiStyleEvolution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestMultiStyleEvolution:

    def test_initialize_all_creates_evolutions(self):
        from genesis.tts.style_evolution import MultiStyleEvolution
        configs = {
            "prosody": {"dim": 32, "num_tokens": 4},
            "emotion": {"dim": 64, "num_tokens": 8},
        }
        mse = MultiStyleEvolution(style_configs=configs, population_size=4)
        assert "prosody" in mse.evolutions
        assert "emotion" in mse.evolutions

    def test_initialize_all_no_base(self):
        from genesis.tts.style_evolution import MultiStyleEvolution
        configs = {"style": {"dim": 16, "num_tokens": 2}}
        mse = MultiStyleEvolution(style_configs=configs, population_size=3)
        mse.initialize_all()
        assert len(mse.evolutions["style"].get_population()) == 3

    def test_evolve_all(self):
        from genesis.tts.style_evolution import MultiStyleEvolution
        configs = {"style": {"dim": 16, "num_tokens": 2}}
        mse = MultiStyleEvolution(style_configs=configs, population_size=3)
        mse.initialize_all()
        for evo in mse.evolutions.values():
            evo.set_all_fitnesses([float(i) for i in range(3)])
        mse.evolve_all()
        assert mse.evolutions["style"].generation == 1

    def test_get_combined_style(self):
        from genesis.tts.style_evolution import MultiStyleEvolution
        configs = {"a": {"dim": 8, "num_tokens": 2}, "b": {"dim": 16, "num_tokens": 3}}
        mse = MultiStyleEvolution(style_configs=configs, population_size=4)
        mse.initialize_all()
        combined = mse.get_combined_style({"a": 0, "b": 1})
        assert "a" in combined and "b" in combined
        assert combined["a"].shape == (2, 8)
        assert combined["b"].shape == (3, 16)


# â”€â”€ TTSChild â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TestTTSChild:

    def test_default_initialization(self):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        assert child.fitness == 0.0
        assert child.generation == 0
        assert child.parent_ids == []
        assert child.id is not None

    def test_style_tokens_initialized(self):
        from genesis.tts.tts_child import TTSChild, TTSConfig
        cfg = TTSConfig(style_dim=64)
        child = TTSChild(config=cfg)
        assert child.style_tokens.shape == (10, 64)

    def test_speaker_embeddings_initialized(self):
        from genesis.tts.tts_child import TTSChild, TTSConfig
        cfg = TTSConfig(speaker_dim=128, num_speakers=2)
        child = TTSChild(config=cfg)
        assert child.speaker_embeddings.shape == (2, 128)

    def test_custom_style_tokens(self):
        from genesis.tts.tts_child import TTSChild
        tokens = torch.randn(5, 32)
        child = TTSChild(style_tokens=tokens)
        assert torch.allclose(child.style_tokens, tokens)

    def test_clone_is_independent(self):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        child.fitness = 0.7
        clone = child.clone()
        clone.style_tokens[0, 0] = 999.0
        assert child.style_tokens[0, 0] != 999.0

    def test_clone_copies_fitness(self):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        child.fitness = 0.8
        clone = child.clone()
        assert clone.fitness == 0.8

    def test_clone_parent_id_set(self):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        clone = child.clone()
        assert child.id in clone.parent_ids

    def test_mutate_returns_different_child(self):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        # With mutation_rate=1.0, mutation will always happen
        mutated = child.mutate(mutation_rate=1.0, mutation_scale=0.1)
        assert mutated is not child

    def test_mutate_increments_generation(self):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        child.generation = 3
        mutated = child.mutate(mutation_rate=1.0)
        assert mutated.generation == 4

    def test_crossover_child_generation(self):
        from genesis.tts.tts_child import TTSChild
        p1 = TTSChild()
        p1.generation = 2
        p2 = TTSChild()
        p2.generation = 4
        child = p1.crossover(p2)
        assert child.generation == 5  # max(2,4) + 1

    def test_crossover_child_has_both_parents(self):
        from genesis.tts.tts_child import TTSChild
        p1 = TTSChild()
        p2 = TTSChild()
        child = p1.crossover(p2)
        assert p1.id in child.parent_ids
        assert p2.id in child.parent_ids

    def test_crossover_interpolates_style_tokens(self):
        from genesis.tts.tts_child import TTSChild, TTSConfig
        cfg = TTSConfig(style_dim=8)
        p1 = TTSChild(config=cfg, style_tokens=torch.zeros(10, 8))
        p2 = TTSChild(config=cfg, style_tokens=torch.ones(10, 8))
        child = p1.crossover(p2)
        # Child should be between 0 and 1
        assert (child.style_tokens >= 0).all()
        assert (child.style_tokens <= 1).all()

    def test_get_and_set_evolved_params(self):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        params = child.get_evolved_params()
        assert "style_tokens" in params
        assert "speaker_embeddings" in params
        # Set new params
        new_tokens = torch.zeros_like(params["style_tokens"])
        child.set_evolved_params({"style_tokens": new_tokens})
        assert torch.allclose(child.style_tokens, new_tokens)

    def test_save_and_load_roundtrip(self, tmp_path):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        child.fitness = 0.65
        child.generation = 7
        path = str(tmp_path / "child.pt")
        child.save(path)

        loaded = TTSChild.load(path)
        assert loaded.id == child.id
        assert loaded.fitness == pytest.approx(0.65)
        assert loaded.generation == 7
        assert torch.allclose(loaded.style_tokens, child.style_tokens)

    def test_synthesize_raises_without_model(self):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        with pytest.raises(RuntimeError, match="Model not loaded"):
            child.synthesize("hello", device="cpu")

    def test_repr(self):
        from genesis.tts.tts_child import TTSChild
        child = TTSChild()
        r = repr(child)
        assert "TTSChild" in r
        assert "fitness" in r
```

---

## ğŸ“Š Summary

- **Files included**: 80
- **Estimated tokens**: ~171,729
- **Generated**: 2026-02-24 19:16
- **Tool**: repo2llm v1.1.0